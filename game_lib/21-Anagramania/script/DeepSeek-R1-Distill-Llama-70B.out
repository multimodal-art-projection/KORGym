INFO 03-02 04:01:06 __init__.py:207] Automatically detected platform cuda.
INFO 03-02 04:01:06 api_server.py:912] vLLM API server version 0.7.3
INFO 03-02 04:01:06 api_server.py:913] args: Namespace(host=None, port=9003, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='/map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Llama-70B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=15000, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=2, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.95, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['DeepSeek-R1-Distill-Llama-70B'], qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False)
INFO 03-02 04:01:12 config.py:549] This model supports multiple tasks: {'classify', 'embed', 'score', 'generate', 'reward'}. Defaulting to 'generate'.
INFO 03-02 04:01:12 config.py:1382] Defaulting to use mp for distributed inference
WARNING 03-02 04:01:12 config.py:676] Async output processing can not be enabled with pipeline parallel
INFO 03-02 04:01:12 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Llama-70B', speculative_config=None, tokenizer='/map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Llama-70B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=15000, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=DeepSeek-R1-Distill-Llama-70B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
WARNING 03-02 04:01:13 multiproc_worker_utils.py:300] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 03-02 04:01:13 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=890)[0;0m INFO 03-02 04:01:13 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=891)[0;0m INFO 03-02 04:01:13 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=892)[0;0m INFO 03-02 04:01:13 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=893)[0;0m INFO 03-02 04:01:13 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=894)[0;0m INFO 03-02 04:01:13 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=895)[0;0m INFO 03-02 04:01:13 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=896)[0;0m INFO 03-02 04:01:13 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=891)[0;0m INFO 03-02 04:01:14 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=894)[0;0m INFO 03-02 04:01:14 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=890)[0;0m INFO 03-02 04:01:14 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=895)[0;0m INFO 03-02 04:01:14 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=892)[0;0m INFO 03-02 04:01:14 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=896)[0;0m INFO 03-02 04:01:14 cuda.py:229] Using Flash Attention backend.
INFO 03-02 04:01:14 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=893)[0;0m INFO 03-02 04:01:14 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=893)[0;0m INFO 03-02 04:01:16 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=894)[0;0m INFO 03-02 04:01:16 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=896)[0;0m INFO 03-02 04:01:16 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=895)[0;0m INFO 03-02 04:01:16 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=893)[0;0m INFO 03-02 04:01:16 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=896)[0;0m INFO 03-02 04:01:16 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=894)[0;0m INFO 03-02 04:01:16 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=895)[0;0m INFO 03-02 04:01:16 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 03-02 04:01:16 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=890)[0;0m INFO 03-02 04:01:16 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=891)[0;0m INFO 03-02 04:01:16 utils.py:916] Found nccl from library libnccl.so.2
INFO 03-02 04:01:16 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=890)[0;0m INFO 03-02 04:01:16 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=891)[0;0m INFO 03-02 04:01:16 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=892)[0;0m INFO 03-02 04:01:16 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=892)[0;0m INFO 03-02 04:01:16 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 03-02 04:01:18 custom_all_reduce_utils.py:206] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
INFO 03-02 04:01:40 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=893)[0;0m INFO 03-02 04:01:40 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=891)[0;0m INFO 03-02 04:01:40 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=896)[0;0m INFO 03-02 04:01:40 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=894)[0;0m INFO 03-02 04:01:40 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=890)[0;0m INFO 03-02 04:01:40 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=892)[0;0m INFO 03-02 04:01:40 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=895)[0;0m INFO 03-02 04:01:40 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=893)[0;0m INFO 03-02 04:01:40 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_08b0d6b7'), local_subscribe_port=59689, remote_subscribe_port=None)
INFO 03-02 04:01:40 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_e5136c68'), local_subscribe_port=49561, remote_subscribe_port=None)
[1;36m(VllmWorkerProcess pid=893)[0;0m INFO 03-02 04:01:40 utils.py:916] Found nccl from library libnccl.so.2
INFO 03-02 04:01:40 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=893)[0;0m INFO 03-02 04:01:40 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 03-02 04:01:40 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=896)[0;0m INFO 03-02 04:01:40 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=892)[0;0m INFO 03-02 04:01:40 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=896)[0;0m INFO 03-02 04:01:40 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=892)[0;0m INFO 03-02 04:01:40 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=891)[0;0m INFO 03-02 04:01:40 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=895)[0;0m INFO 03-02 04:01:40 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=891)[0;0m INFO 03-02 04:01:40 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=890)[0;0m INFO 03-02 04:01:40 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=894)[0;0m INFO 03-02 04:01:40 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=895)[0;0m INFO 03-02 04:01:40 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=890)[0;0m INFO 03-02 04:01:40 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=894)[0;0m INFO 03-02 04:01:40 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 03-02 04:01:41 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Llama-70B...
[1;36m(VllmWorkerProcess pid=890)[0;0m INFO 03-02 04:01:41 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Llama-70B...
[1;36m(VllmWorkerProcess pid=893)[0;0m INFO 03-02 04:01:41 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Llama-70B...
[1;36m(VllmWorkerProcess pid=894)[0;0m INFO 03-02 04:01:41 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Llama-70B...
[1;36m(VllmWorkerProcess pid=891)[0;0m INFO 03-02 04:01:41 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Llama-70B...
[1;36m(VllmWorkerProcess pid=896)[0;0m INFO 03-02 04:01:41 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Llama-70B...
[1;36m(VllmWorkerProcess pid=892)[0;0m INFO 03-02 04:01:41 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Llama-70B...
[1;36m(VllmWorkerProcess pid=895)[0;0m INFO 03-02 04:01:41 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Llama-70B...
[1;36m(VllmWorkerProcess pid=895)[0;0m INFO 03-02 04:01:53 model_runner.py:1115] Loading model weights took 16.4603 GB
INFO 03-02 04:01:53 model_runner.py:1115] Loading model weights took 16.4603 GB
[1;36m(VllmWorkerProcess pid=893)[0;0m INFO 03-02 04:01:53 model_runner.py:1115] Loading model weights took 16.4603 GB
[1;36m(VllmWorkerProcess pid=891)[0;0m INFO 03-02 04:01:54 model_runner.py:1115] Loading model weights took 16.4603 GB
[1;36m(VllmWorkerProcess pid=896)[0;0m INFO 03-02 04:01:54 model_runner.py:1115] Loading model weights took 16.4603 GB
[1;36m(VllmWorkerProcess pid=894)[0;0m INFO 03-02 04:01:54 model_runner.py:1115] Loading model weights took 16.4603 GB
[1;36m(VllmWorkerProcess pid=890)[0;0m INFO 03-02 04:01:54 model_runner.py:1115] Loading model weights took 16.4603 GB
[1;36m(VllmWorkerProcess pid=892)[0;0m INFO 03-02 04:01:54 model_runner.py:1115] Loading model weights took 16.4603 GB
[1;36m(VllmWorkerProcess pid=896)[0;0m INFO 03-02 04:01:56 worker.py:267] Memory profiling takes 2.30 seconds
[1;36m(VllmWorkerProcess pid=896)[0;0m INFO 03-02 04:01:56 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=896)[0;0m INFO 03-02 04:01:56 worker.py:267] model weights take 16.46GiB; non_torch_memory takes 1.51GiB; PyTorch activation peak memory takes 1.58GiB; the rest of the memory reserved for KV Cache is 55.83GiB.
[1;36m(VllmWorkerProcess pid=895)[0;0m INFO 03-02 04:01:56 worker.py:267] Memory profiling takes 2.30 seconds
[1;36m(VllmWorkerProcess pid=895)[0;0m INFO 03-02 04:01:56 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=895)[0;0m INFO 03-02 04:01:56 worker.py:267] model weights take 16.46GiB; non_torch_memory takes 1.70GiB; PyTorch activation peak memory takes 1.58GiB; the rest of the memory reserved for KV Cache is 55.64GiB.
[1;36m(VllmWorkerProcess pid=894)[0;0m INFO 03-02 04:01:56 worker.py:267] Memory profiling takes 2.34 seconds
[1;36m(VllmWorkerProcess pid=894)[0;0m INFO 03-02 04:01:56 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=894)[0;0m INFO 03-02 04:01:56 worker.py:267] model weights take 16.46GiB; non_torch_memory takes 1.70GiB; PyTorch activation peak memory takes 1.58GiB; the rest of the memory reserved for KV Cache is 55.64GiB.
[1;36m(VllmWorkerProcess pid=893)[0;0m INFO 03-02 04:01:56 worker.py:267] Memory profiling takes 2.35 seconds
[1;36m(VllmWorkerProcess pid=893)[0;0m INFO 03-02 04:01:56 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=893)[0;0m INFO 03-02 04:01:56 worker.py:267] model weights take 16.46GiB; non_torch_memory takes 1.70GiB; PyTorch activation peak memory takes 1.86GiB; the rest of the memory reserved for KV Cache is 55.36GiB.
[1;36m(VllmWorkerProcess pid=892)[0;0m INFO 03-02 04:01:58 worker.py:267] Memory profiling takes 4.01 seconds
[1;36m(VllmWorkerProcess pid=892)[0;0m INFO 03-02 04:01:58 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=892)[0;0m INFO 03-02 04:01:58 worker.py:267] model weights take 16.46GiB; non_torch_memory takes 0.94GiB; PyTorch activation peak memory takes 1.35GiB; the rest of the memory reserved for KV Cache is 56.63GiB.
[1;36m(VllmWorkerProcess pid=891)[0;0m INFO 03-02 04:01:58 worker.py:267] Memory profiling takes 4.03 seconds
[1;36m(VllmWorkerProcess pid=891)[0;0m INFO 03-02 04:01:58 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=891)[0;0m INFO 03-02 04:01:58 worker.py:267] model weights take 16.46GiB; non_torch_memory takes 1.03GiB; PyTorch activation peak memory takes 1.35GiB; the rest of the memory reserved for KV Cache is 56.54GiB.
INFO 03-02 04:01:58 worker.py:267] Memory profiling takes 4.05 seconds
INFO 03-02 04:01:58 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
INFO 03-02 04:01:58 worker.py:267] model weights take 16.46GiB; non_torch_memory takes 0.94GiB; PyTorch activation peak memory takes 1.35GiB; the rest of the memory reserved for KV Cache is 56.63GiB.
[1;36m(VllmWorkerProcess pid=890)[0;0m INFO 03-02 04:01:58 worker.py:267] Memory profiling takes 4.06 seconds
[1;36m(VllmWorkerProcess pid=890)[0;0m INFO 03-02 04:01:58 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=890)[0;0m INFO 03-02 04:01:58 worker.py:267] model weights take 16.46GiB; non_torch_memory takes 1.03GiB; PyTorch activation peak memory takes 1.35GiB; the rest of the memory reserved for KV Cache is 56.54GiB.
INFO 03-02 04:01:58 executor_base.py:111] # cuda blocks: 90709, # CPU blocks: 6553
INFO 03-02 04:01:58 executor_base.py:116] Maximum concurrency for 15000 tokens per request: 96.76x
[1;36m(VllmWorkerProcess pid=896)[0;0m INFO 03-02 04:02:01 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=891)[0;0m INFO 03-02 04:02:01 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=893)[0;0m INFO 03-02 04:02:02 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=895)[0;0m INFO 03-02 04:02:02 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=890)[0;0m INFO 03-02 04:02:02 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=892)[0;0m INFO 03-02 04:02:02 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=894)[0;0m INFO 03-02 04:02:02 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 03-02 04:02:02 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=896)[0;0m INFO 03-02 04:02:34 custom_all_reduce.py:226] Registering 5600 cuda graph addresses
[1;36m(VllmWorkerProcess pid=893)[0;0m INFO 03-02 04:02:34 custom_all_reduce.py:226] Registering 5600 cuda graph addresses
[1;36m(VllmWorkerProcess pid=894)[0;0m INFO 03-02 04:02:34 custom_all_reduce.py:226] Registering 5600 cuda graph addresses
[1;36m(VllmWorkerProcess pid=895)[0;0m INFO 03-02 04:02:36 custom_all_reduce.py:226] Registering 5600 cuda graph addresses
[1;36m(VllmWorkerProcess pid=896)[0;0m INFO 03-02 04:02:36 model_runner.py:1562] Graph capturing finished in 34 secs, took 2.46 GiB
[1;36m(VllmWorkerProcess pid=893)[0;0m INFO 03-02 04:02:36 model_runner.py:1562] Graph capturing finished in 34 secs, took 2.46 GiB
[1;36m(VllmWorkerProcess pid=895)[0;0m INFO 03-02 04:02:36 model_runner.py:1562] Graph capturing finished in 34 secs, took 2.46 GiB
[1;36m(VllmWorkerProcess pid=894)[0;0m INFO 03-02 04:02:36 model_runner.py:1562] Graph capturing finished in 34 secs, took 2.46 GiB
[1;36m(VllmWorkerProcess pid=892)[0;0m INFO 03-02 04:02:37 custom_all_reduce.py:226] Registering 5670 cuda graph addresses
[1;36m(VllmWorkerProcess pid=891)[0;0m INFO 03-02 04:02:38 custom_all_reduce.py:226] Registering 5670 cuda graph addresses
INFO 03-02 04:02:38 custom_all_reduce.py:226] Registering 5670 cuda graph addresses
[1;36m(VllmWorkerProcess pid=890)[0;0m INFO 03-02 04:02:38 custom_all_reduce.py:226] Registering 5670 cuda graph addresses
[1;36m(VllmWorkerProcess pid=892)[0;0m INFO 03-02 04:02:38 model_runner.py:1562] Graph capturing finished in 37 secs, took 2.49 GiB
INFO 03-02 04:02:38 model_runner.py:1562] Graph capturing finished in 37 secs, took 2.49 GiB
[1;36m(VllmWorkerProcess pid=891)[0;0m INFO 03-02 04:02:38 model_runner.py:1562] Graph capturing finished in 37 secs, took 2.49 GiB
[1;36m(VllmWorkerProcess pid=890)[0;0m INFO 03-02 04:02:38 model_runner.py:1562] Graph capturing finished in 37 secs, took 2.49 GiB
INFO 03-02 04:02:38 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 44.42 seconds
INFO 03-02 04:02:40 api_server.py:958] Starting vLLM API server on http://0.0.0.0:9003
INFO 03-02 04:02:40 launcher.py:23] Available routes are:
INFO 03-02 04:02:40 launcher.py:31] Route: /openapi.json, Methods: GET, HEAD
INFO 03-02 04:02:40 launcher.py:31] Route: /docs, Methods: GET, HEAD
INFO 03-02 04:02:40 launcher.py:31] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 03-02 04:02:40 launcher.py:31] Route: /redoc, Methods: GET, HEAD
INFO 03-02 04:02:40 launcher.py:31] Route: /health, Methods: GET
INFO 03-02 04:02:40 launcher.py:31] Route: /ping, Methods: GET, POST
INFO 03-02 04:02:40 launcher.py:31] Route: /tokenize, Methods: POST
INFO 03-02 04:02:40 launcher.py:31] Route: /detokenize, Methods: POST
INFO 03-02 04:02:40 launcher.py:31] Route: /v1/models, Methods: GET
INFO 03-02 04:02:40 launcher.py:31] Route: /version, Methods: GET
INFO 03-02 04:02:40 launcher.py:31] Route: /v1/chat/completions, Methods: POST
INFO 03-02 04:02:40 launcher.py:31] Route: /v1/completions, Methods: POST
INFO 03-02 04:02:40 launcher.py:31] Route: /v1/embeddings, Methods: POST
INFO 03-02 04:02:40 launcher.py:31] Route: /pooling, Methods: POST
INFO 03-02 04:02:40 launcher.py:31] Route: /score, Methods: POST
INFO 03-02 04:02:40 launcher.py:31] Route: /v1/score, Methods: POST
INFO 03-02 04:02:40 launcher.py:31] Route: /v1/audio/transcriptions, Methods: POST
INFO 03-02 04:02:40 launcher.py:31] Route: /rerank, Methods: POST
INFO 03-02 04:02:40 launcher.py:31] Route: /v1/rerank, Methods: POST
INFO 03-02 04:02:40 launcher.py:31] Route: /v2/rerank, Methods: POST
INFO 03-02 04:02:40 launcher.py:31] Route: /invocations, Methods: POST
INFO 03-02 04:06:02 chat_utils.py:332] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
INFO 03-02 04:06:02 logger.py:39] Received request chatcmpl-4dc37b21ff864adeb37fc08a8b548aa0: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\ni b l r c v o e r y a\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14881, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:06:02 async_llm_engine.py:211] Added request chatcmpl-4dc37b21ff864adeb37fc08a8b548aa0.
INFO 03-02 04:06:02 logger.py:39] Received request chatcmpl-4b60fe25d5c14deca90317f3672c9c3c: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\nm r e d t e o a\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14884, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:06:02 async_llm_engine.py:211] Added request chatcmpl-4b60fe25d5c14deca90317f3672c9c3c.
INFO 03-02 04:06:02 logger.py:39] Received request chatcmpl-1ce1c06ab6274eab9e95b29d3915d462: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\ns u r b b i u a\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14884, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:06:02 async_llm_engine.py:211] Added request chatcmpl-1ce1c06ab6274eab9e95b29d3915d462.
INFO 03-02 04:06:02 logger.py:39] Received request chatcmpl-55266f7198564f3cac367729b0d7b4d3: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\nr o k y c\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14887, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:06:02 async_llm_engine.py:211] Added request chatcmpl-55266f7198564f3cac367729b0d7b4d3.
INFO 03-02 04:06:02 logger.py:39] Received request chatcmpl-2659149029b6487b8e7702fac81e8696: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\ns g p u n i o t p r\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14882, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:06:02 async_llm_engine.py:211] Added request chatcmpl-2659149029b6487b8e7702fac81e8696.
INFO 03-02 04:06:02 logger.py:39] Received request chatcmpl-aefab171c23d4e0ea5e6e37e3b4ef5ec: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\nb u g y g\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14887, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:06:02 async_llm_engine.py:211] Added request chatcmpl-aefab171c23d4e0ea5e6e37e3b4ef5ec.
INFO 03-02 04:06:02 logger.py:39] Received request chatcmpl-2b23f7475e96452f9741142cb4f4821b: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\ne r e h t\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14887, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:06:02 async_llm_engine.py:211] Added request chatcmpl-2b23f7475e96452f9741142cb4f4821b.
INFO 03-02 04:06:02 logger.py:39] Received request chatcmpl-4927ab8cb2df4704958975d6322ed824: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\nd m g a i r a\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14885, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:06:02 async_llm_engine.py:211] Added request chatcmpl-4927ab8cb2df4704958975d6322ed824.
INFO 03-02 04:06:05 metrics.py:455] Avg prompt throughput: 184.0 tokens/s, Avg generation throughput: 81.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 04:06:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 232.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 04:06:12 async_llm_engine.py:179] Finished request chatcmpl-55266f7198564f3cac367729b0d7b4d3.
INFO:     127.0.0.1:58206 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:06:12 logger.py:39] Received request chatcmpl-60db92b3d1984323843094a751d5e349: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\na g g i n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14887, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:06:12 async_llm_engine.py:211] Added request chatcmpl-60db92b3d1984323843094a751d5e349.
INFO 03-02 04:06:15 async_llm_engine.py:179] Finished request chatcmpl-4927ab8cb2df4704958975d6322ed824.
INFO:     127.0.0.1:58244 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:06:15 logger.py:39] Received request chatcmpl-747da1e95851479dadfc6de5caefa042: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\ns l n a i\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14887, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:06:15 async_llm_engine.py:211] Added request chatcmpl-747da1e95851479dadfc6de5caefa042.
INFO 03-02 04:06:15 metrics.py:455] Avg prompt throughput: 45.2 tokens/s, Avg generation throughput: 226.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 04:06:16 async_llm_engine.py:179] Finished request chatcmpl-2b23f7475e96452f9741142cb4f4821b.
INFO:     127.0.0.1:58242 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:06:16 logger.py:39] Received request chatcmpl-a1c265d014f84a0fa49ad93130c2823a: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\np i g r n i a\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14885, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:06:16 async_llm_engine.py:211] Added request chatcmpl-a1c265d014f84a0fa49ad93130c2823a.
INFO 03-02 04:06:20 metrics.py:455] Avg prompt throughput: 22.9 tokens/s, Avg generation throughput: 228.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-02 04:06:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 228.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-02 04:06:26 async_llm_engine.py:179] Finished request chatcmpl-aefab171c23d4e0ea5e6e37e3b4ef5ec.
INFO:     127.0.0.1:58226 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:06:26 logger.py:39] Received request chatcmpl-2dfeef60165d4a2cb0d0260d24e12898: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\nk t i t e n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14886, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:06:26 async_llm_engine.py:211] Added request chatcmpl-2dfeef60165d4a2cb0d0260d24e12898.
INFO 03-02 04:06:30 metrics.py:455] Avg prompt throughput: 22.8 tokens/s, Avg generation throughput: 227.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:06:31 async_llm_engine.py:179] Finished request chatcmpl-1ce1c06ab6274eab9e95b29d3915d462.
INFO:     127.0.0.1:58190 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:06:31 logger.py:39] Received request chatcmpl-b8f84d5aa5804570bdd68c75b19d99db: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\nc e b a c o e n v l i\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14881, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:06:31 async_llm_engine.py:211] Added request chatcmpl-b8f84d5aa5804570bdd68c75b19d99db.
INFO 03-02 04:06:35 metrics.py:455] Avg prompt throughput: 23.7 tokens/s, Avg generation throughput: 226.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:06:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 227.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-02 04:06:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 226.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 03-02 04:06:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 224.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 03-02 04:06:52 async_llm_engine.py:179] Finished request chatcmpl-2dfeef60165d4a2cb0d0260d24e12898.
INFO:     127.0.0.1:54022 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:06:52 logger.py:39] Received request chatcmpl-04bf4b140931486f8626cf94a6c67f9c: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\ne b m d e\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14887, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:06:52 async_llm_engine.py:211] Added request chatcmpl-04bf4b140931486f8626cf94a6c67f9c.
INFO 03-02 04:06:55 metrics.py:455] Avg prompt throughput: 22.5 tokens/s, Avg generation throughput: 221.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 03-02 04:07:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 223.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 03-02 04:07:02 async_llm_engine.py:179] Finished request chatcmpl-4dc37b21ff864adeb37fc08a8b548aa0.
INFO:     127.0.0.1:58180 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:07:02 logger.py:39] Received request chatcmpl-3cbe6b95ef464ed69cc9d9d21ba89a7d: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\nc e e m r y n o\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14884, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:07:02 async_llm_engine.py:211] Added request chatcmpl-3cbe6b95ef464ed69cc9d9d21ba89a7d.
INFO 03-02 04:07:03 async_llm_engine.py:179] Finished request chatcmpl-a1c265d014f84a0fa49ad93130c2823a.
INFO:     127.0.0.1:33752 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:07:03 logger.py:39] Received request chatcmpl-f734a514ba834b3e9743890173f3b734: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\ns t e m m e i o\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14884, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:07:03 async_llm_engine.py:211] Added request chatcmpl-f734a514ba834b3e9743890173f3b734.
INFO 03-02 04:07:05 metrics.py:455] Avg prompt throughput: 46.4 tokens/s, Avg generation throughput: 220.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 03-02 04:07:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 222.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 03-02 04:07:15 async_llm_engine.py:179] Finished request chatcmpl-747da1e95851479dadfc6de5caefa042.
INFO:     127.0.0.1:33736 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:07:15 logger.py:39] Received request chatcmpl-c8425479e49e42d2b56d1e4fd7adbbb6: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\nm n t i s u e a d i n n r d g s\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14876, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:07:15 async_llm_engine.py:211] Added request chatcmpl-c8425479e49e42d2b56d1e4fd7adbbb6.
INFO 03-02 04:07:15 metrics.py:455] Avg prompt throughput: 24.8 tokens/s, Avg generation throughput: 220.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 03-02 04:07:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 222.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 03-02 04:07:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 221.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 03-02 04:07:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 219.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 03-02 04:07:34 async_llm_engine.py:179] Finished request chatcmpl-3cbe6b95ef464ed69cc9d9d21ba89a7d.
INFO:     127.0.0.1:37898 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:07:34 logger.py:39] Received request chatcmpl-1ccaeb9237784f7dbd3877208dc4ab3f: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\np a e l a t\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14886, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:07:34 async_llm_engine.py:211] Added request chatcmpl-1ccaeb9237784f7dbd3877208dc4ab3f.
INFO 03-02 04:07:35 metrics.py:455] Avg prompt throughput: 22.8 tokens/s, Avg generation throughput: 218.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 03-02 04:07:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 218.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 03-02 04:07:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 215.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 03-02 04:07:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 215.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 03-02 04:07:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 215.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 03-02 04:08:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 215.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-02 04:08:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 213.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-02 04:08:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 213.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:08:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 210.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-02 04:08:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-02 04:08:21 async_llm_engine.py:179] Finished request chatcmpl-60db92b3d1984323843094a751d5e349.
INFO:     127.0.0.1:54422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:08:21 logger.py:39] Received request chatcmpl-a8d2bce227a6413c9bbe5dd9036fbe68: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\ni n t u t o n l r c i a s\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14879, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:08:21 async_llm_engine.py:211] Added request chatcmpl-a8d2bce227a6413c9bbe5dd9036fbe68.
INFO 03-02 04:08:22 async_llm_engine.py:179] Finished request chatcmpl-1ccaeb9237784f7dbd3877208dc4ab3f.
INFO:     127.0.0.1:48752 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:08:22 logger.py:39] Received request chatcmpl-aac77be3991b4a79835d5e95d87a7e53: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\ns m r t o n s a p\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14883, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:08:22 async_llm_engine.py:211] Added request chatcmpl-aac77be3991b4a79835d5e95d87a7e53.
INFO 03-02 04:08:25 metrics.py:455] Avg prompt throughput: 47.6 tokens/s, Avg generation throughput: 207.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-02 04:08:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 210.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:08:33 async_llm_engine.py:179] Finished request chatcmpl-04bf4b140931486f8626cf94a6c67f9c.
INFO:     127.0.0.1:38880 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:08:33 logger.py:39] Received request chatcmpl-d631586376cd403683c067f4d19f1bf7: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\nv t e e d n a t\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14884, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:08:33 async_llm_engine.py:211] Added request chatcmpl-d631586376cd403683c067f4d19f1bf7.
INFO 03-02 04:08:36 metrics.py:455] Avg prompt throughput: 23.1 tokens/s, Avg generation throughput: 208.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-02 04:08:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-02 04:08:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:08:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 209.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-02 04:08:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-02 04:09:00 async_llm_engine.py:179] Finished request chatcmpl-2659149029b6487b8e7702fac81e8696.
INFO:     127.0.0.1:58212 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:09:00 logger.py:39] Received request chatcmpl-3ae1dc08252e4fa7b0f75fc13bcad255: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\np o r n e e v t n i\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14882, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:09:00 async_llm_engine.py:211] Added request chatcmpl-3ae1dc08252e4fa7b0f75fc13bcad255.
INFO 03-02 04:09:01 metrics.py:455] Avg prompt throughput: 23.5 tokens/s, Avg generation throughput: 206.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-02 04:09:03 async_llm_engine.py:179] Finished request chatcmpl-aac77be3991b4a79835d5e95d87a7e53.
INFO:     127.0.0.1:55082 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:09:03 logger.py:39] Received request chatcmpl-3cc798e2d9394daa9c935f36df963667: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\np c u y i o t s r i m\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14881, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:09:03 async_llm_engine.py:211] Added request chatcmpl-3cc798e2d9394daa9c935f36df963667.
INFO 03-02 04:09:06 metrics.py:455] Avg prompt throughput: 23.8 tokens/s, Avg generation throughput: 209.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-02 04:09:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:09:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 214.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:09:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-02 04:09:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 210.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-02 04:09:27 async_llm_engine.py:179] Finished request chatcmpl-d631586376cd403683c067f4d19f1bf7.
INFO:     127.0.0.1:41722 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:09:27 logger.py:39] Received request chatcmpl-aec1c654383c4494a2ad2b6edcbdb480: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\na d e c t i c r\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14884, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:09:27 async_llm_engine.py:211] Added request chatcmpl-aec1c654383c4494a2ad2b6edcbdb480.
INFO 03-02 04:09:31 metrics.py:455] Avg prompt throughput: 23.2 tokens/s, Avg generation throughput: 208.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-02 04:09:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 209.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-02 04:09:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-02 04:09:41 async_llm_engine.py:179] Finished request chatcmpl-f734a514ba834b3e9743890173f3b734.
INFO:     127.0.0.1:37902 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:09:41 logger.py:39] Received request chatcmpl-363ca6ec13e64428989bb09cd07707ae: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\ni c n o a t n v o i\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14882, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:09:41 async_llm_engine.py:211] Added request chatcmpl-363ca6ec13e64428989bb09cd07707ae.
INFO 03-02 04:09:46 metrics.py:455] Avg prompt throughput: 23.6 tokens/s, Avg generation throughput: 209.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-02 04:09:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 209.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-02 04:09:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 209.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-02 04:09:56 async_llm_engine.py:179] Finished request chatcmpl-a8d2bce227a6413c9bbe5dd9036fbe68.
INFO:     127.0.0.1:55074 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:09:56 logger.py:39] Received request chatcmpl-f2408d50947b4952a9135527648cfba8: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\np e t o m o r r\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14884, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:09:56 async_llm_engine.py:211] Added request chatcmpl-f2408d50947b4952a9135527648cfba8.
INFO 03-02 04:10:01 metrics.py:455] Avg prompt throughput: 23.2 tokens/s, Avg generation throughput: 208.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-02 04:10:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 209.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-02 04:10:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-02 04:10:14 async_llm_engine.py:179] Finished request chatcmpl-c8425479e49e42d2b56d1e4fd7adbbb6.
INFO:     127.0.0.1:41390 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:10:14 logger.py:39] Received request chatcmpl-dc065a4d6a7040a69745a3389e5f581c: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\nf e a e l s s c\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14884, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:10:14 async_llm_engine.py:211] Added request chatcmpl-dc065a4d6a7040a69745a3389e5f581c.
INFO 03-02 04:10:16 metrics.py:455] Avg prompt throughput: 23.1 tokens/s, Avg generation throughput: 207.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:10:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-02 04:10:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 209.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-02 04:10:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 209.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-02 04:10:32 async_llm_engine.py:179] Finished request chatcmpl-4b60fe25d5c14deca90317f3672c9c3c.
INFO:     127.0.0.1:58182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:10:32 logger.py:39] Received request chatcmpl-31d0eda936bf479a8258805df8a91a44: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\nm h e i e o u t p c\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14882, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:10:32 async_llm_engine.py:211] Added request chatcmpl-31d0eda936bf479a8258805df8a91a44.
INFO 03-02 04:10:35 async_llm_engine.py:179] Finished request chatcmpl-f2408d50947b4952a9135527648cfba8.
INFO:     127.0.0.1:53100 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:10:35 logger.py:39] Received request chatcmpl-fade6917b82e4156960eb8b498fbc68e: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\nd v i e y e l r\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14884, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:10:35 async_llm_engine.py:211] Added request chatcmpl-fade6917b82e4156960eb8b498fbc68e.
INFO 03-02 04:10:36 metrics.py:455] Avg prompt throughput: 46.7 tokens/s, Avg generation throughput: 209.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 03-02 04:10:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 214.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 03-02 04:10:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 214.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-02 04:10:47 async_llm_engine.py:179] Finished request chatcmpl-aec1c654383c4494a2ad2b6edcbdb480.
INFO:     127.0.0.1:36210 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:10:47 logger.py:39] Received request chatcmpl-dccc4f9c35f54179ae1f94fda9e52ba4: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\nm u l e o c i s y u l t\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14880, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:10:47 async_llm_engine.py:211] Added request chatcmpl-dccc4f9c35f54179ae1f94fda9e52ba4.
INFO 03-02 04:10:51 metrics.py:455] Avg prompt throughput: 23.9 tokens/s, Avg generation throughput: 213.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 03-02 04:10:53 async_llm_engine.py:179] Finished request chatcmpl-363ca6ec13e64428989bb09cd07707ae.
INFO:     127.0.0.1:34526 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:10:53 logger.py:39] Received request chatcmpl-98f41f828cf442cca8b7bff0b92c7332: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\nk t h p e u c\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14885, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:10:53 async_llm_engine.py:211] Added request chatcmpl-98f41f828cf442cca8b7bff0b92c7332.
INFO 03-02 04:10:56 metrics.py:455] Avg prompt throughput: 23.0 tokens/s, Avg generation throughput: 213.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 03-02 04:10:59 async_llm_engine.py:179] Finished request chatcmpl-dc065a4d6a7040a69745a3389e5f581c.
INFO:     127.0.0.1:56562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:10:59 logger.py:39] Received request chatcmpl-3123329b1f51476a8e4bd91fa9d09dd4: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\nl d r a e e\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14886, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:10:59 async_llm_engine.py:211] Added request chatcmpl-3123329b1f51476a8e4bd91fa9d09dd4.
INFO 03-02 04:11:01 metrics.py:455] Avg prompt throughput: 22.8 tokens/s, Avg generation throughput: 214.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 03-02 04:11:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 215.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 03-02 04:11:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 215.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-02 04:11:16 async_llm_engine.py:179] Finished request chatcmpl-98f41f828cf442cca8b7bff0b92c7332.
INFO:     127.0.0.1:60782 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:11:16 logger.py:39] Received request chatcmpl-59a3b6f064014c13bbc891a88382e8e6: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\ne o x s e p\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14886, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:11:16 async_llm_engine.py:211] Added request chatcmpl-59a3b6f064014c13bbc891a88382e8e6.
INFO 03-02 04:11:16 metrics.py:455] Avg prompt throughput: 22.8 tokens/s, Avg generation throughput: 213.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-02 04:11:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 213.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:11:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-02 04:11:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-02 04:11:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 185.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-02 04:11:38 async_llm_engine.py:179] Finished request chatcmpl-3123329b1f51476a8e4bd91fa9d09dd4.
INFO:     127.0.0.1:38598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:11:38 logger.py:39] Received request chatcmpl-cc9b8702b5ff4c89b3d38e50c6f6a0b5: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\nt o h i r d y\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14885, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:11:38 async_llm_engine.py:211] Added request chatcmpl-cc9b8702b5ff4c89b3d38e50c6f6a0b5.
INFO 03-02 04:11:39 async_llm_engine.py:179] Finished request chatcmpl-59a3b6f064014c13bbc891a88382e8e6.
INFO:     127.0.0.1:56176 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:11:39 logger.py:39] Received request chatcmpl-d99662d5197c48c091a364d6158929ed: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\nc k c s e o l w i\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14883, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:11:39 async_llm_engine.py:211] Added request chatcmpl-d99662d5197c48c091a364d6158929ed.
INFO 03-02 04:11:41 metrics.py:455] Avg prompt throughput: 46.2 tokens/s, Avg generation throughput: 170.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-02 04:11:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 172.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-02 04:11:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 172.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-02 04:11:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 172.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-02 04:12:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 172.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-02 04:12:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 171.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-02 04:12:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 172.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-02 04:12:12 async_llm_engine.py:179] Finished request chatcmpl-fade6917b82e4156960eb8b498fbc68e.
INFO:     127.0.0.1:49380 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:12:12 logger.py:39] Received request chatcmpl-115fb8a1b9184cb08df0a2005b2097cb: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\nr n a i o z t l e i a\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14881, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:12:12 async_llm_engine.py:211] Added request chatcmpl-115fb8a1b9184cb08df0a2005b2097cb.
INFO 03-02 04:12:14 async_llm_engine.py:179] Finished request chatcmpl-3ae1dc08252e4fa7b0f75fc13bcad255.
INFO:     127.0.0.1:42838 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:12:14 logger.py:39] Received request chatcmpl-35027f17f5d4416fa86f1813833b2e57: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\nm a m d e a\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14886, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:12:14 async_llm_engine.py:211] Added request chatcmpl-35027f17f5d4416fa86f1813833b2e57.
INFO 03-02 04:12:16 metrics.py:455] Avg prompt throughput: 46.5 tokens/s, Avg generation throughput: 169.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-02 04:12:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 174.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-02 04:12:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 173.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-02 04:12:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 171.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-02 04:12:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 170.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-02 04:12:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 171.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-02 04:12:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 151.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-02 04:12:47 async_llm_engine.py:179] Finished request chatcmpl-cc9b8702b5ff4c89b3d38e50c6f6a0b5.
INFO:     127.0.0.1:48754 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:12:47 logger.py:39] Received request chatcmpl-33cb2739919246268b8939581678dbbd: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\na u e q c y d a\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14884, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:12:47 async_llm_engine.py:211] Added request chatcmpl-33cb2739919246268b8939581678dbbd.
INFO 03-02 04:12:48 async_llm_engine.py:179] Finished request chatcmpl-b8f84d5aa5804570bdd68c75b19d99db.
INFO:     127.0.0.1:54026 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:12:48 logger.py:39] Received request chatcmpl-1ded283e9c7c4d87b20744c7ef85fab7: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\nl e a t x\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14887, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:12:48 async_llm_engine.py:211] Added request chatcmpl-1ded283e9c7c4d87b20744c7ef85fab7.
INFO 03-02 04:12:49 async_llm_engine.py:179] Finished request chatcmpl-35027f17f5d4416fa86f1813833b2e57.
INFO:     127.0.0.1:49854 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:12:49 logger.py:39] Received request chatcmpl-27fcc1fad92d4089bb83d3dbdde5ba6a: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\nd c e t i a t\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14885, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:12:49 async_llm_engine.py:211] Added request chatcmpl-27fcc1fad92d4089bb83d3dbdde5ba6a.
INFO 03-02 04:12:51 metrics.py:455] Avg prompt throughput: 68.7 tokens/s, Avg generation throughput: 191.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 03-02 04:12:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 214.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 03-02 04:12:58 async_llm_engine.py:179] Finished request chatcmpl-1ded283e9c7c4d87b20744c7ef85fab7.
INFO:     127.0.0.1:42654 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:12:58 logger.py:39] Received request chatcmpl-28dd3e4b1d18449d9ad72543cce0e859: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\nh v l e a\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14887, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:12:58 async_llm_engine.py:211] Added request chatcmpl-28dd3e4b1d18449d9ad72543cce0e859.
INFO 03-02 04:13:01 metrics.py:455] Avg prompt throughput: 22.6 tokens/s, Avg generation throughput: 214.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 03-02 04:13:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 213.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-02 04:13:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-02 04:13:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:13:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-02 04:13:26 async_llm_engine.py:179] Finished request chatcmpl-115fb8a1b9184cb08df0a2005b2097cb.
INFO:     127.0.0.1:49852 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:13:26 logger.py:39] Received request chatcmpl-28a7f29467514b609f1220a10e3aa5c0: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\nc u y u i o l r s\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14883, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:13:26 async_llm_engine.py:211] Added request chatcmpl-28a7f29467514b609f1220a10e3aa5c0.
INFO 03-02 04:13:26 async_llm_engine.py:179] Finished request chatcmpl-31d0eda936bf479a8258805df8a91a44.
INFO:     127.0.0.1:48136 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:13:26 logger.py:39] Received request chatcmpl-a545bf19f6b247029e804a2fb7f3899e: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\ns n a g l\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14887, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:13:26 async_llm_engine.py:211] Added request chatcmpl-a545bf19f6b247029e804a2fb7f3899e.
INFO 03-02 04:13:26 metrics.py:455] Avg prompt throughput: 45.9 tokens/s, Avg generation throughput: 208.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 03-02 04:13:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 214.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 03-02 04:13:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 213.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-02 04:13:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:13:44 async_llm_engine.py:179] Finished request chatcmpl-28dd3e4b1d18449d9ad72543cce0e859.
INFO:     127.0.0.1:35346 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:13:44 logger.py:39] Received request chatcmpl-551e5bfec1e849999e4eea87b26c328e: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\np t r p i a i n a t c\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14881, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:13:44 async_llm_engine.py:211] Added request chatcmpl-551e5bfec1e849999e4eea87b26c328e.
INFO 03-02 04:13:46 metrics.py:455] Avg prompt throughput: 23.8 tokens/s, Avg generation throughput: 209.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:13:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:13:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 210.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-02 04:13:59 async_llm_engine.py:179] Finished request chatcmpl-3cc798e2d9394daa9c935f36df963667.
INFO:     127.0.0.1:42852 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:13:59 logger.py:39] Received request chatcmpl-f7efd89de85f455fa0478f1ad2dd3375: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\nw k l e a r\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14886, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:13:59 async_llm_engine.py:211] Added request chatcmpl-f7efd89de85f455fa0478f1ad2dd3375.
INFO 03-02 04:14:01 metrics.py:455] Avg prompt throughput: 22.8 tokens/s, Avg generation throughput: 209.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 03-02 04:14:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 214.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 03-02 04:14:08 async_llm_engine.py:179] Finished request chatcmpl-a545bf19f6b247029e804a2fb7f3899e.
INFO:     127.0.0.1:59144 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:14:08 logger.py:39] Received request chatcmpl-ec3f3b4691f542429680c448daf6efc3: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\nd c s u e b t i r n a\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14881, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:14:08 async_llm_engine.py:211] Added request chatcmpl-ec3f3b4691f542429680c448daf6efc3.
INFO 03-02 04:14:11 metrics.py:455] Avg prompt throughput: 23.7 tokens/s, Avg generation throughput: 212.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 03-02 04:14:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 213.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 03-02 04:14:17 async_llm_engine.py:179] Finished request chatcmpl-551e5bfec1e849999e4eea87b26c328e.
INFO:     127.0.0.1:56396 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:14:17 logger.py:39] Received request chatcmpl-a6b82fc396dd42ae99b73bd54120a7d2: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\nc s e t m i m t i s h a r\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14879, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:14:17 async_llm_engine.py:211] Added request chatcmpl-a6b82fc396dd42ae99b73bd54120a7d2.
INFO 03-02 04:14:20 async_llm_engine.py:179] Finished request chatcmpl-d99662d5197c48c091a364d6158929ed.
INFO:     127.0.0.1:48762 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:14:20 logger.py:39] Received request chatcmpl-83e4fff1a0a14f84ad48dd656685014c: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\nb d e k a\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14887, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:14:20 async_llm_engine.py:211] Added request chatcmpl-83e4fff1a0a14f84ad48dd656685014c.
INFO 03-02 04:14:21 async_llm_engine.py:179] Finished request chatcmpl-f7efd89de85f455fa0478f1ad2dd3375.
INFO:     127.0.0.1:53836 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:14:21 logger.py:39] Received request chatcmpl-b98cf75ab83f4bfdbe36bed716e2c068: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\np r e e t a e t p r\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14882, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:14:21 async_llm_engine.py:211] Added request chatcmpl-b98cf75ab83f4bfdbe36bed716e2c068.
INFO 03-02 04:14:21 metrics.py:455] Avg prompt throughput: 70.3 tokens/s, Avg generation throughput: 211.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 03-02 04:14:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 218.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 03-02 04:14:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 217.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 03-02 04:14:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 215.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 03-02 04:14:37 async_llm_engine.py:179] Finished request chatcmpl-83e4fff1a0a14f84ad48dd656685014c.
INFO:     127.0.0.1:56732 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:14:37 logger.py:39] Received request chatcmpl-afad3887e8cf455798053ed5f94523b2: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nPlease move the letters to get to the original word for this anagram. The first letter is already correct.\na p e e t c r p a i\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14882, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 04:14:37 async_llm_engine.py:211] Added request chatcmpl-afad3887e8cf455798053ed5f94523b2.
INFO 03-02 04:14:41 metrics.py:455] Avg prompt throughput: 23.5 tokens/s, Avg generation throughput: 213.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 03-02 04:14:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 213.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-02 04:14:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-02 04:14:53 async_llm_engine.py:179] Finished request chatcmpl-dccc4f9c35f54179ae1f94fda9e52ba4.
INFO:     127.0.0.1:60778 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:14:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 196.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 03-02 04:15:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 03-02 04:15:04 async_llm_engine.py:179] Finished request chatcmpl-27fcc1fad92d4089bb83d3dbdde5ba6a.
INFO:     127.0.0.1:42660 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:15:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 179.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 03-02 04:15:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 166.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 03-02 04:15:12 async_llm_engine.py:179] Finished request chatcmpl-afad3887e8cf455798053ed5f94523b2.
INFO:     127.0.0.1:38980 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:15:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 142.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 03-02 04:15:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 140.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 03-02 04:15:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 139.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 03-02 04:15:30 async_llm_engine.py:179] Finished request chatcmpl-33cb2739919246268b8939581678dbbd.
INFO:     127.0.0.1:42644 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:15:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 131.5 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 03-02 04:15:32 async_llm_engine.py:179] Finished request chatcmpl-a6b82fc396dd42ae99b73bd54120a7d2.
INFO:     127.0.0.1:56718 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:15:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.5 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 03-02 04:15:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 85.8 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 03-02 04:15:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 86.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 03-02 04:15:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 86.9 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 03-02 04:15:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 86.3 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 03-02 04:16:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 86.3 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 03-02 04:16:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 86.0 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 03-02 04:16:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 85.3 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 03-02 04:16:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 85.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 03-02 04:16:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 84.5 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 03-02 04:16:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 83.8 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 03-02 04:16:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 83.7 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 03-02 04:16:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 83.6 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 03-02 04:16:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 83.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 03-02 04:16:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 84.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 03-02 04:16:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 83.9 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 03-02 04:16:52 async_llm_engine.py:179] Finished request chatcmpl-28a7f29467514b609f1220a10e3aa5c0.
INFO:     127.0.0.1:59132 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:16:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 03-02 04:16:59 async_llm_engine.py:179] Finished request chatcmpl-b98cf75ab83f4bfdbe36bed716e2c068.
INFO:     127.0.0.1:56744 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:17:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 35.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-02 04:17:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-02 04:17:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-02 04:17:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-02 04:17:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:17:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:17:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:17:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:17:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:17:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:17:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:17:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:18:03 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:18:08 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:18:13 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:18:18 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:18:23 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:18:28 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:18:33 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:18:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:18:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:18:48 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:18:53 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:18:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:19:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:19:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:19:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:19:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:19:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:19:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:19:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:19:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:19:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:19:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:19:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:19:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 04:20:01 async_llm_engine.py:179] Finished request chatcmpl-ec3f3b4691f542429680c448daf6efc3.
INFO:     127.0.0.1:36574 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 04:20:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
