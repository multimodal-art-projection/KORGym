INFO 02-28 20:27:43 __init__.py:207] Automatically detected platform cuda.
INFO 02-28 20:27:43 api_server.py:912] vLLM API server version 0.7.3
INFO 02-28 20:27:43 api_server.py:913] args: Namespace(host=None, port=9003, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='/map-vepfs/models/jiajun/Qwen/Qwen2.5-72B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=15000, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=2, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.95, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['Qwen2.5-72B-Instruct'], qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False)
INFO 02-28 20:27:50 config.py:549] This model supports multiple tasks: {'reward', 'generate', 'embed', 'score', 'classify'}. Defaulting to 'generate'.
INFO 02-28 20:27:50 config.py:1382] Defaulting to use mp for distributed inference
WARNING 02-28 20:27:50 config.py:676] Async output processing can not be enabled with pipeline parallel
INFO 02-28 20:27:50 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/map-vepfs/models/jiajun/Qwen/Qwen2.5-72B-Instruct', speculative_config=None, tokenizer='/map-vepfs/models/jiajun/Qwen/Qwen2.5-72B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=15000, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen2.5-72B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
WARNING 02-28 20:27:51 multiproc_worker_utils.py:300] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 02-28 20:27:51 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 02-28 20:27:51 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 02-28 20:27:51 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 02-28 20:27:51 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 02-28 20:27:51 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-28 20:27:51 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-28 20:27:51 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-28 20:27:51 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 02-28 20:27:52 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-28 20:27:52 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-28 20:27:52 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 02-28 20:27:52 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-28 20:27:52 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 02-28 20:27:52 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 02-28 20:27:52 cuda.py:229] Using Flash Attention backend.
INFO 02-28 20:27:52 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-28 20:27:54 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-28 20:27:54 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-28 20:27:54 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 02-28 20:27:54 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-28 20:27:54 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-28 20:27:54 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 02-28 20:27:54 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-28 20:27:54 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 02-28 20:27:54 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 02-28 20:27:54 utils.py:916] Found nccl from library libnccl.so.2
INFO 02-28 20:27:54 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 02-28 20:27:54 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 02-28 20:27:54 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 02-28 20:27:54 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 02-28 20:27:54 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 02-28 20:27:54 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 02-28 20:27:55 custom_all_reduce_utils.py:206] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
INFO 02-28 20:28:19 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 02-28 20:28:19 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-28 20:28:19 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 02-28 20:28:19 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 02-28 20:28:19 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 02-28 20:28:19 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-28 20:28:19 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-28 20:28:19 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 02-28 20:28:19 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_63057bc7'), local_subscribe_port=56567, remote_subscribe_port=None)
INFO 02-28 20:28:19 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_e237c3f7'), local_subscribe_port=56123, remote_subscribe_port=None)
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-28 20:28:19 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 02-28 20:28:19 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-28 20:28:19 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 02-28 20:28:19 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 02-28 20:28:19 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-28 20:28:19 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 02-28 20:28:19 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 02-28 20:28:19 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-28 20:28:19 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-28 20:28:19 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 02-28 20:28:19 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 02-28 20:28:19 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 02-28 20:28:19 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-28 20:28:19 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 02-28 20:28:19 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 02-28 20:28:19 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 02-28 20:28:20 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/Qwen/Qwen2.5-72B-Instruct...
INFO 02-28 20:28:20 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/Qwen/Qwen2.5-72B-Instruct...
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 02-28 20:28:20 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/Qwen/Qwen2.5-72B-Instruct...
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 02-28 20:28:20 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/Qwen/Qwen2.5-72B-Instruct...
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-28 20:28:20 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/Qwen/Qwen2.5-72B-Instruct...
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 02-28 20:28:20 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/Qwen/Qwen2.5-72B-Instruct...
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-28 20:28:20 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/Qwen/Qwen2.5-72B-Instruct...
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-28 20:28:20 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/Qwen/Qwen2.5-72B-Instruct...
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-28 20:29:49 model_runner.py:1115] Loading model weights took 16.9956 GB
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 02-28 20:29:49 model_runner.py:1115] Loading model weights took 16.9956 GB
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-28 20:29:49 model_runner.py:1115] Loading model weights took 16.9956 GB
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-28 20:29:49 model_runner.py:1115] Loading model weights took 16.9956 GB
INFO 02-28 20:29:50 model_runner.py:1115] Loading model weights took 16.9956 GB
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 02-28 20:29:50 model_runner.py:1115] Loading model weights took 16.9956 GB
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 02-28 20:29:50 model_runner.py:1115] Loading model weights took 16.9956 GB
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 02-28 20:29:50 model_runner.py:1115] Loading model weights took 16.9956 GB
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-28 20:29:53 worker.py:267] Memory profiling takes 2.47 seconds
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-28 20:29:53 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-28 20:29:53 worker.py:267] model weights take 17.00GiB; non_torch_memory takes 1.70GiB; PyTorch activation peak memory takes 2.00GiB; the rest of the memory reserved for KV Cache is 54.69GiB.
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-28 20:29:53 worker.py:267] Memory profiling takes 2.46 seconds
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-28 20:29:53 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-28 20:29:53 worker.py:267] model weights take 17.00GiB; non_torch_memory takes 1.70GiB; PyTorch activation peak memory takes 2.00GiB; the rest of the memory reserved for KV Cache is 54.69GiB.
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-28 20:29:53 worker.py:267] Memory profiling takes 2.48 seconds
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-28 20:29:53 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-28 20:29:53 worker.py:267] model weights take 17.00GiB; non_torch_memory takes 1.51GiB; PyTorch activation peak memory takes 2.00GiB; the rest of the memory reserved for KV Cache is 54.87GiB.
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 02-28 20:29:53 worker.py:267] Memory profiling takes 2.61 seconds
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 02-28 20:29:53 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 02-28 20:29:53 worker.py:267] model weights take 17.00GiB; non_torch_memory takes 1.70GiB; PyTorch activation peak memory takes 2.07GiB; the rest of the memory reserved for KV Cache is 54.61GiB.
INFO 02-28 20:29:56 worker.py:267] Memory profiling takes 5.24 seconds
INFO 02-28 20:29:56 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
INFO 02-28 20:29:56 worker.py:267] model weights take 17.00GiB; non_torch_memory takes 0.94GiB; PyTorch activation peak memory takes 1.77GiB; the rest of the memory reserved for KV Cache is 55.67GiB.
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 02-28 20:29:56 worker.py:267] Memory profiling takes 5.23 seconds
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 02-28 20:29:56 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 02-28 20:29:56 worker.py:267] model weights take 17.00GiB; non_torch_memory takes 1.03GiB; PyTorch activation peak memory takes 1.77GiB; the rest of the memory reserved for KV Cache is 55.58GiB.
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 02-28 20:29:56 worker.py:267] Memory profiling takes 5.28 seconds
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 02-28 20:29:56 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 02-28 20:29:56 worker.py:267] model weights take 17.00GiB; non_torch_memory takes 0.94GiB; PyTorch activation peak memory takes 1.77GiB; the rest of the memory reserved for KV Cache is 55.67GiB.
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 02-28 20:29:56 worker.py:267] Memory profiling takes 5.30 seconds
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 02-28 20:29:56 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 02-28 20:29:56 worker.py:267] model weights take 17.00GiB; non_torch_memory takes 1.03GiB; PyTorch activation peak memory takes 1.77GiB; the rest of the memory reserved for KV Cache is 55.58GiB.
INFO 02-28 20:29:56 executor_base.py:111] # cuda blocks: 89477, # CPU blocks: 6553
INFO 02-28 20:29:56 executor_base.py:116] Maximum concurrency for 15000 tokens per request: 95.44x
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 02-28 20:29:59 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 02-28 20:29:59 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-28 20:29:59 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-28 20:29:59 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 02-28 20:29:59 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 02-28 20:29:59 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-28 20:29:59 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 02-28 20:29:59 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-28 20:30:32 custom_all_reduce.py:226] Registering 5600 cuda graph addresses
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 02-28 20:30:32 custom_all_reduce.py:226] Registering 5600 cuda graph addresses
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-28 20:30:32 custom_all_reduce.py:226] Registering 5600 cuda graph addresses
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-28 20:30:33 custom_all_reduce.py:226] Registering 5600 cuda graph addresses
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-28 20:30:33 model_runner.py:1562] Graph capturing finished in 34 secs, took 2.32 GiB
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-28 20:30:33 model_runner.py:1562] Graph capturing finished in 34 secs, took 2.32 GiB
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 02-28 20:30:33 model_runner.py:1562] Graph capturing finished in 34 secs, took 2.32 GiB
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-28 20:30:33 model_runner.py:1562] Graph capturing finished in 34 secs, took 2.32 GiB
INFO 02-28 20:30:35 custom_all_reduce.py:226] Registering 5670 cuda graph addresses
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 02-28 20:30:35 custom_all_reduce.py:226] Registering 5670 cuda graph addresses
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 02-28 20:30:35 custom_all_reduce.py:226] Registering 5670 cuda graph addresses
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 02-28 20:30:36 custom_all_reduce.py:226] Registering 5670 cuda graph addresses
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 02-28 20:30:36 model_runner.py:1562] Graph capturing finished in 36 secs, took 2.35 GiB
INFO 02-28 20:30:36 model_runner.py:1562] Graph capturing finished in 36 secs, took 2.35 GiB
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 02-28 20:30:36 model_runner.py:1562] Graph capturing finished in 36 secs, took 2.35 GiB
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 02-28 20:30:36 model_runner.py:1562] Graph capturing finished in 36 secs, took 2.35 GiB
INFO 02-28 20:30:36 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 45.20 seconds
INFO 02-28 20:30:37 api_server.py:958] Starting vLLM API server on http://0.0.0.0:9003
INFO 02-28 20:30:37 launcher.py:23] Available routes are:
INFO 02-28 20:30:37 launcher.py:31] Route: /openapi.json, Methods: GET, HEAD
INFO 02-28 20:30:37 launcher.py:31] Route: /docs, Methods: GET, HEAD
INFO 02-28 20:30:37 launcher.py:31] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 02-28 20:30:37 launcher.py:31] Route: /redoc, Methods: GET, HEAD
INFO 02-28 20:30:37 launcher.py:31] Route: /health, Methods: GET
INFO 02-28 20:30:37 launcher.py:31] Route: /ping, Methods: GET, POST
INFO 02-28 20:30:37 launcher.py:31] Route: /tokenize, Methods: POST
INFO 02-28 20:30:37 launcher.py:31] Route: /detokenize, Methods: POST
INFO 02-28 20:30:37 launcher.py:31] Route: /v1/models, Methods: GET
INFO 02-28 20:30:37 launcher.py:31] Route: /version, Methods: GET
INFO 02-28 20:30:37 launcher.py:31] Route: /v1/chat/completions, Methods: POST
INFO 02-28 20:30:37 launcher.py:31] Route: /v1/completions, Methods: POST
INFO 02-28 20:30:37 launcher.py:31] Route: /v1/embeddings, Methods: POST
INFO 02-28 20:30:37 launcher.py:31] Route: /pooling, Methods: POST
INFO 02-28 20:30:37 launcher.py:31] Route: /score, Methods: POST
INFO 02-28 20:30:37 launcher.py:31] Route: /v1/score, Methods: POST
INFO 02-28 20:30:37 launcher.py:31] Route: /v1/audio/transcriptions, Methods: POST
INFO 02-28 20:30:37 launcher.py:31] Route: /rerank, Methods: POST
INFO 02-28 20:30:37 launcher.py:31] Route: /v1/rerank, Methods: POST
INFO 02-28 20:30:37 launcher.py:31] Route: /v2/rerank, Methods: POST
INFO 02-28 20:30:37 launcher.py:31] Route: /invocations, Methods: POST
INFO 02-28 20:34:29 chat_utils.py:332] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
INFO 02-28 20:34:29 logger.py:39] Received request chatcmpl-dfdb972f49714e7b9713fcbb6bbaa7f1: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐋🐋🚀🐋\n🚀🌳🌳🚀\n🌳🦩🚀🌳\n🚀🦩🦩🚀\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14814, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:29 async_llm_engine.py:211] Added request chatcmpl-dfdb972f49714e7b9713fcbb6bbaa7f1.
INFO 02-28 20:34:29 logger.py:39] Received request chatcmpl-3a9238b5def74dc8b416305cc40e1975: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🦅🐳🐳🐳\n🐳🌞🦅🐳\n🦅🐳🐳🦅\n🐳🐰🌞🦅\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:29 async_llm_engine.py:211] Added request chatcmpl-3a9238b5def74dc8b416305cc40e1975.
INFO 02-28 20:34:29 logger.py:39] Received request chatcmpl-fd56651be92440978360519ea21234df: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐮🐯🐯🐮\n🐭🐮🐈🐈\n🐯🐈🐯🐭\n🐭🐯🐈🐯\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:29 async_llm_engine.py:211] Added request chatcmpl-fd56651be92440978360519ea21234df.
INFO 02-28 20:34:29 logger.py:39] Received request chatcmpl-25b25171588b4818b22f949e50d1bdc3: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🌑🦆🦀🦀\n🦆🌑☄️☄️\n🦆🦆🦆☄️\n☄️☄️🦆🦀\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14812, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:29 async_llm_engine.py:211] Added request chatcmpl-25b25171588b4818b22f949e50d1bdc3.
INFO 02-28 20:34:29 logger.py:39] Received request chatcmpl-247c04a474fd41c6bb97f6d8a4fdd582: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🦥🐫🦀🦀\n🦀🦥🐵🦀\n🐫🐵🐵🐫\n🦀🐵🐫🦀\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14815, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:29 async_llm_engine.py:211] Added request chatcmpl-247c04a474fd41c6bb97f6d8a4fdd582.
INFO 02-28 20:34:29 logger.py:39] Received request chatcmpl-a7befb65635b40a7ae7373875d4cb1dd: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n💧🌕🐳💧\n🐳💧🦮🌕\n🐳🌕💧🐳\n💧🐳🌕🦮\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14815, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:29 async_llm_engine.py:211] Added request chatcmpl-a7befb65635b40a7ae7373875d4cb1dd.
INFO 02-28 20:34:29 logger.py:39] Received request chatcmpl-456dd6a2cf414c9aa521d510966ff77b: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🌙🌙🐨🐋\n🌼🌼🌙🌼\n🐋🐨🐋🌙\n🐨🌼🌙🐋\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:29 async_llm_engine.py:211] Added request chatcmpl-456dd6a2cf414c9aa521d510966ff77b.
INFO 02-28 20:34:29 logger.py:39] Received request chatcmpl-0e934feca40b4b78b763d1486dafa54b: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐄🐄🦥🐄\n🐗🐄🐄🐶\n🐶🐄🐗🐄\n🐶🐄🐄🐗\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14816, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:29 async_llm_engine.py:211] Added request chatcmpl-0e934feca40b4b78b763d1486dafa54b.
INFO 02-28 20:34:33 metrics.py:455] Avg prompt throughput: 294.7 tokens/s, Avg generation throughput: 103.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-28 20:34:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 226.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-28 20:34:38 async_llm_engine.py:179] Finished request chatcmpl-fd56651be92440978360519ea21234df.
INFO:     127.0.0.1:36448 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:34:38 logger.py:39] Received request chatcmpl-520e59527e1a406c93a3b37df7ce2602: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🦃🦐🦐🦃\n🦃🦃🐥🦐\n🐥🐥🐥🐥\n🐥🦃🐥🦐\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:38 async_llm_engine.py:211] Added request chatcmpl-520e59527e1a406c93a3b37df7ce2602.
INFO 02-28 20:34:39 async_llm_engine.py:179] Finished request chatcmpl-25b25171588b4818b22f949e50d1bdc3.
INFO:     127.0.0.1:36464 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:34:39 logger.py:39] Received request chatcmpl-c5ae54fa7a74467a99a0285e461e204a: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n⛅️⛅️🌾🦚\n🐊🌾🦚🌾\n🐊⛅️🐊🐊\n⛅️⛅️⛅️🌾\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14811, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:39 async_llm_engine.py:211] Added request chatcmpl-c5ae54fa7a74467a99a0285e461e204a.
INFO 02-28 20:34:40 async_llm_engine.py:179] Finished request chatcmpl-3a9238b5def74dc8b416305cc40e1975.
INFO:     127.0.0.1:36438 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:34:40 logger.py:39] Received request chatcmpl-51fd5f0e339e4ccaad09530493176dcb: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🌙🧽🌷🌷\n🌱🧽🌙🌷\n🌱🌙🧽🌱\n🌙🌷🧽🌱\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14813, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:40 async_llm_engine.py:211] Added request chatcmpl-51fd5f0e339e4ccaad09530493176dcb.
INFO 02-28 20:34:40 async_llm_engine.py:179] Finished request chatcmpl-a7befb65635b40a7ae7373875d4cb1dd.
INFO:     127.0.0.1:36488 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:34:40 logger.py:39] Received request chatcmpl-5c6b621fefd0410aa3fb56c7273d7fd4: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🌚🌚🐚🌚\n🍃🐚🌺🌚\n🍃🍃🐚🐚\n🌚🍃🍃🍃\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:40 async_llm_engine.py:211] Added request chatcmpl-5c6b621fefd0410aa3fb56c7273d7fd4.
INFO 02-28 20:34:42 async_llm_engine.py:179] Finished request chatcmpl-dfdb972f49714e7b9713fcbb6bbaa7f1.
INFO:     127.0.0.1:36424 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:34:42 logger.py:39] Received request chatcmpl-7342b11fd52247b0a2177ca10935f33f: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐊🐕🌹🌕\n🐕🌕🐕🐊\n🌹🐊🐕🌕\n🐊🌕🐊🌹\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:42 async_llm_engine.py:211] Added request chatcmpl-7342b11fd52247b0a2177ca10935f33f.
INFO 02-28 20:34:43 metrics.py:455] Avg prompt throughput: 184.6 tokens/s, Avg generation throughput: 214.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-28 20:34:44 async_llm_engine.py:179] Finished request chatcmpl-0e934feca40b4b78b763d1486dafa54b.
INFO:     127.0.0.1:36496 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:34:44 logger.py:39] Received request chatcmpl-61795192cf6d40e1b51711aaa8083142: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🦏🦏🦏🦏\n🐋🦏🦟🐋\n🦞🦏🐋🦟\n🦏🐋🦟🦞\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14805, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:44 async_llm_engine.py:211] Added request chatcmpl-61795192cf6d40e1b51711aaa8083142.
INFO 02-28 20:34:48 metrics.py:455] Avg prompt throughput: 38.9 tokens/s, Avg generation throughput: 223.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-28 20:34:49 async_llm_engine.py:179] Finished request chatcmpl-520e59527e1a406c93a3b37df7ce2602.
INFO:     127.0.0.1:39026 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:34:49 logger.py:39] Received request chatcmpl-1e1d6f43c4ef4c72a70ba330e4c918de: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🌏🌏🌏🐔\n🐟🌏🐟🐟\n🐟🐔🌏☁️\n🌏🌏🌏🌏\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14816, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:49 async_llm_engine.py:211] Added request chatcmpl-1e1d6f43c4ef4c72a70ba330e4c918de.
INFO 02-28 20:34:49 async_llm_engine.py:179] Finished request chatcmpl-247c04a474fd41c6bb97f6d8a4fdd582.
INFO:     127.0.0.1:36476 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:34:49 logger.py:39] Received request chatcmpl-0af254cae81d48aaba14b3c4f052f697: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n😄😄🦎🦎\n😄🌍😄🦎\n🌍🤚🌍🌍\n🤚🌍🌍😄\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:49 async_llm_engine.py:211] Added request chatcmpl-0af254cae81d48aaba14b3c4f052f697.
INFO 02-28 20:34:52 async_llm_engine.py:179] Finished request chatcmpl-7342b11fd52247b0a2177ca10935f33f.
INFO:     127.0.0.1:39064 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:34:52 async_llm_engine.py:179] Finished request chatcmpl-c5ae54fa7a74467a99a0285e461e204a.
INFO:     127.0.0.1:39034 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:34:52 logger.py:39] Received request chatcmpl-17bc33e0eddc409c8b77950574c1f3f1: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🦍🌻🦍🐈\n🦍🌷🌻🌻\n🐈🌷🌷🌷\n🌻🌷🐈🌻\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:52 async_llm_engine.py:211] Added request chatcmpl-17bc33e0eddc409c8b77950574c1f3f1.
INFO 02-28 20:34:52 logger.py:39] Received request chatcmpl-497c1fbee269448a98795cf64ef0fabf: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐫🌲🐫🌍\n🌍🌲🐓🐓\n🐫🐫🐓🌲\n🌍🌲🌲🌲\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:52 async_llm_engine.py:211] Added request chatcmpl-497c1fbee269448a98795cf64ef0fabf.
INFO 02-28 20:34:53 metrics.py:455] Avg prompt throughput: 146.3 tokens/s, Avg generation throughput: 216.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-28 20:34:53 async_llm_engine.py:179] Finished request chatcmpl-51fd5f0e339e4ccaad09530493176dcb.
INFO:     127.0.0.1:39046 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:34:53 logger.py:39] Received request chatcmpl-9d97fdd40ec54f4999c8f2f6103835d6: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐣🐣🐏🐏\n🐣🐏🎋🐏\n🎋🐣🎋🐜\n🎋🐣🐣🐣\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14813, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:53 async_llm_engine.py:211] Added request chatcmpl-9d97fdd40ec54f4999c8f2f6103835d6.
INFO 02-28 20:34:53 async_llm_engine.py:179] Finished request chatcmpl-456dd6a2cf414c9aa521d510966ff77b.
INFO:     127.0.0.1:36494 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:34:53 logger.py:39] Received request chatcmpl-898c61159c2b4d3fa6849dd5ea91372f: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🌍🐚🐤🐤\n🌍🐤🎁🐤\n🐚🐤🐤🎁\n🐤🐤🎁🌍\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:53 async_llm_engine.py:211] Added request chatcmpl-898c61159c2b4d3fa6849dd5ea91372f.
INFO 02-28 20:34:53 async_llm_engine.py:179] Finished request chatcmpl-61795192cf6d40e1b51711aaa8083142.
INFO:     127.0.0.1:39078 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:34:53 logger.py:39] Received request chatcmpl-b20d952a392d496fbcca9e125552fbda: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐋🐗🐎🐎\n🐗🐗🐋🐸\n🐎🐎🐎🐋\n🐋🐸🐸🐸\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:53 async_llm_engine.py:211] Added request chatcmpl-b20d952a392d496fbcca9e125552fbda.
INFO 02-28 20:34:58 metrics.py:455] Avg prompt throughput: 110.3 tokens/s, Avg generation throughput: 219.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-28 20:34:59 async_llm_engine.py:179] Finished request chatcmpl-0af254cae81d48aaba14b3c4f052f697.
INFO:     127.0.0.1:40086 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:34:59 logger.py:39] Received request chatcmpl-f7ae13b783d4457ba0397d03fb871932: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🌳🥀🌳🌳\n🐛🐛🦍🥀\n🐛🌳🌳🌳\n🐛🥀🥀🥀\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:59 async_llm_engine.py:211] Added request chatcmpl-f7ae13b783d4457ba0397d03fb871932.
INFO 02-28 20:34:59 async_llm_engine.py:179] Finished request chatcmpl-5c6b621fefd0410aa3fb56c7273d7fd4.
INFO:     127.0.0.1:39048 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:34:59 logger.py:39] Received request chatcmpl-6dc7f271bcd746d78fcf8826598929e8: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n⛅️🐠🦉🏄\n🦉🐠🏄🦉\n🏄🐠⛅️🦉\n🦉🏄🏄⛅️\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14814, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:59 async_llm_engine.py:211] Added request chatcmpl-6dc7f271bcd746d78fcf8826598929e8.
INFO 02-28 20:35:01 async_llm_engine.py:179] Finished request chatcmpl-1e1d6f43c4ef4c72a70ba330e4c918de.
INFO:     127.0.0.1:40082 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:01 logger.py:39] Received request chatcmpl-a76ca81390a94102804afb743a59cdfa: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n😆☄️☄️😆\n🐯🐯😆☄️\n🦏🐯☄️🦏\n🦏🦏☄️☄️\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14807, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:01 async_llm_engine.py:211] Added request chatcmpl-a76ca81390a94102804afb743a59cdfa.
INFO 02-28 20:35:03 metrics.py:455] Avg prompt throughput: 112.3 tokens/s, Avg generation throughput: 219.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-28 20:35:03 async_llm_engine.py:179] Finished request chatcmpl-17bc33e0eddc409c8b77950574c1f3f1.
INFO:     127.0.0.1:40090 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:03 logger.py:39] Received request chatcmpl-01c077312c744ed9a4e7028a6c2616ca: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n💫💫💫🦩\n💫🐜🦩🦕\n🦕🦩🦩💫\n🦕🦕🐜🦕\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14813, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:03 async_llm_engine.py:211] Added request chatcmpl-01c077312c744ed9a4e7028a6c2616ca.
INFO 02-28 20:35:03 async_llm_engine.py:179] Finished request chatcmpl-b20d952a392d496fbcca9e125552fbda.
INFO:     127.0.0.1:40136 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:03 logger.py:39] Received request chatcmpl-94a29f78abed40e0885424b1f7ba5cf5: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🦒🦜🦖🦜\n🦖🦒🦜🦜\n😁🦖😁🦖\n🦒🦜🦜🦖\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14811, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:03 async_llm_engine.py:211] Added request chatcmpl-94a29f78abed40e0885424b1f7ba5cf5.
INFO 02-28 20:35:04 async_llm_engine.py:179] Finished request chatcmpl-497c1fbee269448a98795cf64ef0fabf.
INFO:     127.0.0.1:40102 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:04 async_llm_engine.py:179] Finished request chatcmpl-898c61159c2b4d3fa6849dd5ea91372f.
INFO:     127.0.0.1:40120 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:04 logger.py:39] Received request chatcmpl-926f9055d52b4cddbc76c8a9bcfc984a: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐢🐍💥🐢\n🐍💥🐍💥\n🐍🐢🐍🐍\n🐍🐍💥💥\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:04 async_llm_engine.py:211] Added request chatcmpl-926f9055d52b4cddbc76c8a9bcfc984a.
INFO 02-28 20:35:04 logger.py:39] Received request chatcmpl-7f22aaceafa5424d87b90ff448f3e9a1: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🥀🥀🐘🐘\n🐘🐘🦂🐆\n🐆🐆🦂🥀\n🥀🦂🦂🥀\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14813, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:04 async_llm_engine.py:211] Added request chatcmpl-7f22aaceafa5424d87b90ff448f3e9a1.
INFO 02-28 20:35:08 metrics.py:455] Avg prompt throughput: 148.9 tokens/s, Avg generation throughput: 218.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-28 20:35:08 async_llm_engine.py:179] Finished request chatcmpl-9d97fdd40ec54f4999c8f2f6103835d6.
INFO:     127.0.0.1:40106 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:08 logger.py:39] Received request chatcmpl-85c33ded99854823a98ce73dddd0399c: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🦆🦆🦆🔥\n🔥🦆🦆🦆\n🔥🦆🦆🦆\n🦆🐧🔥🦆\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:08 async_llm_engine.py:211] Added request chatcmpl-85c33ded99854823a98ce73dddd0399c.
INFO 02-28 20:35:08 async_llm_engine.py:179] Finished request chatcmpl-f7ae13b783d4457ba0397d03fb871932.
INFO:     127.0.0.1:44230 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:08 logger.py:39] Received request chatcmpl-38b20fc259af4b13a0b7bc3ccaa9fbf1: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐐🌒🐐💥\n💥🌒🌒🌒\n💥🐼🐐🌒\n💥🐐💥🐼\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14812, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:08 async_llm_engine.py:211] Added request chatcmpl-38b20fc259af4b13a0b7bc3ccaa9fbf1.
INFO 02-28 20:35:09 async_llm_engine.py:179] Finished request chatcmpl-6dc7f271bcd746d78fcf8826598929e8.
INFO:     127.0.0.1:44240 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:09 logger.py:39] Received request chatcmpl-1f4983115df74e0b96bc57d31c8b3528: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🦏⛅️🌘🦭\n🦭🦏🌘🦭\n🌘🦏🌘🌘\n🦏🦭⛅️🌘\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14807, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:09 async_llm_engine.py:211] Added request chatcmpl-1f4983115df74e0b96bc57d31c8b3528.
INFO 02-28 20:35:13 metrics.py:455] Avg prompt throughput: 112.6 tokens/s, Avg generation throughput: 219.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-28 20:35:14 async_llm_engine.py:179] Finished request chatcmpl-7f22aaceafa5424d87b90ff448f3e9a1.
INFO:     127.0.0.1:44300 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:14 logger.py:39] Received request chatcmpl-b752e8fe7d2941ef9265d99f635975d8: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🌻😁🌻😁\n🌻🌻🌻😁\n😁🌻🌻🦦\n🦦🌻😁😁\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14815, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:14 async_llm_engine.py:211] Added request chatcmpl-b752e8fe7d2941ef9265d99f635975d8.
INFO 02-28 20:35:14 async_llm_engine.py:179] Finished request chatcmpl-01c077312c744ed9a4e7028a6c2616ca.
INFO:     127.0.0.1:44270 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:14 logger.py:39] Received request chatcmpl-965575d9686049c69150754a5b512cc4: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🦖🐫🐼🐼\n🐼🐺🐺🐼\n🐫🐺🐼🦖\n🐼🐫🦖🐺\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:14 async_llm_engine.py:211] Added request chatcmpl-965575d9686049c69150754a5b512cc4.
INFO 02-28 20:35:16 async_llm_engine.py:179] Finished request chatcmpl-a76ca81390a94102804afb743a59cdfa.
INFO:     127.0.0.1:44254 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:16 logger.py:39] Received request chatcmpl-6c806aa5d1b64683ad814edeea6b8890: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🌊🦞🐛🌊\n🦞🦞🌊🌙\n🌊🦞🌊🦞\n🌙🦞🌊🌊\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14811, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:16 async_llm_engine.py:211] Added request chatcmpl-6c806aa5d1b64683ad814edeea6b8890.
INFO 02-28 20:35:16 async_llm_engine.py:179] Finished request chatcmpl-926f9055d52b4cddbc76c8a9bcfc984a.
INFO:     127.0.0.1:44290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:16 logger.py:39] Received request chatcmpl-c999f3da398e4fbf8718e7e21dfb61fc: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🦮🌕🦭💫\n🌕🦭💫💫\n🌕🦮🌕💫\n💫🌕🦮🌕\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14812, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:16 async_llm_engine.py:211] Added request chatcmpl-c999f3da398e4fbf8718e7e21dfb61fc.
INFO 02-28 20:35:17 async_llm_engine.py:179] Finished request chatcmpl-1f4983115df74e0b96bc57d31c8b3528.
INFO:     127.0.0.1:50466 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:17 logger.py:39] Received request chatcmpl-7f50d1b100d04a1eb3bc7fe8b3b5e03d: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐏🦆🦙🦆\n🌒🦆🦆🌒\n🦆🌒🌒🌒\n🦙🦆🌒🦆\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14810, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:17 async_llm_engine.py:211] Added request chatcmpl-7f50d1b100d04a1eb3bc7fe8b3b5e03d.
INFO 02-28 20:35:18 async_llm_engine.py:179] Finished request chatcmpl-94a29f78abed40e0885424b1f7ba5cf5.
INFO:     127.0.0.1:44274 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:18 logger.py:39] Received request chatcmpl-efab622a78d84ae1beec0b678fdb481a: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐖🐖🐭🐭\n🥰🦍🥰🥰\n🐖🦍🦍🦍\n🥰🦍🐭🦍\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:18 async_llm_engine.py:211] Added request chatcmpl-efab622a78d84ae1beec0b678fdb481a.
INFO 02-28 20:35:18 metrics.py:455] Avg prompt throughput: 223.0 tokens/s, Avg generation throughput: 213.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-28 20:35:18 async_llm_engine.py:179] Finished request chatcmpl-85c33ded99854823a98ce73dddd0399c.
INFO:     127.0.0.1:50450 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:18 logger.py:39] Received request chatcmpl-c09d59b6fa2d442b9970d99227a80102: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n☀️❄️🪐☀️\n🪐❄️🪐🪐\n🪐🐮☀️☀️\n🪐☀️❄️❄️\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14796, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:18 async_llm_engine.py:211] Added request chatcmpl-c09d59b6fa2d442b9970d99227a80102.
INFO 02-28 20:35:18 async_llm_engine.py:179] Finished request chatcmpl-38b20fc259af4b13a0b7bc3ccaa9fbf1.
INFO:     127.0.0.1:50458 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:18 logger.py:39] Received request chatcmpl-f823fafc696644f3b5a452300f9cb3b6: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐵🐰🐵🌳\n🌱🐵🐵🐵\n🐵🐵🐰🐰\n🌱🌳🐰🌱\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:18 async_llm_engine.py:211] Added request chatcmpl-f823fafc696644f3b5a452300f9cb3b6.
INFO 02-28 20:35:23 metrics.py:455] Avg prompt throughput: 77.3 tokens/s, Avg generation throughput: 222.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-28 20:35:23 async_llm_engine.py:179] Finished request chatcmpl-965575d9686049c69150754a5b512cc4.
INFO:     127.0.0.1:50486 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:23 logger.py:39] Received request chatcmpl-02b602bc7f7c4d90acc10b4fccfa9560: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐋🐋🏄🦎\n🦩🦩🐋🦎\n🦩🦎🦩🦎\n🦎🏄🦩🏄\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14812, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:23 async_llm_engine.py:211] Added request chatcmpl-02b602bc7f7c4d90acc10b4fccfa9560.
INFO 02-28 20:35:24 async_llm_engine.py:179] Finished request chatcmpl-b752e8fe7d2941ef9265d99f635975d8.
INFO:     127.0.0.1:50472 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:24 logger.py:39] Received request chatcmpl-1b86f0a7b3d549d8a1a93663100ee44b: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🌟🌏🌟🌟\n🌏🌏🧽🍁\n🌟🍁🌟🧽\n🍁🌟🌏🍁\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14815, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:24 async_llm_engine.py:211] Added request chatcmpl-1b86f0a7b3d549d8a1a93663100ee44b.
INFO 02-28 20:35:26 async_llm_engine.py:179] Finished request chatcmpl-c999f3da398e4fbf8718e7e21dfb61fc.
INFO:     127.0.0.1:53194 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:26 logger.py:39] Received request chatcmpl-48567faaee2142119893b183f1a9ff89: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐛🦞🦞🐛\n🦢🐏🦢🐛\n🐏🐏🦞🐏\n🦞🦢🦢🦢\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14804, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:26 async_llm_engine.py:211] Added request chatcmpl-48567faaee2142119893b183f1a9ff89.
INFO 02-28 20:35:28 async_llm_engine.py:179] Finished request chatcmpl-6c806aa5d1b64683ad814edeea6b8890.
INFO:     127.0.0.1:53182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:28 logger.py:39] Received request chatcmpl-b9e6fe8bf3844fc6bbfb8b7b8a8cd55f: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🏄🏄🐧🐧\n🐠🐧🐧🐧\n🏄🏄🐧🏄\n🐠🏄🐠🐆\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:28 async_llm_engine.py:211] Added request chatcmpl-b9e6fe8bf3844fc6bbfb8b7b8a8cd55f.
INFO 02-28 20:35:28 metrics.py:455] Avg prompt throughput: 150.0 tokens/s, Avg generation throughput: 217.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-28 20:35:28 async_llm_engine.py:179] Finished request chatcmpl-7f50d1b100d04a1eb3bc7fe8b3b5e03d.
INFO:     127.0.0.1:53204 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:28 logger.py:39] Received request chatcmpl-608e2f0d38234951ac058feef0b9ef30: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🦇🤚🌾🌾\n🤚🌾🦇🌾\n🌾🐘🤚🤚\n🤚🌾🐘🤚\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:28 async_llm_engine.py:211] Added request chatcmpl-608e2f0d38234951ac058feef0b9ef30.
INFO 02-28 20:35:28 async_llm_engine.py:179] Finished request chatcmpl-efab622a78d84ae1beec0b678fdb481a.
INFO:     127.0.0.1:53208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:28 logger.py:39] Received request chatcmpl-6fef4befa804425cab50bf62c158ff41: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🌍🐦🌍🌘\n🌵🌍🌵🌵\n🌵🌍🌘🐦\n🐦🌘🌘🌵\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:28 async_llm_engine.py:211] Added request chatcmpl-6fef4befa804425cab50bf62c158ff41.
INFO 02-28 20:35:29 async_llm_engine.py:179] Finished request chatcmpl-f823fafc696644f3b5a452300f9cb3b6.
INFO:     127.0.0.1:53224 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:30 logger.py:39] Received request chatcmpl-eda9e0b7b47848ceb6097859c63c06cf: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐅🌼🌼🐅\n🐅🐳🐅🐅\n🌼🐅🌼🐅\n🐅🌲🌼🌲\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:30 async_llm_engine.py:211] Added request chatcmpl-eda9e0b7b47848ceb6097859c63c06cf.
INFO 02-28 20:35:30 async_llm_engine.py:179] Finished request chatcmpl-c09d59b6fa2d442b9970d99227a80102.
INFO:     127.0.0.1:53210 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:30 logger.py:39] Received request chatcmpl-0d78958a47114febb2a1741c0a879a66: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐿🐿🐼🐼\n🐼🚀🐼🐼\n🐿🚀🚀🐼\n✨✨🚀🐿\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:30 async_llm_engine.py:211] Added request chatcmpl-0d78958a47114febb2a1741c0a879a66.
INFO 02-28 20:35:33 async_llm_engine.py:179] Finished request chatcmpl-02b602bc7f7c4d90acc10b4fccfa9560.
INFO:     127.0.0.1:53232 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:33 logger.py:39] Received request chatcmpl-bd9bac023d824533bc6662b5602fd009: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🌓🌿🌿🌓\n🌓🐩🌓🐩\n🐩🐩🌿🌓\n🐩🌿🌓🐰\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:33 async_llm_engine.py:211] Added request chatcmpl-bd9bac023d824533bc6662b5602fd009.
INFO 02-28 20:35:33 metrics.py:455] Avg prompt throughput: 182.9 tokens/s, Avg generation throughput: 215.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-28 20:35:34 async_llm_engine.py:179] Finished request chatcmpl-1b86f0a7b3d549d8a1a93663100ee44b.
INFO:     127.0.0.1:53236 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:34 logger.py:39] Received request chatcmpl-3bfaeec2f6014f009137a7aed3bf6374: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🦆🐃🌔🐃\n🐃🌒🌔🐃\n🐃🐃🌔🐃\n🌒🌒🌔🌒\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14813, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:34 async_llm_engine.py:211] Added request chatcmpl-3bfaeec2f6014f009137a7aed3bf6374.
INFO 02-28 20:35:35 async_llm_engine.py:179] Finished request chatcmpl-bd9bac023d824533bc6662b5602fd009.
INFO:     127.0.0.1:36134 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:35 logger.py:39] Received request chatcmpl-0fbe1a12834f4c2bae97c77ef773cf8f: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🎄🦊🦊🎄\n🐐🐧🦊🐐\n🎄🎄🐧🦊\n🐐🐐🦊🐧\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:35 async_llm_engine.py:211] Added request chatcmpl-0fbe1a12834f4c2bae97c77ef773cf8f.
INFO 02-28 20:35:36 async_llm_engine.py:179] Finished request chatcmpl-48567faaee2142119893b183f1a9ff89.
INFO:     127.0.0.1:36076 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:36 async_llm_engine.py:179] Finished request chatcmpl-eda9e0b7b47848ceb6097859c63c06cf.
INFO:     127.0.0.1:36104 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:37 async_llm_engine.py:179] Finished request chatcmpl-608e2f0d38234951ac058feef0b9ef30.
INFO:     127.0.0.1:36088 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:38 metrics.py:455] Avg prompt throughput: 74.0 tokens/s, Avg generation throughput: 203.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-28 20:35:39 async_llm_engine.py:179] Finished request chatcmpl-6fef4befa804425cab50bf62c158ff41.
INFO:     127.0.0.1:36092 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:41 async_llm_engine.py:179] Finished request chatcmpl-0d78958a47114febb2a1741c0a879a66.
INFO:     127.0.0.1:36120 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 112.3 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-28 20:35:45 async_llm_engine.py:179] Finished request chatcmpl-0fbe1a12834f4c2bae97c77ef773cf8f.
INFO:     127.0.0.1:39014 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:46 async_llm_engine.py:179] Finished request chatcmpl-b9e6fe8bf3844fc6bbfb8b7b8a8cd55f.
INFO:     127.0.0.1:36086 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:48 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 53.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-28 20:35:53 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-28 20:35:55 async_llm_engine.py:179] Finished request chatcmpl-3bfaeec2f6014f009137a7aed3bf6374.
INFO:     127.0.0.1:36150 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:36:08 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.7 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
