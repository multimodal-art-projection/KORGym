INFO 02-28 20:27:43 __init__.py:207] Automatically detected platform cuda.
INFO 02-28 20:27:43 api_server.py:912] vLLM API server version 0.7.3
INFO 02-28 20:27:43 api_server.py:913] args: Namespace(host=None, port=9003, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='/map-vepfs/models/jiajun/Qwen/Qwen2.5-72B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=15000, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=2, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.95, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['Qwen2.5-72B-Instruct'], qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False)
INFO 02-28 20:27:50 config.py:549] This model supports multiple tasks: {'reward', 'generate', 'embed', 'score', 'classify'}. Defaulting to 'generate'.
INFO 02-28 20:27:50 config.py:1382] Defaulting to use mp for distributed inference
WARNING 02-28 20:27:50 config.py:676] Async output processing can not be enabled with pipeline parallel
INFO 02-28 20:27:50 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/map-vepfs/models/jiajun/Qwen/Qwen2.5-72B-Instruct', speculative_config=None, tokenizer='/map-vepfs/models/jiajun/Qwen/Qwen2.5-72B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=15000, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen2.5-72B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
WARNING 02-28 20:27:51 multiproc_worker_utils.py:300] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 02-28 20:27:51 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 02-28 20:27:51 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 02-28 20:27:51 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 02-28 20:27:51 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 02-28 20:27:51 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-28 20:27:51 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-28 20:27:51 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-28 20:27:51 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 02-28 20:27:52 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-28 20:27:52 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-28 20:27:52 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 02-28 20:27:52 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-28 20:27:52 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 02-28 20:27:52 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 02-28 20:27:52 cuda.py:229] Using Flash Attention backend.
INFO 02-28 20:27:52 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-28 20:27:54 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-28 20:27:54 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-28 20:27:54 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 02-28 20:27:54 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-28 20:27:54 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-28 20:27:54 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 02-28 20:27:54 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-28 20:27:54 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 02-28 20:27:54 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 02-28 20:27:54 utils.py:916] Found nccl from library libnccl.so.2
INFO 02-28 20:27:54 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 02-28 20:27:54 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 02-28 20:27:54 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 02-28 20:27:54 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 02-28 20:27:54 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 02-28 20:27:54 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 02-28 20:27:55 custom_all_reduce_utils.py:206] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
INFO 02-28 20:28:19 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 02-28 20:28:19 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-28 20:28:19 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 02-28 20:28:19 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 02-28 20:28:19 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 02-28 20:28:19 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-28 20:28:19 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-28 20:28:19 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 02-28 20:28:19 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_63057bc7'), local_subscribe_port=56567, remote_subscribe_port=None)
INFO 02-28 20:28:19 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_e237c3f7'), local_subscribe_port=56123, remote_subscribe_port=None)
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-28 20:28:19 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 02-28 20:28:19 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-28 20:28:19 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 02-28 20:28:19 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 02-28 20:28:19 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-28 20:28:19 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 02-28 20:28:19 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 02-28 20:28:19 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-28 20:28:19 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-28 20:28:19 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 02-28 20:28:19 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 02-28 20:28:19 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 02-28 20:28:19 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-28 20:28:19 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 02-28 20:28:19 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 02-28 20:28:19 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 02-28 20:28:20 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/Qwen/Qwen2.5-72B-Instruct...
INFO 02-28 20:28:20 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/Qwen/Qwen2.5-72B-Instruct...
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 02-28 20:28:20 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/Qwen/Qwen2.5-72B-Instruct...
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 02-28 20:28:20 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/Qwen/Qwen2.5-72B-Instruct...
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-28 20:28:20 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/Qwen/Qwen2.5-72B-Instruct...
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 02-28 20:28:20 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/Qwen/Qwen2.5-72B-Instruct...
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-28 20:28:20 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/Qwen/Qwen2.5-72B-Instruct...
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-28 20:28:20 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/Qwen/Qwen2.5-72B-Instruct...
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-28 20:29:49 model_runner.py:1115] Loading model weights took 16.9956 GB
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 02-28 20:29:49 model_runner.py:1115] Loading model weights took 16.9956 GB
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-28 20:29:49 model_runner.py:1115] Loading model weights took 16.9956 GB
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-28 20:29:49 model_runner.py:1115] Loading model weights took 16.9956 GB
INFO 02-28 20:29:50 model_runner.py:1115] Loading model weights took 16.9956 GB
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 02-28 20:29:50 model_runner.py:1115] Loading model weights took 16.9956 GB
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 02-28 20:29:50 model_runner.py:1115] Loading model weights took 16.9956 GB
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 02-28 20:29:50 model_runner.py:1115] Loading model weights took 16.9956 GB
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-28 20:29:53 worker.py:267] Memory profiling takes 2.47 seconds
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-28 20:29:53 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-28 20:29:53 worker.py:267] model weights take 17.00GiB; non_torch_memory takes 1.70GiB; PyTorch activation peak memory takes 2.00GiB; the rest of the memory reserved for KV Cache is 54.69GiB.
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-28 20:29:53 worker.py:267] Memory profiling takes 2.46 seconds
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-28 20:29:53 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-28 20:29:53 worker.py:267] model weights take 17.00GiB; non_torch_memory takes 1.70GiB; PyTorch activation peak memory takes 2.00GiB; the rest of the memory reserved for KV Cache is 54.69GiB.
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-28 20:29:53 worker.py:267] Memory profiling takes 2.48 seconds
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-28 20:29:53 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-28 20:29:53 worker.py:267] model weights take 17.00GiB; non_torch_memory takes 1.51GiB; PyTorch activation peak memory takes 2.00GiB; the rest of the memory reserved for KV Cache is 54.87GiB.
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 02-28 20:29:53 worker.py:267] Memory profiling takes 2.61 seconds
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 02-28 20:29:53 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 02-28 20:29:53 worker.py:267] model weights take 17.00GiB; non_torch_memory takes 1.70GiB; PyTorch activation peak memory takes 2.07GiB; the rest of the memory reserved for KV Cache is 54.61GiB.
INFO 02-28 20:29:56 worker.py:267] Memory profiling takes 5.24 seconds
INFO 02-28 20:29:56 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
INFO 02-28 20:29:56 worker.py:267] model weights take 17.00GiB; non_torch_memory takes 0.94GiB; PyTorch activation peak memory takes 1.77GiB; the rest of the memory reserved for KV Cache is 55.67GiB.
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 02-28 20:29:56 worker.py:267] Memory profiling takes 5.23 seconds
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 02-28 20:29:56 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 02-28 20:29:56 worker.py:267] model weights take 17.00GiB; non_torch_memory takes 1.03GiB; PyTorch activation peak memory takes 1.77GiB; the rest of the memory reserved for KV Cache is 55.58GiB.
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 02-28 20:29:56 worker.py:267] Memory profiling takes 5.28 seconds
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 02-28 20:29:56 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 02-28 20:29:56 worker.py:267] model weights take 17.00GiB; non_torch_memory takes 0.94GiB; PyTorch activation peak memory takes 1.77GiB; the rest of the memory reserved for KV Cache is 55.67GiB.
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 02-28 20:29:56 worker.py:267] Memory profiling takes 5.30 seconds
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 02-28 20:29:56 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 02-28 20:29:56 worker.py:267] model weights take 17.00GiB; non_torch_memory takes 1.03GiB; PyTorch activation peak memory takes 1.77GiB; the rest of the memory reserved for KV Cache is 55.58GiB.
INFO 02-28 20:29:56 executor_base.py:111] # cuda blocks: 89477, # CPU blocks: 6553
INFO 02-28 20:29:56 executor_base.py:116] Maximum concurrency for 15000 tokens per request: 95.44x
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 02-28 20:29:59 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 02-28 20:29:59 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-28 20:29:59 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-28 20:29:59 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 02-28 20:29:59 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 02-28 20:29:59 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-28 20:29:59 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 02-28 20:29:59 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-28 20:30:32 custom_all_reduce.py:226] Registering 5600 cuda graph addresses
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 02-28 20:30:32 custom_all_reduce.py:226] Registering 5600 cuda graph addresses
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-28 20:30:32 custom_all_reduce.py:226] Registering 5600 cuda graph addresses
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-28 20:30:33 custom_all_reduce.py:226] Registering 5600 cuda graph addresses
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-28 20:30:33 model_runner.py:1562] Graph capturing finished in 34 secs, took 2.32 GiB
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-28 20:30:33 model_runner.py:1562] Graph capturing finished in 34 secs, took 2.32 GiB
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 02-28 20:30:33 model_runner.py:1562] Graph capturing finished in 34 secs, took 2.32 GiB
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-28 20:30:33 model_runner.py:1562] Graph capturing finished in 34 secs, took 2.32 GiB
INFO 02-28 20:30:35 custom_all_reduce.py:226] Registering 5670 cuda graph addresses
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 02-28 20:30:35 custom_all_reduce.py:226] Registering 5670 cuda graph addresses
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 02-28 20:30:35 custom_all_reduce.py:226] Registering 5670 cuda graph addresses
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 02-28 20:30:36 custom_all_reduce.py:226] Registering 5670 cuda graph addresses
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 02-28 20:30:36 model_runner.py:1562] Graph capturing finished in 36 secs, took 2.35 GiB
INFO 02-28 20:30:36 model_runner.py:1562] Graph capturing finished in 36 secs, took 2.35 GiB
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 02-28 20:30:36 model_runner.py:1562] Graph capturing finished in 36 secs, took 2.35 GiB
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 02-28 20:30:36 model_runner.py:1562] Graph capturing finished in 36 secs, took 2.35 GiB
INFO 02-28 20:30:36 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 45.20 seconds
INFO 02-28 20:30:37 api_server.py:958] Starting vLLM API server on http://0.0.0.0:9003
INFO 02-28 20:30:37 launcher.py:23] Available routes are:
INFO 02-28 20:30:37 launcher.py:31] Route: /openapi.json, Methods: GET, HEAD
INFO 02-28 20:30:37 launcher.py:31] Route: /docs, Methods: GET, HEAD
INFO 02-28 20:30:37 launcher.py:31] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 02-28 20:30:37 launcher.py:31] Route: /redoc, Methods: GET, HEAD
INFO 02-28 20:30:37 launcher.py:31] Route: /health, Methods: GET
INFO 02-28 20:30:37 launcher.py:31] Route: /ping, Methods: GET, POST
INFO 02-28 20:30:37 launcher.py:31] Route: /tokenize, Methods: POST
INFO 02-28 20:30:37 launcher.py:31] Route: /detokenize, Methods: POST
INFO 02-28 20:30:37 launcher.py:31] Route: /v1/models, Methods: GET
INFO 02-28 20:30:37 launcher.py:31] Route: /version, Methods: GET
INFO 02-28 20:30:37 launcher.py:31] Route: /v1/chat/completions, Methods: POST
INFO 02-28 20:30:37 launcher.py:31] Route: /v1/completions, Methods: POST
INFO 02-28 20:30:37 launcher.py:31] Route: /v1/embeddings, Methods: POST
INFO 02-28 20:30:37 launcher.py:31] Route: /pooling, Methods: POST
INFO 02-28 20:30:37 launcher.py:31] Route: /score, Methods: POST
INFO 02-28 20:30:37 launcher.py:31] Route: /v1/score, Methods: POST
INFO 02-28 20:30:37 launcher.py:31] Route: /v1/audio/transcriptions, Methods: POST
INFO 02-28 20:30:37 launcher.py:31] Route: /rerank, Methods: POST
INFO 02-28 20:30:37 launcher.py:31] Route: /v1/rerank, Methods: POST
INFO 02-28 20:30:37 launcher.py:31] Route: /v2/rerank, Methods: POST
INFO 02-28 20:30:37 launcher.py:31] Route: /invocations, Methods: POST
INFO 02-28 20:34:29 chat_utils.py:332] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
INFO 02-28 20:34:29 logger.py:39] Received request chatcmpl-dfdb972f49714e7b9713fcbb6bbaa7f1: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüêãüêãüöÄüêã\nüöÄüå≥üå≥üöÄ\nüå≥ü¶©üöÄüå≥\nüöÄü¶©ü¶©üöÄ\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14814, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:29 async_llm_engine.py:211] Added request chatcmpl-dfdb972f49714e7b9713fcbb6bbaa7f1.
INFO 02-28 20:34:29 logger.py:39] Received request chatcmpl-3a9238b5def74dc8b416305cc40e1975: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nü¶Öüê≥üê≥üê≥\nüê≥üåûü¶Öüê≥\nü¶Öüê≥üê≥ü¶Ö\nüê≥üê∞üåûü¶Ö\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:29 async_llm_engine.py:211] Added request chatcmpl-3a9238b5def74dc8b416305cc40e1975.
INFO 02-28 20:34:29 logger.py:39] Received request chatcmpl-fd56651be92440978360519ea21234df: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüêÆüêØüêØüêÆ\nüê≠üêÆüêàüêà\nüêØüêàüêØüê≠\nüê≠üêØüêàüêØ\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:29 async_llm_engine.py:211] Added request chatcmpl-fd56651be92440978360519ea21234df.
INFO 02-28 20:34:29 logger.py:39] Received request chatcmpl-25b25171588b4818b22f949e50d1bdc3: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüåëü¶Üü¶Äü¶Ä\nü¶Üüåë‚òÑÔ∏è‚òÑÔ∏è\nü¶Üü¶Üü¶Ü‚òÑÔ∏è\n‚òÑÔ∏è‚òÑÔ∏èü¶Üü¶Ä\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14812, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:29 async_llm_engine.py:211] Added request chatcmpl-25b25171588b4818b22f949e50d1bdc3.
INFO 02-28 20:34:29 logger.py:39] Received request chatcmpl-247c04a474fd41c6bb97f6d8a4fdd582: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nü¶•üê´ü¶Äü¶Ä\nü¶Äü¶•üêµü¶Ä\nüê´üêµüêµüê´\nü¶Äüêµüê´ü¶Ä\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14815, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:29 async_llm_engine.py:211] Added request chatcmpl-247c04a474fd41c6bb97f6d8a4fdd582.
INFO 02-28 20:34:29 logger.py:39] Received request chatcmpl-a7befb65635b40a7ae7373875d4cb1dd: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüíßüåïüê≥üíß\nüê≥üíßü¶Æüåï\nüê≥üåïüíßüê≥\nüíßüê≥üåïü¶Æ\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14815, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:29 async_llm_engine.py:211] Added request chatcmpl-a7befb65635b40a7ae7373875d4cb1dd.
INFO 02-28 20:34:29 logger.py:39] Received request chatcmpl-456dd6a2cf414c9aa521d510966ff77b: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüåôüåôüê®üêã\nüåºüåºüåôüåº\nüêãüê®üêãüåô\nüê®üåºüåôüêã\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:29 async_llm_engine.py:211] Added request chatcmpl-456dd6a2cf414c9aa521d510966ff77b.
INFO 02-28 20:34:29 logger.py:39] Received request chatcmpl-0e934feca40b4b78b763d1486dafa54b: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüêÑüêÑü¶•üêÑ\nüêóüêÑüêÑüê∂\nüê∂üêÑüêóüêÑ\nüê∂üêÑüêÑüêó\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14816, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:29 async_llm_engine.py:211] Added request chatcmpl-0e934feca40b4b78b763d1486dafa54b.
INFO 02-28 20:34:33 metrics.py:455] Avg prompt throughput: 294.7 tokens/s, Avg generation throughput: 103.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-28 20:34:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 226.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-28 20:34:38 async_llm_engine.py:179] Finished request chatcmpl-fd56651be92440978360519ea21234df.
INFO:     127.0.0.1:36448 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:34:38 logger.py:39] Received request chatcmpl-520e59527e1a406c93a3b37df7ce2602: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nü¶Éü¶êü¶êü¶É\nü¶Éü¶Éüê•ü¶ê\nüê•üê•üê•üê•\nüê•ü¶Éüê•ü¶ê\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:38 async_llm_engine.py:211] Added request chatcmpl-520e59527e1a406c93a3b37df7ce2602.
INFO 02-28 20:34:39 async_llm_engine.py:179] Finished request chatcmpl-25b25171588b4818b22f949e50d1bdc3.
INFO:     127.0.0.1:36464 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:34:39 logger.py:39] Received request chatcmpl-c5ae54fa7a74467a99a0285e461e204a: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n‚õÖÔ∏è‚õÖÔ∏èüåæü¶ö\nüêäüåæü¶öüåæ\nüêä‚õÖÔ∏èüêäüêä\n‚õÖÔ∏è‚õÖÔ∏è‚õÖÔ∏èüåæ\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14811, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:39 async_llm_engine.py:211] Added request chatcmpl-c5ae54fa7a74467a99a0285e461e204a.
INFO 02-28 20:34:40 async_llm_engine.py:179] Finished request chatcmpl-3a9238b5def74dc8b416305cc40e1975.
INFO:     127.0.0.1:36438 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:34:40 logger.py:39] Received request chatcmpl-51fd5f0e339e4ccaad09530493176dcb: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüåôüßΩüå∑üå∑\nüå±üßΩüåôüå∑\nüå±üåôüßΩüå±\nüåôüå∑üßΩüå±\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14813, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:40 async_llm_engine.py:211] Added request chatcmpl-51fd5f0e339e4ccaad09530493176dcb.
INFO 02-28 20:34:40 async_llm_engine.py:179] Finished request chatcmpl-a7befb65635b40a7ae7373875d4cb1dd.
INFO:     127.0.0.1:36488 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:34:40 logger.py:39] Received request chatcmpl-5c6b621fefd0410aa3fb56c7273d7fd4: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüåöüåöüêöüåö\nüçÉüêöüå∫üåö\nüçÉüçÉüêöüêö\nüåöüçÉüçÉüçÉ\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:40 async_llm_engine.py:211] Added request chatcmpl-5c6b621fefd0410aa3fb56c7273d7fd4.
INFO 02-28 20:34:42 async_llm_engine.py:179] Finished request chatcmpl-dfdb972f49714e7b9713fcbb6bbaa7f1.
INFO:     127.0.0.1:36424 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:34:42 logger.py:39] Received request chatcmpl-7342b11fd52247b0a2177ca10935f33f: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüêäüêïüåπüåï\nüêïüåïüêïüêä\nüåπüêäüêïüåï\nüêäüåïüêäüåπ\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:42 async_llm_engine.py:211] Added request chatcmpl-7342b11fd52247b0a2177ca10935f33f.
INFO 02-28 20:34:43 metrics.py:455] Avg prompt throughput: 184.6 tokens/s, Avg generation throughput: 214.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-28 20:34:44 async_llm_engine.py:179] Finished request chatcmpl-0e934feca40b4b78b763d1486dafa54b.
INFO:     127.0.0.1:36496 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:34:44 logger.py:39] Received request chatcmpl-61795192cf6d40e1b51711aaa8083142: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nü¶èü¶èü¶èü¶è\nüêãü¶èü¶üüêã\nü¶ûü¶èüêãü¶ü\nü¶èüêãü¶üü¶û\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14805, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:44 async_llm_engine.py:211] Added request chatcmpl-61795192cf6d40e1b51711aaa8083142.
INFO 02-28 20:34:48 metrics.py:455] Avg prompt throughput: 38.9 tokens/s, Avg generation throughput: 223.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-28 20:34:49 async_llm_engine.py:179] Finished request chatcmpl-520e59527e1a406c93a3b37df7ce2602.
INFO:     127.0.0.1:39026 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:34:49 logger.py:39] Received request chatcmpl-1e1d6f43c4ef4c72a70ba330e4c918de: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüåèüåèüåèüêî\nüêüüåèüêüüêü\nüêüüêîüåè‚òÅÔ∏è\nüåèüåèüåèüåè\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14816, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:49 async_llm_engine.py:211] Added request chatcmpl-1e1d6f43c4ef4c72a70ba330e4c918de.
INFO 02-28 20:34:49 async_llm_engine.py:179] Finished request chatcmpl-247c04a474fd41c6bb97f6d8a4fdd582.
INFO:     127.0.0.1:36476 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:34:49 logger.py:39] Received request chatcmpl-0af254cae81d48aaba14b3c4f052f697: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüòÑüòÑü¶éü¶é\nüòÑüåçüòÑü¶é\nüåçü§öüåçüåç\nü§öüåçüåçüòÑ\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:49 async_llm_engine.py:211] Added request chatcmpl-0af254cae81d48aaba14b3c4f052f697.
INFO 02-28 20:34:52 async_llm_engine.py:179] Finished request chatcmpl-7342b11fd52247b0a2177ca10935f33f.
INFO:     127.0.0.1:39064 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:34:52 async_llm_engine.py:179] Finished request chatcmpl-c5ae54fa7a74467a99a0285e461e204a.
INFO:     127.0.0.1:39034 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:34:52 logger.py:39] Received request chatcmpl-17bc33e0eddc409c8b77950574c1f3f1: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nü¶çüåªü¶çüêà\nü¶çüå∑üåªüåª\nüêàüå∑üå∑üå∑\nüåªüå∑üêàüåª\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:52 async_llm_engine.py:211] Added request chatcmpl-17bc33e0eddc409c8b77950574c1f3f1.
INFO 02-28 20:34:52 logger.py:39] Received request chatcmpl-497c1fbee269448a98795cf64ef0fabf: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüê´üå≤üê´üåç\nüåçüå≤üêìüêì\nüê´üê´üêìüå≤\nüåçüå≤üå≤üå≤\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:52 async_llm_engine.py:211] Added request chatcmpl-497c1fbee269448a98795cf64ef0fabf.
INFO 02-28 20:34:53 metrics.py:455] Avg prompt throughput: 146.3 tokens/s, Avg generation throughput: 216.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-28 20:34:53 async_llm_engine.py:179] Finished request chatcmpl-51fd5f0e339e4ccaad09530493176dcb.
INFO:     127.0.0.1:39046 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:34:53 logger.py:39] Received request chatcmpl-9d97fdd40ec54f4999c8f2f6103835d6: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüê£üê£üêèüêè\nüê£üêèüéãüêè\nüéãüê£üéãüêú\nüéãüê£üê£üê£\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14813, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:53 async_llm_engine.py:211] Added request chatcmpl-9d97fdd40ec54f4999c8f2f6103835d6.
INFO 02-28 20:34:53 async_llm_engine.py:179] Finished request chatcmpl-456dd6a2cf414c9aa521d510966ff77b.
INFO:     127.0.0.1:36494 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:34:53 logger.py:39] Received request chatcmpl-898c61159c2b4d3fa6849dd5ea91372f: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüåçüêöüê§üê§\nüåçüê§üéÅüê§\nüêöüê§üê§üéÅ\nüê§üê§üéÅüåç\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:53 async_llm_engine.py:211] Added request chatcmpl-898c61159c2b4d3fa6849dd5ea91372f.
INFO 02-28 20:34:53 async_llm_engine.py:179] Finished request chatcmpl-61795192cf6d40e1b51711aaa8083142.
INFO:     127.0.0.1:39078 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:34:53 logger.py:39] Received request chatcmpl-b20d952a392d496fbcca9e125552fbda: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüêãüêóüêéüêé\nüêóüêóüêãüê∏\nüêéüêéüêéüêã\nüêãüê∏üê∏üê∏\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:53 async_llm_engine.py:211] Added request chatcmpl-b20d952a392d496fbcca9e125552fbda.
INFO 02-28 20:34:58 metrics.py:455] Avg prompt throughput: 110.3 tokens/s, Avg generation throughput: 219.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-28 20:34:59 async_llm_engine.py:179] Finished request chatcmpl-0af254cae81d48aaba14b3c4f052f697.
INFO:     127.0.0.1:40086 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:34:59 logger.py:39] Received request chatcmpl-f7ae13b783d4457ba0397d03fb871932: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüå≥ü•Äüå≥üå≥\nüêõüêõü¶çü•Ä\nüêõüå≥üå≥üå≥\nüêõü•Äü•Äü•Ä\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:59 async_llm_engine.py:211] Added request chatcmpl-f7ae13b783d4457ba0397d03fb871932.
INFO 02-28 20:34:59 async_llm_engine.py:179] Finished request chatcmpl-5c6b621fefd0410aa3fb56c7273d7fd4.
INFO:     127.0.0.1:39048 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:34:59 logger.py:39] Received request chatcmpl-6dc7f271bcd746d78fcf8826598929e8: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n‚õÖÔ∏èüê†ü¶âüèÑ\nü¶âüê†üèÑü¶â\nüèÑüê†‚õÖÔ∏èü¶â\nü¶âüèÑüèÑ‚õÖÔ∏è\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14814, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:34:59 async_llm_engine.py:211] Added request chatcmpl-6dc7f271bcd746d78fcf8826598929e8.
INFO 02-28 20:35:01 async_llm_engine.py:179] Finished request chatcmpl-1e1d6f43c4ef4c72a70ba330e4c918de.
INFO:     127.0.0.1:40082 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:01 logger.py:39] Received request chatcmpl-a76ca81390a94102804afb743a59cdfa: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüòÜ‚òÑÔ∏è‚òÑÔ∏èüòÜ\nüêØüêØüòÜ‚òÑÔ∏è\nü¶èüêØ‚òÑÔ∏èü¶è\nü¶èü¶è‚òÑÔ∏è‚òÑÔ∏è\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14807, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:01 async_llm_engine.py:211] Added request chatcmpl-a76ca81390a94102804afb743a59cdfa.
INFO 02-28 20:35:03 metrics.py:455] Avg prompt throughput: 112.3 tokens/s, Avg generation throughput: 219.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-28 20:35:03 async_llm_engine.py:179] Finished request chatcmpl-17bc33e0eddc409c8b77950574c1f3f1.
INFO:     127.0.0.1:40090 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:03 logger.py:39] Received request chatcmpl-01c077312c744ed9a4e7028a6c2616ca: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüí´üí´üí´ü¶©\nüí´üêúü¶©ü¶ï\nü¶ïü¶©ü¶©üí´\nü¶ïü¶ïüêúü¶ï\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14813, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:03 async_llm_engine.py:211] Added request chatcmpl-01c077312c744ed9a4e7028a6c2616ca.
INFO 02-28 20:35:03 async_llm_engine.py:179] Finished request chatcmpl-b20d952a392d496fbcca9e125552fbda.
INFO:     127.0.0.1:40136 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:03 logger.py:39] Received request chatcmpl-94a29f78abed40e0885424b1f7ba5cf5: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nü¶íü¶úü¶ñü¶ú\nü¶ñü¶íü¶úü¶ú\nüòÅü¶ñüòÅü¶ñ\nü¶íü¶úü¶úü¶ñ\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14811, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:03 async_llm_engine.py:211] Added request chatcmpl-94a29f78abed40e0885424b1f7ba5cf5.
INFO 02-28 20:35:04 async_llm_engine.py:179] Finished request chatcmpl-497c1fbee269448a98795cf64ef0fabf.
INFO:     127.0.0.1:40102 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:04 async_llm_engine.py:179] Finished request chatcmpl-898c61159c2b4d3fa6849dd5ea91372f.
INFO:     127.0.0.1:40120 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:04 logger.py:39] Received request chatcmpl-926f9055d52b4cddbc76c8a9bcfc984a: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüê¢üêçüí•üê¢\nüêçüí•üêçüí•\nüêçüê¢üêçüêç\nüêçüêçüí•üí•\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:04 async_llm_engine.py:211] Added request chatcmpl-926f9055d52b4cddbc76c8a9bcfc984a.
INFO 02-28 20:35:04 logger.py:39] Received request chatcmpl-7f22aaceafa5424d87b90ff448f3e9a1: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nü•Äü•Äüêòüêò\nüêòüêòü¶ÇüêÜ\nüêÜüêÜü¶Çü•Ä\nü•Äü¶Çü¶Çü•Ä\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14813, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:04 async_llm_engine.py:211] Added request chatcmpl-7f22aaceafa5424d87b90ff448f3e9a1.
INFO 02-28 20:35:08 metrics.py:455] Avg prompt throughput: 148.9 tokens/s, Avg generation throughput: 218.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-28 20:35:08 async_llm_engine.py:179] Finished request chatcmpl-9d97fdd40ec54f4999c8f2f6103835d6.
INFO:     127.0.0.1:40106 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:08 logger.py:39] Received request chatcmpl-85c33ded99854823a98ce73dddd0399c: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nü¶Üü¶Üü¶Üüî•\nüî•ü¶Üü¶Üü¶Ü\nüî•ü¶Üü¶Üü¶Ü\nü¶Üüêßüî•ü¶Ü\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:08 async_llm_engine.py:211] Added request chatcmpl-85c33ded99854823a98ce73dddd0399c.
INFO 02-28 20:35:08 async_llm_engine.py:179] Finished request chatcmpl-f7ae13b783d4457ba0397d03fb871932.
INFO:     127.0.0.1:44230 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:08 logger.py:39] Received request chatcmpl-38b20fc259af4b13a0b7bc3ccaa9fbf1: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüêêüåíüêêüí•\nüí•üåíüåíüåí\nüí•üêºüêêüåí\nüí•üêêüí•üêº\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14812, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:08 async_llm_engine.py:211] Added request chatcmpl-38b20fc259af4b13a0b7bc3ccaa9fbf1.
INFO 02-28 20:35:09 async_llm_engine.py:179] Finished request chatcmpl-6dc7f271bcd746d78fcf8826598929e8.
INFO:     127.0.0.1:44240 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:09 logger.py:39] Received request chatcmpl-1f4983115df74e0b96bc57d31c8b3528: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nü¶è‚õÖÔ∏èüåòü¶≠\nü¶≠ü¶èüåòü¶≠\nüåòü¶èüåòüåò\nü¶èü¶≠‚õÖÔ∏èüåò\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14807, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:09 async_llm_engine.py:211] Added request chatcmpl-1f4983115df74e0b96bc57d31c8b3528.
INFO 02-28 20:35:13 metrics.py:455] Avg prompt throughput: 112.6 tokens/s, Avg generation throughput: 219.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-28 20:35:14 async_llm_engine.py:179] Finished request chatcmpl-7f22aaceafa5424d87b90ff448f3e9a1.
INFO:     127.0.0.1:44300 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:14 logger.py:39] Received request chatcmpl-b752e8fe7d2941ef9265d99f635975d8: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüåªüòÅüåªüòÅ\nüåªüåªüåªüòÅ\nüòÅüåªüåªü¶¶\nü¶¶üåªüòÅüòÅ\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14815, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:14 async_llm_engine.py:211] Added request chatcmpl-b752e8fe7d2941ef9265d99f635975d8.
INFO 02-28 20:35:14 async_llm_engine.py:179] Finished request chatcmpl-01c077312c744ed9a4e7028a6c2616ca.
INFO:     127.0.0.1:44270 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:14 logger.py:39] Received request chatcmpl-965575d9686049c69150754a5b512cc4: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nü¶ñüê´üêºüêº\nüêºüê∫üê∫üêº\nüê´üê∫üêºü¶ñ\nüêºüê´ü¶ñüê∫\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:14 async_llm_engine.py:211] Added request chatcmpl-965575d9686049c69150754a5b512cc4.
INFO 02-28 20:35:16 async_llm_engine.py:179] Finished request chatcmpl-a76ca81390a94102804afb743a59cdfa.
INFO:     127.0.0.1:44254 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:16 logger.py:39] Received request chatcmpl-6c806aa5d1b64683ad814edeea6b8890: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüåäü¶ûüêõüåä\nü¶ûü¶ûüåäüåô\nüåäü¶ûüåäü¶û\nüåôü¶ûüåäüåä\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14811, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:16 async_llm_engine.py:211] Added request chatcmpl-6c806aa5d1b64683ad814edeea6b8890.
INFO 02-28 20:35:16 async_llm_engine.py:179] Finished request chatcmpl-926f9055d52b4cddbc76c8a9bcfc984a.
INFO:     127.0.0.1:44290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:16 logger.py:39] Received request chatcmpl-c999f3da398e4fbf8718e7e21dfb61fc: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nü¶Æüåïü¶≠üí´\nüåïü¶≠üí´üí´\nüåïü¶Æüåïüí´\nüí´üåïü¶Æüåï\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14812, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:16 async_llm_engine.py:211] Added request chatcmpl-c999f3da398e4fbf8718e7e21dfb61fc.
INFO 02-28 20:35:17 async_llm_engine.py:179] Finished request chatcmpl-1f4983115df74e0b96bc57d31c8b3528.
INFO:     127.0.0.1:50466 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:17 logger.py:39] Received request chatcmpl-7f50d1b100d04a1eb3bc7fe8b3b5e03d: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüêèü¶Üü¶ôü¶Ü\nüåíü¶Üü¶Üüåí\nü¶Üüåíüåíüåí\nü¶ôü¶Üüåíü¶Ü\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14810, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:17 async_llm_engine.py:211] Added request chatcmpl-7f50d1b100d04a1eb3bc7fe8b3b5e03d.
INFO 02-28 20:35:18 async_llm_engine.py:179] Finished request chatcmpl-94a29f78abed40e0885424b1f7ba5cf5.
INFO:     127.0.0.1:44274 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:18 logger.py:39] Received request chatcmpl-efab622a78d84ae1beec0b678fdb481a: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüêñüêñüê≠üê≠\nü•∞ü¶çü•∞ü•∞\nüêñü¶çü¶çü¶ç\nü•∞ü¶çüê≠ü¶ç\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:18 async_llm_engine.py:211] Added request chatcmpl-efab622a78d84ae1beec0b678fdb481a.
INFO 02-28 20:35:18 metrics.py:455] Avg prompt throughput: 223.0 tokens/s, Avg generation throughput: 213.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-28 20:35:18 async_llm_engine.py:179] Finished request chatcmpl-85c33ded99854823a98ce73dddd0399c.
INFO:     127.0.0.1:50450 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:18 logger.py:39] Received request chatcmpl-c09d59b6fa2d442b9970d99227a80102: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n‚òÄÔ∏è‚ùÑÔ∏èü™ê‚òÄÔ∏è\nü™ê‚ùÑÔ∏èü™êü™ê\nü™êüêÆ‚òÄÔ∏è‚òÄÔ∏è\nü™ê‚òÄÔ∏è‚ùÑÔ∏è‚ùÑÔ∏è\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14796, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:18 async_llm_engine.py:211] Added request chatcmpl-c09d59b6fa2d442b9970d99227a80102.
INFO 02-28 20:35:18 async_llm_engine.py:179] Finished request chatcmpl-38b20fc259af4b13a0b7bc3ccaa9fbf1.
INFO:     127.0.0.1:50458 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:18 logger.py:39] Received request chatcmpl-f823fafc696644f3b5a452300f9cb3b6: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüêµüê∞üêµüå≥\nüå±üêµüêµüêµ\nüêµüêµüê∞üê∞\nüå±üå≥üê∞üå±\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:18 async_llm_engine.py:211] Added request chatcmpl-f823fafc696644f3b5a452300f9cb3b6.
INFO 02-28 20:35:23 metrics.py:455] Avg prompt throughput: 77.3 tokens/s, Avg generation throughput: 222.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-28 20:35:23 async_llm_engine.py:179] Finished request chatcmpl-965575d9686049c69150754a5b512cc4.
INFO:     127.0.0.1:50486 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:23 logger.py:39] Received request chatcmpl-02b602bc7f7c4d90acc10b4fccfa9560: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüêãüêãüèÑü¶é\nü¶©ü¶©üêãü¶é\nü¶©ü¶éü¶©ü¶é\nü¶éüèÑü¶©üèÑ\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14812, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:23 async_llm_engine.py:211] Added request chatcmpl-02b602bc7f7c4d90acc10b4fccfa9560.
INFO 02-28 20:35:24 async_llm_engine.py:179] Finished request chatcmpl-b752e8fe7d2941ef9265d99f635975d8.
INFO:     127.0.0.1:50472 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:24 logger.py:39] Received request chatcmpl-1b86f0a7b3d549d8a1a93663100ee44b: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüåüüåèüåüüåü\nüåèüåèüßΩüçÅ\nüåüüçÅüåüüßΩ\nüçÅüåüüåèüçÅ\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14815, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:24 async_llm_engine.py:211] Added request chatcmpl-1b86f0a7b3d549d8a1a93663100ee44b.
INFO 02-28 20:35:26 async_llm_engine.py:179] Finished request chatcmpl-c999f3da398e4fbf8718e7e21dfb61fc.
INFO:     127.0.0.1:53194 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:26 logger.py:39] Received request chatcmpl-48567faaee2142119893b183f1a9ff89: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüêõü¶ûü¶ûüêõ\nü¶¢üêèü¶¢üêõ\nüêèüêèü¶ûüêè\nü¶ûü¶¢ü¶¢ü¶¢\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14804, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:26 async_llm_engine.py:211] Added request chatcmpl-48567faaee2142119893b183f1a9ff89.
INFO 02-28 20:35:28 async_llm_engine.py:179] Finished request chatcmpl-6c806aa5d1b64683ad814edeea6b8890.
INFO:     127.0.0.1:53182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:28 logger.py:39] Received request chatcmpl-b9e6fe8bf3844fc6bbfb8b7b8a8cd55f: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüèÑüèÑüêßüêß\nüê†üêßüêßüêß\nüèÑüèÑüêßüèÑ\nüê†üèÑüê†üêÜ\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:28 async_llm_engine.py:211] Added request chatcmpl-b9e6fe8bf3844fc6bbfb8b7b8a8cd55f.
INFO 02-28 20:35:28 metrics.py:455] Avg prompt throughput: 150.0 tokens/s, Avg generation throughput: 217.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-28 20:35:28 async_llm_engine.py:179] Finished request chatcmpl-7f50d1b100d04a1eb3bc7fe8b3b5e03d.
INFO:     127.0.0.1:53204 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:28 logger.py:39] Received request chatcmpl-608e2f0d38234951ac058feef0b9ef30: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nü¶áü§öüåæüåæ\nü§öüåæü¶áüåæ\nüåæüêòü§öü§ö\nü§öüåæüêòü§ö\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:28 async_llm_engine.py:211] Added request chatcmpl-608e2f0d38234951ac058feef0b9ef30.
INFO 02-28 20:35:28 async_llm_engine.py:179] Finished request chatcmpl-efab622a78d84ae1beec0b678fdb481a.
INFO:     127.0.0.1:53208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:28 logger.py:39] Received request chatcmpl-6fef4befa804425cab50bf62c158ff41: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüåçüê¶üåçüåò\nüåµüåçüåµüåµ\nüåµüåçüåòüê¶\nüê¶üåòüåòüåµ\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:28 async_llm_engine.py:211] Added request chatcmpl-6fef4befa804425cab50bf62c158ff41.
INFO 02-28 20:35:29 async_llm_engine.py:179] Finished request chatcmpl-f823fafc696644f3b5a452300f9cb3b6.
INFO:     127.0.0.1:53224 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:30 logger.py:39] Received request chatcmpl-eda9e0b7b47848ceb6097859c63c06cf: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüêÖüåºüåºüêÖ\nüêÖüê≥üêÖüêÖ\nüåºüêÖüåºüêÖ\nüêÖüå≤üåºüå≤\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:30 async_llm_engine.py:211] Added request chatcmpl-eda9e0b7b47848ceb6097859c63c06cf.
INFO 02-28 20:35:30 async_llm_engine.py:179] Finished request chatcmpl-c09d59b6fa2d442b9970d99227a80102.
INFO:     127.0.0.1:53210 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:30 logger.py:39] Received request chatcmpl-0d78958a47114febb2a1741c0a879a66: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüêøüêøüêºüêº\nüêºüöÄüêºüêº\nüêøüöÄüöÄüêº\n‚ú®‚ú®üöÄüêø\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:30 async_llm_engine.py:211] Added request chatcmpl-0d78958a47114febb2a1741c0a879a66.
INFO 02-28 20:35:33 async_llm_engine.py:179] Finished request chatcmpl-02b602bc7f7c4d90acc10b4fccfa9560.
INFO:     127.0.0.1:53232 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:33 logger.py:39] Received request chatcmpl-bd9bac023d824533bc6662b5602fd009: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüåìüåøüåøüåì\nüåìüê©üåìüê©\nüê©üê©üåøüåì\nüê©üåøüåìüê∞\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:33 async_llm_engine.py:211] Added request chatcmpl-bd9bac023d824533bc6662b5602fd009.
INFO 02-28 20:35:33 metrics.py:455] Avg prompt throughput: 182.9 tokens/s, Avg generation throughput: 215.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-28 20:35:34 async_llm_engine.py:179] Finished request chatcmpl-1b86f0a7b3d549d8a1a93663100ee44b.
INFO:     127.0.0.1:53236 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:34 logger.py:39] Received request chatcmpl-3bfaeec2f6014f009137a7aed3bf6374: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nü¶ÜüêÉüåîüêÉ\nüêÉüåíüåîüêÉ\nüêÉüêÉüåîüêÉ\nüåíüåíüåîüåí\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14813, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:34 async_llm_engine.py:211] Added request chatcmpl-3bfaeec2f6014f009137a7aed3bf6374.
INFO 02-28 20:35:35 async_llm_engine.py:179] Finished request chatcmpl-bd9bac023d824533bc6662b5602fd009.
INFO:     127.0.0.1:36134 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:35 logger.py:39] Received request chatcmpl-0fbe1a12834f4c2bae97c77ef773cf8f: prompt: "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nüéÑü¶äü¶äüéÑ\nüêêüêßü¶äüêê\nüéÑüéÑüêßü¶ä\nüêêüêêü¶äüêß\n\n\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14817, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-28 20:35:35 async_llm_engine.py:211] Added request chatcmpl-0fbe1a12834f4c2bae97c77ef773cf8f.
INFO 02-28 20:35:36 async_llm_engine.py:179] Finished request chatcmpl-48567faaee2142119893b183f1a9ff89.
INFO:     127.0.0.1:36076 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:36 async_llm_engine.py:179] Finished request chatcmpl-eda9e0b7b47848ceb6097859c63c06cf.
INFO:     127.0.0.1:36104 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:37 async_llm_engine.py:179] Finished request chatcmpl-608e2f0d38234951ac058feef0b9ef30.
INFO:     127.0.0.1:36088 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:38 metrics.py:455] Avg prompt throughput: 74.0 tokens/s, Avg generation throughput: 203.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-28 20:35:39 async_llm_engine.py:179] Finished request chatcmpl-6fef4befa804425cab50bf62c158ff41.
INFO:     127.0.0.1:36092 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:41 async_llm_engine.py:179] Finished request chatcmpl-0d78958a47114febb2a1741c0a879a66.
INFO:     127.0.0.1:36120 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 112.3 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-28 20:35:45 async_llm_engine.py:179] Finished request chatcmpl-0fbe1a12834f4c2bae97c77ef773cf8f.
INFO:     127.0.0.1:39014 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:46 async_llm_engine.py:179] Finished request chatcmpl-b9e6fe8bf3844fc6bbfb8b7b8a8cd55f.
INFO:     127.0.0.1:36086 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:35:48 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 53.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-28 20:35:53 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-28 20:35:55 async_llm_engine.py:179] Finished request chatcmpl-3bfaeec2f6014f009137a7aed3bf6374.
INFO:     127.0.0.1:36150 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-28 20:36:08 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.7 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
