INFO 03-02 02:34:45 __init__.py:207] Automatically detected platform cuda.
INFO 03-02 02:34:45 api_server.py:912] vLLM API server version 0.7.3
INFO 03-02 02:34:45 api_server.py:913] args: Namespace(host=None, port=9003, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='/map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Qwen-32B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=15000, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=2, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.95, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['DeepSeek-R1-Distill-Qwen-32B'], qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False)
INFO 03-02 02:34:52 config.py:549] This model supports multiple tasks: {'generate', 'score', 'embed', 'reward', 'classify'}. Defaulting to 'generate'.
INFO 03-02 02:34:52 config.py:1382] Defaulting to use mp for distributed inference
WARNING 03-02 02:34:52 config.py:676] Async output processing can not be enabled with pipeline parallel
INFO 03-02 02:34:52 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Qwen-32B', speculative_config=None, tokenizer='/map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Qwen-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=15000, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=DeepSeek-R1-Distill-Qwen-32B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
WARNING 03-02 02:34:53 multiproc_worker_utils.py:300] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 03-02 02:34:53 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-02 02:34:53 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-02 02:34:53 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-02 02:34:53 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-02 02:34:53 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-02 02:34:53 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-02 02:34:53 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-02 02:34:53 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-02 02:34:54 cuda.py:229] Using Flash Attention backend.
INFO 03-02 02:34:54 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-02 02:34:54 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-02 02:34:54 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-02 02:34:54 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-02 02:34:54 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-02 02:34:54 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-02 02:34:54 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-02 02:34:57 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-02 02:34:57 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-02 02:34:57 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-02 02:34:57 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-02 02:34:57 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-02 02:34:57 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-02 02:34:57 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-02 02:34:57 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-02 02:34:57 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-02 02:34:57 utils.py:916] Found nccl from library libnccl.so.2
INFO 03-02 02:34:57 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-02 02:34:57 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-02 02:34:57 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 03-02 02:34:57 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-02 02:34:57 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-02 02:34:57 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 03-02 02:34:58 custom_all_reduce_utils.py:206] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
INFO 03-02 02:35:22 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-02 02:35:22 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-02 02:35:22 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-02 02:35:22 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-02 02:35:22 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-02 02:35:22 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-02 02:35:22 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-02 02:35:22 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
INFO 03-02 02:35:23 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_4a1f15cc'), local_subscribe_port=40731, remote_subscribe_port=None)
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-02 02:35:23 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_1b6a5997'), local_subscribe_port=51399, remote_subscribe_port=None)
INFO 03-02 02:35:23 utils.py:916] Found nccl from library libnccl.so.2
INFO 03-02 02:35:23 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-02 02:35:23 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-02 02:35:23 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-02 02:35:23 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-02 02:35:23 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-02 02:35:23 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-02 02:35:23 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-02 02:35:23 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-02 02:35:23 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-02 02:35:23 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-02 02:35:23 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-02 02:35:23 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-02 02:35:23 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-02 02:35:23 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-02 02:35:23 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 03-02 02:35:23 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Qwen-32B...
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-02 02:35:23 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Qwen-32B...
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-02 02:35:23 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Qwen-32B...
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-02 02:35:23 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Qwen-32B...
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-02 02:35:23 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Qwen-32B...
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-02 02:35:23 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Qwen-32B...
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-02 02:35:23 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Qwen-32B...
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-02 02:35:23 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Qwen-32B...
INFO 03-02 02:35:50 model_runner.py:1115] Loading model weights took 7.7215 GB
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-02 02:35:50 model_runner.py:1115] Loading model weights took 7.7215 GB
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-02 02:35:50 model_runner.py:1115] Loading model weights took 7.7215 GB
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-02 02:35:50 model_runner.py:1115] Loading model weights took 7.7215 GB
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-02 02:35:50 model_runner.py:1115] Loading model weights took 7.7215 GB
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-02 02:35:50 model_runner.py:1115] Loading model weights took 7.7215 GB
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-02 02:35:50 model_runner.py:1115] Loading model weights took 7.7215 GB
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-02 02:35:50 model_runner.py:1115] Loading model weights took 7.7215 GB
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-02 02:35:52 worker.py:267] Memory profiling takes 1.87 seconds
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-02 02:35:52 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-02 02:35:52 worker.py:267] model weights take 7.72GiB; non_torch_memory takes 1.70GiB; PyTorch activation peak memory takes 1.45GiB; the rest of the memory reserved for KV Cache is 64.51GiB.
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-02 02:35:52 worker.py:267] Memory profiling takes 1.87 seconds
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-02 02:35:52 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-02 02:35:52 worker.py:267] model weights take 7.72GiB; non_torch_memory takes 1.51GiB; PyTorch activation peak memory takes 1.45GiB; the rest of the memory reserved for KV Cache is 64.70GiB.
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-02 02:35:52 worker.py:267] Memory profiling takes 1.87 seconds
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-02 02:35:52 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-02 02:35:52 worker.py:267] model weights take 7.72GiB; non_torch_memory takes 1.70GiB; PyTorch activation peak memory takes 1.45GiB; the rest of the memory reserved for KV Cache is 64.51GiB.
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-02 02:35:52 worker.py:267] Memory profiling takes 2.05 seconds
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-02 02:35:52 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-02 02:35:52 worker.py:267] model weights take 7.72GiB; non_torch_memory takes 1.70GiB; PyTorch activation peak memory takes 1.82GiB; the rest of the memory reserved for KV Cache is 64.14GiB.
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-02 02:35:55 worker.py:267] Memory profiling takes 4.96 seconds
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-02 02:35:55 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-02 02:35:55 worker.py:267] model weights take 7.72GiB; non_torch_memory takes 1.03GiB; PyTorch activation peak memory takes 1.30GiB; the rest of the memory reserved for KV Cache is 65.32GiB.
INFO 03-02 02:35:55 worker.py:267] Memory profiling takes 4.97 seconds
INFO 03-02 02:35:55 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
INFO 03-02 02:35:55 worker.py:267] model weights take 7.72GiB; non_torch_memory takes 0.94GiB; PyTorch activation peak memory takes 1.30GiB; the rest of the memory reserved for KV Cache is 65.42GiB.
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-02 02:35:55 worker.py:267] Memory profiling takes 4.97 seconds
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-02 02:35:55 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-02 02:35:55 worker.py:267] model weights take 7.72GiB; non_torch_memory takes 0.94GiB; PyTorch activation peak memory takes 1.30GiB; the rest of the memory reserved for KV Cache is 65.42GiB.
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-02 02:35:55 worker.py:267] Memory profiling takes 4.98 seconds
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-02 02:35:55 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-02 02:35:55 worker.py:267] model weights take 7.72GiB; non_torch_memory takes 1.03GiB; PyTorch activation peak memory takes 1.30GiB; the rest of the memory reserved for KV Cache is 65.32GiB.
INFO 03-02 02:35:55 executor_base.py:111] # cuda blocks: 131365, # CPU blocks: 8192
INFO 03-02 02:35:55 executor_base.py:116] Maximum concurrency for 15000 tokens per request: 140.12x
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-02 02:35:58 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-02 02:35:58 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-02 02:35:58 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-02 02:35:58 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-02 02:35:58 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-02 02:35:58 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-02 02:35:58 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 03-02 02:35:58 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-02 02:36:29 custom_all_reduce.py:226] Registering 4480 cuda graph addresses
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-02 02:36:29 custom_all_reduce.py:226] Registering 4480 cuda graph addresses
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-02 02:36:29 custom_all_reduce.py:226] Registering 4480 cuda graph addresses
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-02 02:36:29 custom_all_reduce.py:226] Registering 4480 cuda graph addresses
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-02 02:36:29 model_runner.py:1562] Graph capturing finished in 31 secs, took 1.87 GiB
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-02 02:36:29 model_runner.py:1562] Graph capturing finished in 31 secs, took 1.87 GiB
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-02 02:36:29 model_runner.py:1562] Graph capturing finished in 31 secs, took 1.87 GiB
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-02 02:36:29 model_runner.py:1562] Graph capturing finished in 31 secs, took 1.87 GiB
INFO 03-02 02:36:32 custom_all_reduce.py:226] Registering 4550 cuda graph addresses
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-02 02:36:32 custom_all_reduce.py:226] Registering 4550 cuda graph addresses
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-02 02:36:33 custom_all_reduce.py:226] Registering 4550 cuda graph addresses
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-02 02:36:33 custom_all_reduce.py:226] Registering 4550 cuda graph addresses
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-02 02:36:34 model_runner.py:1562] Graph capturing finished in 35 secs, took 1.90 GiB
INFO 03-02 02:36:34 model_runner.py:1562] Graph capturing finished in 35 secs, took 1.90 GiB
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-02 02:36:34 model_runner.py:1562] Graph capturing finished in 35 secs, took 1.90 GiB
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-02 02:36:34 model_runner.py:1562] Graph capturing finished in 36 secs, took 1.90 GiB
INFO 03-02 02:36:34 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 43.53 seconds
INFO 03-02 02:36:36 api_server.py:958] Starting vLLM API server on http://0.0.0.0:9003
INFO 03-02 02:36:36 launcher.py:23] Available routes are:
INFO 03-02 02:36:36 launcher.py:31] Route: /openapi.json, Methods: HEAD, GET
INFO 03-02 02:36:36 launcher.py:31] Route: /docs, Methods: HEAD, GET
INFO 03-02 02:36:36 launcher.py:31] Route: /docs/oauth2-redirect, Methods: HEAD, GET
INFO 03-02 02:36:36 launcher.py:31] Route: /redoc, Methods: HEAD, GET
INFO 03-02 02:36:36 launcher.py:31] Route: /health, Methods: GET
INFO 03-02 02:36:36 launcher.py:31] Route: /ping, Methods: POST, GET
INFO 03-02 02:36:36 launcher.py:31] Route: /tokenize, Methods: POST
INFO 03-02 02:36:36 launcher.py:31] Route: /detokenize, Methods: POST
INFO 03-02 02:36:36 launcher.py:31] Route: /v1/models, Methods: GET
INFO 03-02 02:36:36 launcher.py:31] Route: /version, Methods: GET
INFO 03-02 02:36:36 launcher.py:31] Route: /v1/chat/completions, Methods: POST
INFO 03-02 02:36:36 launcher.py:31] Route: /v1/completions, Methods: POST
INFO 03-02 02:36:36 launcher.py:31] Route: /v1/embeddings, Methods: POST
INFO 03-02 02:36:36 launcher.py:31] Route: /pooling, Methods: POST
INFO 03-02 02:36:36 launcher.py:31] Route: /score, Methods: POST
INFO 03-02 02:36:36 launcher.py:31] Route: /v1/score, Methods: POST
INFO 03-02 02:36:36 launcher.py:31] Route: /v1/audio/transcriptions, Methods: POST
INFO 03-02 02:36:36 launcher.py:31] Route: /rerank, Methods: POST
INFO 03-02 02:36:36 launcher.py:31] Route: /v1/rerank, Methods: POST
INFO 03-02 02:36:36 launcher.py:31] Route: /v2/rerank, Methods: POST
INFO 03-02 02:36:36 launcher.py:31] Route: /invocations, Methods: POST
INFO 03-02 02:41:33 chat_utils.py:332] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
INFO 03-02 02:41:33 logger.py:39] Received request chatcmpl-51a2e45ff889403fb6ede2278816e16a: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ‹ğŸ‹ğŸš€ğŸ‹ğŸš€ğŸšğŸŒ³\nğŸšğŸŒ³ğŸš€ğŸŒ³ğŸ¦©ğŸšğŸš€\nğŸšğŸšğŸŒ³ğŸš€ğŸ¦©ğŸ¦©ğŸš€\nğŸ‹ğŸšğŸ¦©ğŸš€ğŸ‹ğŸš€ğŸš\nğŸŒ³ğŸšğŸ‹ğŸ‹ğŸšğŸš€ğŸ¦©\nğŸšğŸ¦©ğŸ¦©ğŸ‹ğŸ¦©ğŸšğŸ‹\nğŸš€ğŸŒ³ğŸš€ğŸ¦©ğŸŒ³ğŸšğŸŒ³\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14797, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:41:33 async_llm_engine.py:211] Added request chatcmpl-51a2e45ff889403fb6ede2278816e16a.
INFO 03-02 02:41:33 logger.py:39] Received request chatcmpl-5334ca09b3a84f7488214807bca2fe54: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ³ğŸ³ğŸ³ğŸ³ğŸŒğŸ¦…ğŸ³\nğŸ¦…ğŸ³ğŸ³ğŸ¤ğŸ¦…ğŸ³ğŸ°\nğŸŒğŸ¤ğŸ¦…ğŸ°ğŸ¦…ğŸ¦…ğŸ¦…\nğŸ¤ğŸ¦…ğŸ³ğŸŒğŸ³ğŸ¦…ğŸ¤\nğŸŒğŸ³ğŸ³ğŸ¤ğŸŒğŸ°ğŸŒ\nğŸŒğŸ³ğŸ°ğŸ¦…ğŸ³ğŸ¤ğŸ¦…\nğŸŒğŸ°ğŸ¦…ğŸ°ğŸ¤ğŸ³ğŸ¤\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14806, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:41:33 async_llm_engine.py:211] Added request chatcmpl-5334ca09b3a84f7488214807bca2fe54.
INFO 03-02 02:41:33 logger.py:39] Received request chatcmpl-b2daab115dee4fb9a15b84f321f6c8eb: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¯ğŸ¯ğŸ¦‹ğŸ®ğŸ¦‹ğŸ­ğŸ¦‹\nğŸ®ğŸˆğŸˆğŸ¦‹ğŸ¯ğŸ¦‹ğŸˆ\nğŸ¦‹ğŸ¯ğŸ­ğŸ­ğŸ¯ğŸˆğŸ¯\nğŸˆğŸˆğŸ¦‹ğŸ®ğŸ¦‹ğŸ®ğŸ®\nğŸ®ğŸ­ğŸ®ğŸ¯ğŸ®ğŸ®ğŸ¦‹\nğŸ¦‹ğŸ¯ğŸ¦‹ğŸ¦‹ğŸ®ğŸˆğŸˆ\nğŸ¦‹ğŸ¯ğŸ¦‹ğŸ¯ğŸ¯ğŸˆğŸ®\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14806, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:41:33 async_llm_engine.py:211] Added request chatcmpl-b2daab115dee4fb9a15b84f321f6c8eb.
INFO 03-02 02:41:33 logger.py:39] Received request chatcmpl-ec8a65fbe1dc4e0d94655a811dd630cc: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¦ƒğŸ¦†ğŸ¦ƒğŸ¦€ğŸ¦ƒğŸ¦€ğŸ¦†\nğŸŒ‘ğŸ¦ƒâ˜„ï¸â˜„ï¸ğŸ¦†ğŸ¦ƒğŸ¦ƒ\nğŸ¦†ğŸ¦†â˜„ï¸â˜„ï¸â˜„ï¸ğŸ¦ƒğŸ¦†\nğŸ¦€ğŸ¦€â˜„ï¸ğŸ¦ƒğŸ¦€ğŸŒ‘ğŸ¦€\nğŸŒ‘ğŸ¦†ğŸ¦ƒğŸ¦†ğŸ¦†ğŸ¦†ğŸ¦ƒ\nğŸ¦†â˜„ï¸ğŸŒ‘ğŸ¦€ğŸ¦€â˜„ï¸ğŸ¦†\nâ˜„ï¸ğŸŒ‘ğŸ¦†ğŸŒ‘ğŸ¦†ğŸ¦ƒğŸ¦†\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14797, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:41:33 async_llm_engine.py:211] Added request chatcmpl-ec8a65fbe1dc4e0d94655a811dd630cc.
INFO 03-02 02:41:33 logger.py:39] Received request chatcmpl-1f903b85fc3844fb899af817a8ca1445: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ«ğŸ¦€ğŸ¦€ğŸ¦€ğŸ¦¥ğŸ¥€ğŸµ\nğŸ¦€ğŸ«ğŸ¥€ğŸ¥€ğŸµğŸµğŸ«\nğŸ¦€ğŸµğŸ«ğŸ¦€ğŸµğŸµğŸ«\nğŸ«ğŸµğŸµğŸµğŸ¦€ğŸ¥€ğŸµ\nğŸ¦¥ğŸ¥€ğŸ«ğŸ«ğŸ«ğŸ¦¥ğŸµ\nğŸ¦€ğŸ¥€ğŸµğŸ¦€ğŸµğŸ¥€ğŸµ\nğŸ¥€ğŸ«ğŸ¦¥ğŸ¦¥ğŸ¥€ğŸµğŸ¦¥\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14800, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:41:33 async_llm_engine.py:211] Added request chatcmpl-1f903b85fc3844fb899af817a8ca1445.
INFO 03-02 02:41:33 logger.py:39] Received request chatcmpl-bf90f753d6474e4da2dd8cfdd27c7575: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸŒ•ğŸ³ğŸ’§ğŸ³ğŸ’§ğŸ¦®ğŸŒ•\nğŸ³ğŸŒ•ğŸ¦­ğŸ’§ğŸ¦­ğŸ³ğŸ’§\nğŸ³ğŸŒ•ğŸ¦®ğŸ³ğŸŒ•ğŸ³ğŸ’§\nğŸ³ğŸ¦­ğŸ¦­ğŸŒ•ğŸ³ğŸ³ğŸ’§\nğŸ’§ğŸ³ğŸ³ğŸ³ğŸ³ğŸ¦®ğŸ¦®\nğŸ³ğŸ¦­ğŸ³ğŸ³ğŸ³ğŸŒ•ğŸ¦®\nğŸ’§ğŸ¦®ğŸŒ•ğŸ³ğŸ³ğŸ¦®ğŸ’§\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14794, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:41:33 async_llm_engine.py:211] Added request chatcmpl-bf90f753d6474e4da2dd8cfdd27c7575.
INFO 03-02 02:41:33 logger.py:39] Received request chatcmpl-95a0d83af5614d3093509f9af261773d: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸŒ™ğŸ¨ğŸ¤šğŸ‹ğŸŒ¼ğŸŒ¼ğŸŒ™\nğŸŒ¼ğŸ‹ğŸ¨ğŸ‹ğŸ¤šğŸ¤šğŸŒ™\nğŸ¨ğŸ¤šğŸ¤šğŸŒ¼ğŸ¤šğŸŒ™ğŸ‹\nğŸŒ¼ğŸŒ™ğŸŒ¼ğŸ‹ğŸŒ¼ğŸ‹ğŸŒ™\nğŸ¨ğŸŒ¼ğŸŒ™ğŸŒ™ğŸ¤šğŸ¨ğŸŒ¼\nğŸ‹ğŸ¨ğŸ¤šğŸ¤šğŸ¤šğŸŒ™ğŸŒ¼\nğŸ¨ğŸ¤šğŸ‹ğŸŒ¼ğŸŒ¼ğŸ¤šğŸŒ™\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14806, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:41:33 async_llm_engine.py:211] Added request chatcmpl-95a0d83af5614d3093509f9af261773d.
INFO 03-02 02:41:33 logger.py:39] Received request chatcmpl-fed12493e84449c4842b43c799414949: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ»ğŸ„ğŸ¦¥ğŸ»ğŸ„ğŸ»ğŸ—\nğŸ„ğŸ„ğŸ¶ğŸ¶ğŸ„ğŸ—ğŸ„\nğŸ»ğŸ¶ğŸ„ğŸ»ğŸ„ğŸ—ğŸ»\nğŸ„ğŸ»ğŸ»ğŸ¶ğŸ„ğŸ—ğŸ„\nğŸ»ğŸ—ğŸ¦¥ğŸ¶ğŸ—ğŸ»ğŸ„\nğŸ»ğŸ¦¥ğŸ»ğŸ—ğŸ„ğŸ»ğŸ»\nğŸ—ğŸ¦¥ğŸ„ğŸ»ğŸ„ğŸ»ğŸ„\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14802, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:41:33 async_llm_engine.py:211] Added request chatcmpl-fed12493e84449c4842b43c799414949.
INFO 03-02 02:41:34 metrics.py:455] Avg prompt throughput: 52.0 tokens/s, Avg generation throughput: 0.3 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 02:41:39 metrics.py:455] Avg prompt throughput: 238.8 tokens/s, Avg generation throughput: 371.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 02:41:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 371.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-02 02:41:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 370.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-02 02:41:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 367.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 02:41:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 365.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-02 02:42:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 362.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 03-02 02:42:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 356.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 03-02 02:42:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 03-02 02:42:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 03-02 02:42:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 345.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 03-02 02:42:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 340.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 03-02 02:42:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 338.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 03-02 02:42:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 332.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 03-02 02:42:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 330.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-02 02:42:47 async_llm_engine.py:179] Finished request chatcmpl-b2daab115dee4fb9a15b84f321f6c8eb.
INFO:     127.0.0.1:37520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:42:47 logger.py:39] Received request chatcmpl-7e2ff89af9c442f4ab6000e59293ed5f: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¦ğŸ¦ğŸ¦ƒğŸ¦ƒğŸ•·ğŸ¦ƒğŸ¥\nğŸ¦ğŸ¥ğŸ¥ğŸ¥ğŸ¥ğŸ¥ğŸ•·\nğŸ¦ƒğŸ¥ğŸ¦ğŸ¥ğŸ¦ƒğŸ¦ğŸ¦œ\nğŸ•·ğŸ¥ğŸ¥ğŸ¥ğŸ¦ğŸ¦œğŸ¦\nğŸ¦ğŸ¥ğŸ•·ğŸ¥ğŸ¦ğŸ¦ğŸ¦œ\nğŸ¦ƒğŸ¦ğŸ¥ğŸ•·ğŸ¦ƒğŸ•·ğŸ¦ƒ\nğŸ•·ğŸ¦ğŸ•·ğŸ¦ğŸ¥ğŸ¦ƒğŸ¦ƒ\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14803, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:42:47 async_llm_engine.py:211] Added request chatcmpl-7e2ff89af9c442f4ab6000e59293ed5f.
INFO 03-02 02:42:49 metrics.py:455] Avg prompt throughput: 39.4 tokens/s, Avg generation throughput: 327.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 03-02 02:42:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 330.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-02 02:42:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 328.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-02 02:43:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 325.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-02 02:43:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 324.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-02 02:43:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 316.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-02 02:43:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 312.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-02 02:43:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 313.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-02 02:43:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 306.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-02 02:43:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 305.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-02 02:43:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 303.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-02 02:43:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 300.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-02 02:43:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-02 02:43:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 295.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-02 02:43:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-02 02:44:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 290.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-02 02:44:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 286.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-02 02:44:10 async_llm_engine.py:179] Finished request chatcmpl-5334ca09b3a84f7488214807bca2fe54.
INFO:     127.0.0.1:37510 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:44:10 logger.py:39] Received request chatcmpl-c18ede0e157b453fb8e0fa4c9639133c: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nâ›…ï¸ğŸŒ¾ğŸ¦šğŸ¦‰ğŸŠğŸ¦‰ğŸŒ¾\nğŸ¦šğŸ¦‰ğŸ¦‰ğŸŒ¾ğŸŠâ›…ï¸ğŸŠ\nğŸŠâ›…ï¸â›…ï¸â›…ï¸ğŸŒ¾ğŸŒ¾â›…ï¸\nğŸ¦‰ğŸ¦‰ğŸŒ¾ğŸŠğŸŒ¾ğŸ¦šâ›…ï¸\nâ›…ï¸ğŸŠğŸŒ¾ğŸ¦šâ›…ï¸ğŸŠğŸ¦š\nğŸ¦šğŸŒ¾â›…ï¸ğŸŒ¾ğŸŠğŸŒ¾ğŸŠ\nğŸŠâ›…ï¸ğŸŒ¾â›…ï¸ğŸŠğŸŒ¾ğŸ¦‰\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14794, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:44:10 async_llm_engine.py:211] Added request chatcmpl-c18ede0e157b453fb8e0fa4c9639133c.
INFO 03-02 02:44:14 metrics.py:455] Avg prompt throughput: 41.1 tokens/s, Avg generation throughput: 288.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-02 02:44:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 291.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-02 02:44:19 async_llm_engine.py:179] Finished request chatcmpl-bf90f753d6474e4da2dd8cfdd27c7575.
INFO:     127.0.0.1:37536 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:44:19 logger.py:39] Received request chatcmpl-788087ce7ba84aef86f2e42228caf51d: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸŒ™ğŸ§½ğŸŒ·ğŸŒ·ğŸŒ±ğŸ§½ğŸŒ™\nâ­ï¸ğŸŒ·ğŸŒ±ğŸŒ™ğŸ§½ğŸŒ±ğŸŒ™\nğŸŒ·ğŸ§½â­ï¸ğŸŒ±ğŸŒ·ğŸŒ·ğŸŒ±\nğŸŒ±ğŸŒ·ğŸ§½ğŸŒ±ğŸŒ±ğŸ§½ğŸŒ·\nğŸ§½ğŸŒ·â­ï¸ğŸŒ·ğŸŒ™â­ï¸ğŸŒ™\nğŸ§½ğŸ§½ğŸ§½ğŸŒ±â­ï¸ğŸŒ±ğŸ§½\nğŸŒ±â­ï¸ğŸŒ·ğŸŒ·ğŸŒ·ğŸŒ™â­ï¸\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14788, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:44:19 async_llm_engine.py:211] Added request chatcmpl-788087ce7ba84aef86f2e42228caf51d.
INFO 03-02 02:44:24 metrics.py:455] Avg prompt throughput: 42.3 tokens/s, Avg generation throughput: 299.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-02 02:44:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-02 02:44:33 async_llm_engine.py:179] Finished request chatcmpl-fed12493e84449c4842b43c799414949.
INFO:     127.0.0.1:37558 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:44:33 logger.py:39] Received request chatcmpl-40e171e697da4707acf643b3ef864d3d: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸŒšğŸŒšâœ¨ğŸšâœ¨ğŸŒšğŸƒ\nğŸšğŸŒºğŸŒšğŸƒâœ¨ğŸƒâœ¨\nğŸšğŸšâœ¨ğŸŒšâœ¨ğŸƒâœ¨\nğŸƒğŸƒğŸƒğŸŒšğŸŒšâœ¨ğŸƒ\nğŸšğŸŒºğŸšâœ¨ğŸŒšâœ¨ğŸŒš\nğŸŒºğŸšğŸƒğŸƒğŸšğŸŒºğŸš\nâœ¨ğŸƒğŸŒºğŸŒºğŸŒšâœ¨ğŸŒº\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14806, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:44:33 async_llm_engine.py:211] Added request chatcmpl-40e171e697da4707acf643b3ef864d3d.
INFO 03-02 02:44:34 metrics.py:455] Avg prompt throughput: 38.8 tokens/s, Avg generation throughput: 299.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-02 02:44:35 async_llm_engine.py:179] Finished request chatcmpl-7e2ff89af9c442f4ab6000e59293ed5f.
INFO:     127.0.0.1:48088 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:44:35 logger.py:39] Received request chatcmpl-359c396836214a65838646943ae2f090: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ•ğŸŒ¹ğŸŒ•ğŸ•ğŸŒ•ğŸ•ğŸ¦‡\nğŸŠğŸ¦‡ğŸŒ¹ğŸ¦‡ğŸŠğŸ•ğŸŒ•\nğŸŠğŸŒ•ğŸŠğŸŒ¹ğŸ¦‡ğŸŠğŸŒ¹\nğŸ¦‡ğŸŒ•ğŸ•ğŸŒ¹ğŸŒ¹ğŸŒ¹ğŸ¦‡\nğŸŠğŸŒ¹ğŸ•ğŸ•ğŸŒ¹ğŸ•ğŸŠ\nğŸ¦‡ğŸŒ•ğŸ¦‡ğŸ¦‡ğŸŠğŸŒ¹ğŸ¦‡\nğŸŠğŸ¦‡ğŸŒ¹ğŸ•ğŸ¦‡ğŸ•ğŸ¦‡\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14806, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:44:35 async_llm_engine.py:211] Added request chatcmpl-359c396836214a65838646943ae2f090.
INFO 03-02 02:44:39 metrics.py:455] Avg prompt throughput: 38.7 tokens/s, Avg generation throughput: 309.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-02 02:44:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 309.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-02 02:44:45 async_llm_engine.py:179] Finished request chatcmpl-1f903b85fc3844fb899af817a8ca1445.
INFO:     127.0.0.1:37530 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:44:45 logger.py:39] Received request chatcmpl-798eacec392143329e01a7ad1d18b96b: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¦ğŸ¦ğŸ¦ğŸ‹ğŸºğŸ¦ğŸ¦Ÿ\nğŸ‹ğŸ¦ğŸ¦ğŸºğŸ‹ğŸ¦ŸğŸ¦\nğŸ‹ğŸ¦ŸğŸ¦ğŸ¦ğŸ¦ğŸ¦ŸğŸ¦Ÿ\nğŸ¦ğŸ¦ğŸºğŸºğŸ¦ğŸ¦ŸğŸ¦\nğŸ¦ŸğŸ‹ğŸ¦ŸğŸºğŸ¦ŸğŸ¦ğŸº\nğŸºğŸ¦ğŸ¦ğŸ¦ğŸºğŸ¦ğŸ¦Ÿ\nğŸ¦ğŸ¦ğŸºğŸ¦ŸğŸ¦ŸğŸ¦ŸğŸ¦\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14771, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:44:45 async_llm_engine.py:211] Added request chatcmpl-798eacec392143329e01a7ad1d18b96b.
INFO 03-02 02:44:49 metrics.py:455] Avg prompt throughput: 45.7 tokens/s, Avg generation throughput: 317.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-02 02:44:49 async_llm_engine.py:179] Finished request chatcmpl-51a2e45ff889403fb6ede2278816e16a.
INFO:     127.0.0.1:37500 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:44:49 logger.py:39] Received request chatcmpl-128f9e329dd24346b380ca67acf25dba: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸŒğŸŒğŸ”ğŸŸğŸŒğŸŸğŸŸ\nğŸŸğŸ”ğŸŒâ˜ï¸ğŸŒğŸŒğŸŒ\nğŸŒğŸ…â˜ï¸â˜ï¸ğŸ…ğŸŒâ˜ï¸\nğŸ”ğŸ”ğŸ”ğŸ…ğŸŒğŸ”ğŸ”\nğŸŒâ˜ï¸ğŸŸğŸŸğŸ…ğŸŸğŸ…\nğŸ”ğŸ”ğŸ…ğŸ…ğŸŒâ˜ï¸ğŸ”\nğŸŸğŸ…â˜ï¸ğŸŒğŸŸğŸ…â˜ï¸\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14798, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:44:49 async_llm_engine.py:211] Added request chatcmpl-128f9e329dd24346b380ca67acf25dba.
INFO 03-02 02:44:54 metrics.py:455] Avg prompt throughput: 40.3 tokens/s, Avg generation throughput: 255.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 03-02 02:44:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 197.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 03-02 02:45:04 async_llm_engine.py:179] Finished request chatcmpl-ec8a65fbe1dc4e0d94655a811dd630cc.
INFO:     127.0.0.1:37528 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:45:04 logger.py:39] Received request chatcmpl-ba0f21741ec644b98c43846defd39492: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ˜„ğŸ¦ğŸ¦ğŸ˜„ğŸŒğŸ˜„ğŸ¦\nğŸŒğŸ¤šğŸŒğŸŒğŸ¤šğŸŒğŸŒ\nğŸ˜„ğŸ˜„ğŸŒğŸŒğŸ˜„ğŸŒğŸ¦„\nğŸ¤šğŸ˜„ğŸ¦„ğŸ¤šğŸ¤šğŸ¤šğŸ¦\nğŸ¤šğŸ¦„ğŸŒğŸ¤šğŸ¦„ğŸŒğŸ¤š\nğŸ¤šğŸ¦ğŸ¤šğŸ¦ğŸ˜„ğŸ˜„ğŸ¦„\nğŸ˜„ğŸ¦„ğŸ¦ğŸŒğŸ¦„ğŸ¤šğŸ˜„\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14806, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:45:04 async_llm_engine.py:211] Added request chatcmpl-ba0f21741ec644b98c43846defd39492.
INFO 03-02 02:45:04 metrics.py:455] Avg prompt throughput: 38.7 tokens/s, Avg generation throughput: 200.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 03-02 02:45:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 216.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 03-02 02:45:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 213.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 03-02 02:45:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 214.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 03-02 02:45:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 213.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 03-02 02:45:26 async_llm_engine.py:179] Finished request chatcmpl-95a0d83af5614d3093509f9af261773d.
INFO:     127.0.0.1:37550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:45:26 logger.py:39] Received request chatcmpl-e1addcb244664a378a62f3f338cb3f07: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸŒ»ğŸ¦ğŸˆğŸ¦ğŸŒ·ğŸŒ»ğŸŒ»\nğŸˆğŸŒ·ğŸŒ·ğŸŒ·ğŸŒ»ğŸŒ²ğŸŒ·\nğŸˆğŸŒ»ğŸŒ²ğŸŒ·ğŸˆğŸŒ»ğŸŒ²\nğŸˆğŸ¦ğŸ¦ğŸŒ²ğŸŒ·ğŸŒ»ğŸŒ·\nğŸŒ·ğŸŒ·ğŸ¦ğŸˆğŸŒ·ğŸ¦ğŸ¦\nğŸŒ²ğŸŒ»ğŸŒ²ğŸŒ·ğŸŒ²ğŸˆğŸŒ·\nğŸˆğŸˆğŸŒ²ğŸŒ·ğŸŒ»ğŸ¦ğŸŒ»\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14806, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:45:26 async_llm_engine.py:211] Added request chatcmpl-e1addcb244664a378a62f3f338cb3f07.
INFO 03-02 02:45:29 metrics.py:455] Avg prompt throughput: 38.7 tokens/s, Avg generation throughput: 311.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 03-02 02:45:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 354.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 03-02 02:45:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 352.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 03-02 02:45:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 345.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 03-02 02:45:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 339.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 03-02 02:45:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 03-02 02:45:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 334.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 03-02 02:46:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 331.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 03-02 02:46:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 326.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-02 02:46:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 322.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-02 02:46:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 320.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-02 02:46:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 318.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-02 02:46:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 315.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-02 02:46:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 311.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-02 02:46:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 308.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-02 02:46:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 306.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-02 02:46:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 304.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-02 02:46:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-02 02:46:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-02 02:47:04 async_llm_engine.py:179] Finished request chatcmpl-128f9e329dd24346b380ca67acf25dba.
INFO:     127.0.0.1:42316 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:47:04 logger.py:39] Received request chatcmpl-f0bd7677bdcf46b886f0bf640f878f32: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸŒ²ğŸ¦ğŸ«ğŸŒğŸŒğŸŒ²ğŸ“\nğŸ“ğŸ«ğŸ¦ğŸ«ğŸ“ğŸŒ²ğŸ¦\nğŸŒğŸŒ²ğŸŒ²ğŸŒ²ğŸ¦ğŸ¦ğŸŒ²\nğŸ«ğŸ¦ğŸŒğŸŒğŸ«ğŸ“ğŸŒ\nğŸ¦ğŸ“ğŸ¦ğŸŒ²ğŸ“ğŸ¦ğŸ«\nğŸŒğŸ“ğŸ«ğŸ¦ğŸŒğŸ«ğŸŒ\nğŸ“ğŸ«ğŸ¦ğŸŒğŸ“ğŸŒ²ğŸŒ\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14795, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:47:04 async_llm_engine.py:211] Added request chatcmpl-f0bd7677bdcf46b886f0bf640f878f32.
INFO 03-02 02:47:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 294.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-02 02:47:09 async_llm_engine.py:179] Finished request chatcmpl-359c396836214a65838646943ae2f090.
INFO:     127.0.0.1:58272 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:47:09 logger.py:39] Received request chatcmpl-ccbc262ef5ea476794de9db5ca8315d3: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ£ğŸğŸğŸ£ğŸğŸ‹ğŸ\nğŸ‹ğŸ£ğŸ‹ğŸœğŸ‹ğŸ¡ğŸ£\nğŸ£ğŸ£ğŸ£ğŸ£ğŸ‹ğŸ¡ğŸ¡\nğŸ£ğŸğŸ£ğŸ‹ğŸ‹ğŸœğŸ‹\nğŸğŸ£ğŸğŸ¡ğŸ‹ğŸ¡ğŸ¡\nğŸ¡ğŸ‹ğŸ‹ğŸ¡ğŸ‹ğŸ¡ğŸœ\nğŸ‹ğŸğŸ¡ğŸ¡ğŸ£ğŸğŸ£\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14797, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:47:09 async_llm_engine.py:211] Added request chatcmpl-ccbc262ef5ea476794de9db5ca8315d3.
INFO 03-02 02:47:09 metrics.py:455] Avg prompt throughput: 40.9 tokens/s, Avg generation throughput: 295.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-02 02:47:14 metrics.py:455] Avg prompt throughput: 40.6 tokens/s, Avg generation throughput: 303.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-02 02:47:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 304.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-02 02:47:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 302.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-02 02:47:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 300.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-02 02:47:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 296.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-02 02:47:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-02 02:47:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 291.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-02 02:47:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 220.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-02 02:47:51 async_llm_engine.py:179] Finished request chatcmpl-40e171e697da4707acf643b3ef864d3d.
INFO:     127.0.0.1:58262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:47:51 logger.py:39] Received request chatcmpl-ccc19605e1bb4a61bfba6707a8af3de4: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸšğŸ¤ğŸ¢ğŸ¤ğŸ¢ğŸŒğŸ¢\nğŸ¤ğŸğŸ¤ğŸšğŸ¤ğŸ¤ğŸ\nğŸ¤ğŸ¤ğŸğŸ¢ğŸ¢ğŸŒğŸ\nğŸŒğŸğŸ¢ğŸšğŸšğŸğŸ\nğŸšğŸğŸğŸ¢ğŸšğŸšğŸš\nğŸ¤ğŸ¢ğŸ¢ğŸŒğŸšğŸŒğŸ¢\nğŸ¢ğŸğŸŒğŸšğŸŒğŸŒğŸŒ\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14806, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:47:51 async_llm_engine.py:211] Added request chatcmpl-ccc19605e1bb4a61bfba6707a8af3de4.
INFO 03-02 02:47:54 metrics.py:455] Avg prompt throughput: 38.7 tokens/s, Avg generation throughput: 200.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-02 02:47:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 200.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-02 02:48:02 async_llm_engine.py:179] Finished request chatcmpl-788087ce7ba84aef86f2e42228caf51d.
INFO:     127.0.0.1:59562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:48:02 logger.py:39] Received request chatcmpl-8767339f430842f792a8b085cd6be22a: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ‹ğŸ—ğŸğŸğŸ—ğŸ—ğŸ‹\nğŸ¸ğŸğŸŒ™ğŸğŸğŸ‹ğŸ‹\nğŸ¸ğŸ¸ğŸ¸ğŸğŸ—ğŸŒ™ğŸ\nğŸğŸ—ğŸ‹ğŸŒ™ğŸ‹ğŸ—ğŸ‹\nğŸ—ğŸ—ğŸ‹ğŸ¸ğŸ¸ğŸ¸ğŸ¸\nğŸ‹ğŸŒ™ğŸ—ğŸ—ğŸ¸ğŸ‹ğŸ‹\nğŸŒ™ğŸ‹ğŸ—ğŸ—ğŸŒ™ğŸ¸ğŸ¸\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14806, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:48:02 async_llm_engine.py:211] Added request chatcmpl-8767339f430842f792a8b085cd6be22a.
INFO 03-02 02:48:04 metrics.py:455] Avg prompt throughput: 38.8 tokens/s, Avg generation throughput: 201.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-02 02:48:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-02 02:48:11 async_llm_engine.py:179] Finished request chatcmpl-c18ede0e157b453fb8e0fa4c9639133c.
INFO:     127.0.0.1:60928 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:48:11 logger.py:39] Received request chatcmpl-2c926f77d20d457db2688c703a2733e6: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¥€ğŸ¦•ğŸŒ³ğŸ¦•ğŸ¦•ğŸŒ³ğŸ›\nğŸ›ğŸ¦ğŸ¦•ğŸ¥€ğŸ›ğŸŒ³ğŸŒ³\nğŸŒ³ğŸ›ğŸ¥€ğŸ¥€ğŸ¦•ğŸ¥€ğŸ›\nğŸ¦ğŸ¦•ğŸ¥€ğŸ›ğŸ¦ğŸ›ğŸŒ³\nğŸ›ğŸ›ğŸ›ğŸ¥€ğŸŒ³ğŸ¦•ğŸ¦\nğŸ¥€ğŸ¥€ğŸŒ³ğŸŒ³ğŸ›ğŸ¥€ğŸŒ³\nğŸ›ğŸŒ³ğŸŒ³ğŸ¦•ğŸ›ğŸ›ğŸ¦•\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14806, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:48:11 async_llm_engine.py:211] Added request chatcmpl-2c926f77d20d457db2688c703a2733e6.
INFO 03-02 02:48:11 async_llm_engine.py:179] Finished request chatcmpl-ba0f21741ec644b98c43846defd39492.
INFO:     127.0.0.1:35552 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:48:11 logger.py:39] Received request chatcmpl-0ef3db32fb37415189577c9f64eb8ef8: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ ğŸ¦‰ğŸ„ğŸ¦‰ğŸ ğŸ„ğŸ¦‰\nğŸ„ğŸ‹ğŸ ğŸ‹â›…ï¸ğŸ¦‰ğŸ‹\nğŸ¦‰ğŸ‹ğŸ„ğŸ„â›…ï¸ğŸ ğŸ \nğŸ¦‰ğŸ‹ğŸ‹ğŸ‹ğŸ¦‰ğŸ„ğŸ‹\nğŸ„ğŸ„ğŸ â›…ï¸ğŸ„ğŸ‹ğŸ \nğŸ„ğŸ„ğŸ â›…ï¸ğŸ¦‰ğŸ„ğŸ‹\nğŸ¦‰â›…ï¸ğŸ„ğŸ‹ğŸ„ğŸ‹â›…ï¸\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14800, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:48:11 async_llm_engine.py:211] Added request chatcmpl-0ef3db32fb37415189577c9f64eb8ef8.
INFO 03-02 02:48:14 metrics.py:455] Avg prompt throughput: 78.7 tokens/s, Avg generation throughput: 286.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 03-02 02:48:15 async_llm_engine.py:179] Finished request chatcmpl-798eacec392143329e01a7ad1d18b96b.
INFO:     127.0.0.1:60788 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:48:15 logger.py:39] Received request chatcmpl-e730230311514418b8645b8b35e2e65b: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nâ˜„ï¸â˜„ï¸ğŸ¦’ğŸ˜†ğŸ¯ğŸ¯ğŸ˜†\nâ˜„ï¸ğŸ¦ğŸ¯ğŸ¦’â˜„ï¸ğŸ¦ğŸ¦\nğŸ¦â˜„ï¸â˜„ï¸ğŸ¦ğŸ¦’â˜„ï¸ğŸ˜†\nğŸ¦ğŸ¯ğŸ¦ğŸ¦’ğŸ˜†ğŸ¯ğŸ¦’\nğŸ¯ğŸ˜†ğŸ¦’ğŸ˜†ğŸ¦’ğŸ˜†ğŸ˜†\nğŸ¦’ğŸ¦â˜„ï¸ğŸ¯â˜„ï¸ğŸ¦’â˜„ï¸\nğŸ¯ğŸ¯ğŸ¯ğŸ¦’ğŸ˜†ğŸ¦’ğŸ¯\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14788, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:48:15 async_llm_engine.py:211] Added request chatcmpl-e730230311514418b8645b8b35e2e65b.
INFO 03-02 02:48:19 metrics.py:455] Avg prompt throughput: 42.3 tokens/s, Avg generation throughput: 340.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 03-02 02:48:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 03-02 02:48:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 340.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 03-02 02:48:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 335.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 03-02 02:48:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 03-02 02:48:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 334.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 03-02 02:48:49 async_llm_engine.py:179] Finished request chatcmpl-e1addcb244664a378a62f3f338cb3f07.
INFO:     127.0.0.1:42200 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:48:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 328.9 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 03-02 02:48:49 logger.py:39] Received request chatcmpl-6afe37f4a1a249f99ad67acf0c7d40fc: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ’«ğŸ’«ğŸ¦©ğŸ’«ğŸœğŸ¦©ğŸ¦•\nğŸ¦•ğŸ¦©ğŸ¦©ğŸ›ğŸ’«ğŸ¦•ğŸ¦•\nğŸœğŸ¦•ğŸ¦©ğŸœğŸ›ğŸ’«ğŸœ\nğŸœğŸ¦©ğŸ›ğŸœğŸ¦©ğŸ’«ğŸœ\nğŸœğŸ¦©ğŸœğŸ¦©ğŸ’«ğŸ›ğŸ’«\nğŸ¦•ğŸœğŸœğŸ›ğŸœğŸ’«ğŸ¦©\nğŸ›ğŸ¦•ğŸ’«ğŸ’«ğŸœğŸ¦•ğŸ’«\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14796, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:48:49 async_llm_engine.py:211] Added request chatcmpl-6afe37f4a1a249f99ad67acf0c7d40fc.
INFO 03-02 02:48:54 metrics.py:455] Avg prompt throughput: 40.7 tokens/s, Avg generation throughput: 342.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 03-02 02:48:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 340.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 03-02 02:49:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 338.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 03-02 02:49:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 334.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 03-02 02:49:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 324.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 03-02 02:49:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 323.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-02 02:49:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 319.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-02 02:49:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 313.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-02 02:49:34 async_llm_engine.py:179] Finished request chatcmpl-ccbc262ef5ea476794de9db5ca8315d3.
INFO:     127.0.0.1:57982 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:49:34 logger.py:39] Received request chatcmpl-d9e77f27a11a43e693c4d170f5f09303: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¦œğŸ¦–ğŸ¦œğŸ¦–ğŸŒ¹ğŸ¦’ğŸ¦œ\nğŸŒ¹ğŸ¦œğŸŒ¹ğŸ˜ğŸŒ¹ğŸŒ¹ğŸ¦–\nğŸ˜ğŸŒ¹ğŸ¦–ğŸŒ¹ğŸ¦’ğŸŒ¹ğŸ¦œ\nğŸ¦œğŸŒ¹ğŸŒ¹ğŸ¦–ğŸ¦’ğŸ¦–ğŸ˜\nğŸ˜ğŸŒ¹ğŸ¦’ğŸ¦œğŸ¦œğŸ¦’ğŸŒ¹\nğŸ¦œğŸŒ¹ğŸ˜ğŸ¦œğŸŒ¹ğŸ˜ğŸ¦œ\nğŸŒ¹ğŸ˜ğŸ¦’ğŸŒ¹ğŸ¦’ğŸ¦’ğŸ¦’\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14795, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:49:34 async_llm_engine.py:211] Added request chatcmpl-d9e77f27a11a43e693c4d170f5f09303.
INFO 03-02 02:49:35 metrics.py:455] Avg prompt throughput: 40.9 tokens/s, Avg generation throughput: 316.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-02 02:49:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 317.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-02 02:49:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 322.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-02 02:49:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 318.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-02 02:49:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 316.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-02 02:50:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 309.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-02 02:50:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 305.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-02 02:50:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 300.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-02 02:50:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 301.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-02 02:50:16 async_llm_engine.py:179] Finished request chatcmpl-0ef3db32fb37415189577c9f64eb8ef8.
INFO:     127.0.0.1:34056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:50:16 logger.py:39] Received request chatcmpl-06149af810a6497d874ab0ad36a3dca6: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¢ğŸğŸ’¥ğŸ¢ğŸŒ‘ğŸŒ‘ğŸ\nğŸ’¥ğŸğŸ’¥ğŸğŸ¢ğŸğŸ\nğŸğŸğŸŒ‘ğŸ’¥ğŸ’¥ğŸŒ¿ğŸ\nğŸŒ‘ğŸ¢ğŸ¢ğŸŒ¿ğŸŒ‘ğŸŒ‘ğŸ¢\nğŸŒ‘ğŸ¢ğŸ¢ğŸ’¥ğŸŒ‘ğŸŒ‘ğŸŒ¿\nğŸŒ¿ğŸ’¥ğŸ’¥ğŸğŸ¢ğŸŒ¿ğŸ\nğŸŒ‘ğŸ’¥ğŸ’¥ğŸŒ‘ğŸŒ‘ğŸğŸ¢\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14806, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:50:16 async_llm_engine.py:211] Added request chatcmpl-06149af810a6497d874ab0ad36a3dca6.
INFO 03-02 02:50:16 async_llm_engine.py:179] Finished request chatcmpl-e730230311514418b8645b8b35e2e65b.
INFO:     127.0.0.1:34058 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:50:16 logger.py:39] Received request chatcmpl-accdfb30c4bc411faf2255a598734fc4: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¥€ğŸ˜ğŸ»ğŸ˜ğŸ˜ğŸ˜ğŸ¦‚\nğŸ†ğŸ†ğŸ†ğŸ¦‚ğŸ¥€ğŸ»ğŸ¥€\nğŸ»ğŸ¦‚ğŸ¦‚ğŸ¥€ğŸ¦‚ğŸ˜ğŸ¥€\nğŸ¦‚ğŸ˜ğŸ†ğŸ»ğŸ˜ğŸ»ğŸ†\nğŸ»ğŸ†ğŸ¥€ğŸ¦‚ğŸ˜ğŸ¥€ğŸ»\nğŸ¥€ğŸ†ğŸ¦‚ğŸ†ğŸ˜ğŸ»ğŸ¥€\nğŸ†ğŸ¥€ğŸ¦‚ğŸ†ğŸ˜ğŸ»ğŸ»\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14797, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:50:16 async_llm_engine.py:211] Added request chatcmpl-accdfb30c4bc411faf2255a598734fc4.
INFO 03-02 02:50:19 async_llm_engine.py:179] Finished request chatcmpl-2c926f77d20d457db2688c703a2733e6.
INFO:     127.0.0.1:34046 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:50:19 logger.py:39] Received request chatcmpl-aa77b8239bca40e19c8b09b4c26a163c: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¦†ğŸ¦†ğŸ”¥ğŸ”¥ğŸ¦†ğŸ¦†ğŸ¦†\nğŸ”¥ğŸ¦†ğŸ¦†ğŸğŸ¦†ğŸ¦†ğŸ§\nğŸ”¥ğŸ¦†ğŸ§ğŸ§ğŸ”¥ğŸŒ‘ğŸ¦†\nğŸ¦†ğŸğŸğŸŒ‘ğŸğŸ¦†ğŸ”¥\nğŸ¦†ğŸ¦†ğŸ§ğŸŒ‘ğŸŒ‘ğŸ§ğŸ\nğŸğŸŒ‘ğŸ§ğŸğŸ”¥ğŸ¦†ğŸ¦†\nğŸŒ‘ğŸ§ğŸ§ğŸ”¥ğŸ”¥ğŸŒ‘ğŸ¦†\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14806, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:50:19 async_llm_engine.py:211] Added request chatcmpl-aa77b8239bca40e19c8b09b4c26a163c.
INFO 03-02 02:50:20 metrics.py:455] Avg prompt throughput: 118.1 tokens/s, Avg generation throughput: 307.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 03-02 02:50:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 325.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-02 02:50:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 321.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-02 02:50:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 310.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-02 02:50:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-02 02:50:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-02 02:50:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-02 02:50:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-02 02:51:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-02 02:51:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-02 02:51:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 197.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-02 02:51:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 198.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-02 02:51:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 193.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-02 02:51:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-02 02:51:28 async_llm_engine.py:179] Finished request chatcmpl-ccc19605e1bb4a61bfba6707a8af3de4.
INFO:     127.0.0.1:37984 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:51:28 logger.py:39] Received request chatcmpl-a14853be59fa4bb3acabd07135751363: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸğŸŒ’â˜€ï¸ğŸğŸ’¥ğŸ’¥ğŸŒ’\nğŸŒ’ğŸŒ’ğŸ’¥ğŸ¼ğŸğŸŒ’ğŸ’¥\nğŸâ˜€ï¸â˜€ï¸ğŸ’¥ğŸ¼ğŸ’¥ğŸ¼\nğŸ¼ğŸ’¥ğŸ’¥â˜€ï¸â˜€ï¸ğŸ¼â˜€ï¸\nğŸ’¥ğŸğŸâ˜€ï¸ğŸ¼ğŸ’¥ğŸ’¥\nâ˜€ï¸ğŸ¼ğŸğŸâ˜€ï¸ğŸ’¥ğŸ’¥\nâ˜€ï¸ğŸŒ’ğŸğŸ¼ğŸğŸ¼ğŸ\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14790, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:51:28 async_llm_engine.py:211] Added request chatcmpl-a14853be59fa4bb3acabd07135751363.
INFO 03-02 02:51:30 metrics.py:455] Avg prompt throughput: 41.8 tokens/s, Avg generation throughput: 198.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-02 02:51:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-02 02:51:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-02 02:51:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 194.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-02 02:51:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 197.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-02 02:51:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 196.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-02 02:52:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 183.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-02 02:52:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 183.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-02 02:52:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 185.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-02 02:52:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 187.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-02 02:52:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 185.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-02 02:52:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 188.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-02 02:52:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 185.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-02 02:52:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 184.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-02 02:52:36 async_llm_engine.py:179] Finished request chatcmpl-f0bd7677bdcf46b886f0bf640f878f32.
INFO:     127.0.0.1:46352 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:52:36 logger.py:39] Received request chatcmpl-af148dc660784864861729b82a4ae483: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¦â›…ï¸ğŸŒ˜ğŸ¦­ğŸ¦­ğŸ¦ğŸŒ˜\nğŸ¦­ğŸŒ˜ğŸ’§ğŸ¦ğŸŒ˜ğŸŒ˜ğŸ¦\nğŸ’§ğŸ’§ğŸ¦­â›…ï¸ğŸ’§ğŸŒ˜ğŸ¦­\nğŸŒ˜ğŸ¦ğŸ’§â›…ï¸ğŸ¦­ğŸ’§ğŸŒ˜\nâ›…ï¸ğŸ’§ğŸŒ˜ğŸ’§ğŸ¦­â›…ï¸ğŸ¦\nâ›…ï¸â›…ï¸â›…ï¸ğŸ’§ğŸŒ˜ğŸ¦ğŸ¦\nâ›…ï¸ğŸŒ˜ğŸŒ˜ğŸ¦­ğŸ’§â›…ï¸ğŸ¦\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14779, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:52:36 async_llm_engine.py:211] Added request chatcmpl-af148dc660784864861729b82a4ae483.
INFO 03-02 02:52:40 metrics.py:455] Avg prompt throughput: 44.0 tokens/s, Avg generation throughput: 200.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-02 02:52:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-02 02:52:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-02 02:52:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-02 02:52:56 async_llm_engine.py:179] Finished request chatcmpl-06149af810a6497d874ab0ad36a3dca6.
INFO:     127.0.0.1:51392 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:52:56 logger.py:39] Received request chatcmpl-572b45bfb04b4b24ae55e1e7943778d8: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ˜ğŸŒ»ğŸ˜ğŸ¦‡ğŸŒ»ğŸŒ»ğŸŒ»\nğŸ˜ğŸ˜ğŸŒ»ğŸŒ»ğŸ¦‡ğŸ¦¦ğŸ¦‡\nğŸ¦¦ğŸŒ»ğŸ¦‡ğŸ˜ğŸ˜ğŸ˜ğŸ¦¦\nğŸ§ğŸŒ»ğŸŒ»ğŸ§ğŸ¦¦ğŸ§ğŸ¦‡\nğŸŒ»ğŸŒ»ğŸŒ»ğŸ¦¦ğŸ¦‡ğŸ¦‡ğŸ˜\nğŸ˜ğŸŒ»ğŸ§ğŸŒ»ğŸŒ»ğŸŒ»ğŸ¦¦\nğŸ˜ğŸ˜ğŸ¦‡ğŸ¦¦ğŸŒ»ğŸŒ»ğŸ¦¦\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14798, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:52:56 async_llm_engine.py:211] Added request chatcmpl-572b45bfb04b4b24ae55e1e7943778d8.
INFO 03-02 02:53:00 metrics.py:455] Avg prompt throughput: 40.3 tokens/s, Avg generation throughput: 200.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-02 02:53:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-02 02:53:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 196.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-02 02:53:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 181.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-02 02:53:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 185.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-02 02:53:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 186.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-02 02:53:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 184.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-02 02:53:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 184.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-02 02:53:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 185.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-02 02:53:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 183.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-02 02:53:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 183.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-02 02:53:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 183.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-02 02:54:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 182.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-02 02:54:04 async_llm_engine.py:179] Finished request chatcmpl-6afe37f4a1a249f99ad67acf0c7d40fc.
INFO:     127.0.0.1:58938 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:54:04 logger.py:39] Received request chatcmpl-ab63784694b54359b3c366d80e1e86f8: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ«ğŸ¼ğŸ¼ğŸ¼ğŸºğŸ¦€ğŸº\nğŸ¼ğŸ¦€ğŸ«ğŸºğŸ¦€ğŸ¼ğŸ¦€\nğŸ¦–ğŸ¦€ğŸ¼ğŸ«ğŸ¦–ğŸºğŸ¦–\nğŸ«ğŸ¼ğŸºğŸ¼ğŸ¼ğŸ¦€ğŸ¼\nğŸ¦–ğŸ«ğŸ¼ğŸ«ğŸ¼ğŸ¼ğŸ¦€\nğŸºğŸ¦–ğŸ¦–ğŸ«ğŸºğŸ«ğŸ¦–\nğŸ¦€ğŸºğŸ¦€ğŸ¦€ğŸ¦€ğŸ¼ğŸ¼\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14806, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:54:04 async_llm_engine.py:211] Added request chatcmpl-ab63784694b54359b3c366d80e1e86f8.
INFO 03-02 02:54:05 metrics.py:455] Avg prompt throughput: 38.7 tokens/s, Avg generation throughput: 184.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-02 02:54:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-02 02:54:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 200.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-02 02:54:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-02 02:54:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-02 02:54:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 187.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-02 02:54:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 183.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-02 02:54:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 183.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-02 02:54:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 182.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-02 02:54:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 182.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-02 02:54:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 182.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-02 02:55:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-02 02:55:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 178.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-02 02:55:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 177.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-02 02:55:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 181.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-02 02:55:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 179.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-02 02:55:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 179.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-02 02:55:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 177.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-02 02:55:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 176.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.
INFO 03-02 02:55:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 171.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.
INFO 03-02 02:55:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 173.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%.
INFO 03-02 02:55:50 async_llm_engine.py:179] Finished request chatcmpl-accdfb30c4bc411faf2255a598734fc4.
INFO:     127.0.0.1:56688 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:55:50 logger.py:39] Received request chatcmpl-a09e1a0a11c140219572a9042d1c4f93: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¦ğŸ†ğŸ†ğŸ›ğŸ†ğŸ†ğŸŒŠ\nğŸ†ğŸ¦ğŸ¦ğŸŒŠğŸŒ™ğŸŒŠğŸ¦\nğŸŒŠğŸ†ğŸ¦ğŸŒ™ğŸ†ğŸ¦ğŸŒŠ\nğŸŒŠğŸ†ğŸ›ğŸŒ™ğŸŒ™ğŸŒŠğŸŒŠ\nğŸ›ğŸŒ™ğŸŒŠğŸŒ™ğŸŒŠğŸ›ğŸ†\nğŸŒŠğŸŒŠğŸŒ™ğŸŒŠğŸ†ğŸŒŠğŸ†\nğŸ¦ğŸ¦ğŸ›ğŸŒ™ğŸŒ™ğŸŒŠğŸ†\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14798, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:55:50 async_llm_engine.py:211] Added request chatcmpl-a09e1a0a11c140219572a9042d1c4f93.
INFO 03-02 02:55:50 metrics.py:455] Avg prompt throughput: 40.3 tokens/s, Avg generation throughput: 175.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-02 02:55:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-02 02:56:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 177.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-02 02:56:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 177.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-02 02:56:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 177.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-02 02:56:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 176.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-02 02:56:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 174.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.
INFO 03-02 02:56:24 async_llm_engine.py:179] Finished request chatcmpl-8767339f430842f792a8b085cd6be22a.
INFO:     127.0.0.1:54474 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:56:24 logger.py:39] Received request chatcmpl-d45c6ad4f62947ab848fc2d39a1e5cab: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸŒ•ğŸ¦­ğŸ’«ğŸŒ•ğŸ¦­ğŸ’«ğŸ’«\nğŸŒ•ğŸ¦ğŸ¦ğŸ¦®ğŸŒ•ğŸ’«ğŸ’«\nğŸ¦ğŸŒ•ğŸ¦®ğŸ¦ğŸŒ•ğŸŒ•ğŸ’«\nğŸ’«ğŸ¦ğŸŒ•ğŸ¦­ğŸ¦®ğŸ¦ğŸ’«\nğŸ’«ğŸ¦­ğŸ’«ğŸ¦®ğŸ¦®ğŸ¦­ğŸ¦\nğŸ¦®ğŸ¦ğŸ¦®ğŸ¦ğŸ¦­ğŸ¦ğŸ¦­\nğŸ¦®ğŸ¦®ğŸŒ•ğŸ¦ğŸŒ•ğŸ¦­ğŸ’«\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14789, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:56:24 async_llm_engine.py:211] Added request chatcmpl-d45c6ad4f62947ab848fc2d39a1e5cab.
INFO 03-02 02:56:25 metrics.py:455] Avg prompt throughput: 42.0 tokens/s, Avg generation throughput: 178.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-02 02:56:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 185.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-02 02:56:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 182.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-02 02:56:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 184.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-02 02:56:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 184.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-02 02:56:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 179.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-02 02:56:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 181.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-02 02:56:59 async_llm_engine.py:179] Finished request chatcmpl-572b45bfb04b4b24ae55e1e7943778d8.
INFO:     127.0.0.1:53704 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:56:59 logger.py:39] Received request chatcmpl-7bc710c751f043fe87f5d8f4fda6296e: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¦†ğŸ¦™ğŸ¦†ğŸ´ğŸŒ’ğŸ´ğŸ¦†\nğŸ¦†ğŸ´ğŸŒ’ğŸ¦†ğŸŒ’ğŸ´ğŸŒ’\nğŸŒ’ğŸ¦™ğŸ¦†ğŸŒ’ğŸ¦†ğŸ¦™ğŸŒ’\nğŸŒ’ğŸğŸ¦†ğŸŒ’ğŸğŸ¦†ğŸ\nğŸ´ğŸŒ’ğŸ¦™ğŸ¦†ğŸŒ’ğŸ¦†ğŸ\nğŸ´ğŸŒ’ğŸğŸ´ğŸ¦™ğŸ¦™ğŸ¦†\nğŸ¦™ğŸ´ğŸ¦™ğŸŒ’ğŸ¦†ğŸ´ğŸ¦™\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14788, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:56:59 async_llm_engine.py:211] Added request chatcmpl-7bc710c751f043fe87f5d8f4fda6296e.
INFO 03-02 02:57:00 metrics.py:455] Avg prompt throughput: 42.2 tokens/s, Avg generation throughput: 181.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-02 02:57:01 async_llm_engine.py:179] Finished request chatcmpl-aa77b8239bca40e19c8b09b4c26a163c.
INFO:     127.0.0.1:56692 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:57:01 logger.py:39] Received request chatcmpl-4c25dd688fa349fdbfb33a67f2cba09c: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ–ğŸ¨ğŸ­ğŸ­ğŸ¥°ğŸ¦ğŸ¥°\nğŸ¨ğŸ¥°ğŸ¨ğŸ–ğŸ¨ğŸ¦ğŸ¦\nğŸ¦ğŸ¥°ğŸ¦ğŸ­ğŸ¦ğŸ­ğŸ–\nğŸ¨ğŸ¦ğŸ­ğŸ¨ğŸ¥°ğŸ­ğŸ¦\nğŸ¨ğŸ¥°ğŸ¨ğŸ­ğŸ–ğŸ¨ğŸ¦\nğŸ¦ğŸ­ğŸ­ğŸ¦ğŸ¥°ğŸ­ğŸ¦\nğŸ¥°ğŸ¨ğŸ–ğŸ­ğŸ¨ğŸ¦ğŸ¨\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14806, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:57:01 async_llm_engine.py:211] Added request chatcmpl-4c25dd688fa349fdbfb33a67f2cba09c.
INFO 03-02 02:57:05 metrics.py:455] Avg prompt throughput: 38.7 tokens/s, Avg generation throughput: 203.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-02 02:57:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-02 02:57:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 186.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-02 02:57:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 186.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-02 02:57:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-02 02:57:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 188.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-02 02:57:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 187.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-02 02:57:37 async_llm_engine.py:179] Finished request chatcmpl-af148dc660784864861729b82a4ae483.
INFO:     127.0.0.1:55722 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:57:37 logger.py:39] Received request chatcmpl-86cbbf7306134bebbd7054d84970f523: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nâ„ï¸ğŸªğŸ¤šâ˜€ï¸ğŸªğŸ¤šâ„ï¸\nğŸªğŸªğŸªğŸ®â˜€ï¸â˜€ï¸ğŸª\nâ˜€ï¸ğŸ¤šâ„ï¸â„ï¸ğŸªğŸ¤šğŸ¤š\nâ„ï¸ğŸ¤šâ˜€ï¸ğŸ¤šğŸ¤šğŸ®ğŸ¤š\nğŸªâ˜€ï¸â˜€ï¸â˜€ï¸ğŸ¤šâ„ï¸â˜€ï¸\nğŸ¤šğŸ®â˜€ï¸â„ï¸â„ï¸ğŸ¤šğŸ®\nâ˜€ï¸â˜€ï¸ğŸ®â„ï¸ğŸ®ğŸ¤šâ„ï¸\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14768, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:57:37 async_llm_engine.py:211] Added request chatcmpl-86cbbf7306134bebbd7054d84970f523.
INFO 03-02 02:57:41 metrics.py:455] Avg prompt throughput: 46.4 tokens/s, Avg generation throughput: 188.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-02 02:57:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 191.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-02 02:57:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 191.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-02 02:57:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 187.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-02 02:58:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 187.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-02 02:58:01 async_llm_engine.py:179] Finished request chatcmpl-a14853be59fa4bb3acabd07135751363.
INFO:     127.0.0.1:58526 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:58:01 logger.py:39] Received request chatcmpl-735be6724f9c47cca206e2ee2846e96a: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ°ğŸµğŸŒ³ğŸ“ğŸŒ±ğŸ“ğŸµ\nğŸµğŸµğŸµğŸ“ğŸµğŸ°ğŸ“\nğŸ°ğŸŒ±ğŸ“ğŸŒ³ğŸ“ğŸ°ğŸŒ±\nğŸ°ğŸ“ğŸŒ±ğŸŒ³ğŸ°ğŸµğŸ“\nğŸ“ğŸ“ğŸŒ³ğŸŒ±ğŸ°ğŸŒ³ğŸµ\nğŸŒ³ğŸŒ±ğŸ°ğŸŒ³ğŸŒ³ğŸ°ğŸµ\nğŸŒ±ğŸµğŸ°ğŸ°ğŸ°ğŸµğŸµ\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14806, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:58:01 async_llm_engine.py:211] Added request chatcmpl-735be6724f9c47cca206e2ee2846e96a.
INFO 03-02 02:58:06 metrics.py:455] Avg prompt throughput: 38.8 tokens/s, Avg generation throughput: 207.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-02 02:58:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-02 02:58:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-02 02:58:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-02 02:58:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-02 02:58:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-02 02:58:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-02 02:58:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-02 02:58:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-02 02:58:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-02 02:58:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-02 02:59:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 200.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-02 02:59:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-02 02:59:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 200.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-02 02:59:13 async_llm_engine.py:179] Finished request chatcmpl-d9e77f27a11a43e693c4d170f5f09303.
INFO:     127.0.0.1:44374 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:59:13 logger.py:39] Received request chatcmpl-cffee3cd772c4be48f903c8c75095320: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ‹ğŸ„ğŸ•·ğŸ¦ğŸ¦©ğŸ¦©ğŸ‹\nğŸ¦ğŸ¦©ğŸ¦ğŸ¦©ğŸ¦ğŸ¦ğŸ„\nğŸ¦©ğŸ„ğŸ¦ğŸ„ğŸ•·ğŸ•·ğŸ¦©\nğŸ¦©ğŸ¦ğŸ„ğŸ„ğŸ¦©ğŸ•·ğŸ¦©\nğŸ•·ğŸ¦ğŸ¦ğŸ‹ğŸ•·ğŸ•·ğŸ„\nğŸ•·ğŸ„ğŸ¦©ğŸ¦©ğŸ„ğŸ¦©ğŸ„\nğŸ‹ğŸ•·ğŸ„ğŸ¦ğŸ‹ğŸ¦©ğŸ¦©\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14792, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:59:13 async_llm_engine.py:211] Added request chatcmpl-cffee3cd772c4be48f903c8c75095320.
INFO 03-02 02:59:16 metrics.py:455] Avg prompt throughput: 41.5 tokens/s, Avg generation throughput: 263.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-02 02:59:20 async_llm_engine.py:179] Finished request chatcmpl-4c25dd688fa349fdbfb33a67f2cba09c.
INFO:     127.0.0.1:42524 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:59:20 logger.py:39] Received request chatcmpl-8be07430b3594f0b9bc0b30cea386518: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸŒğŸŒŸğŸŒŸğŸŒğŸŒğŸ§½ğŸ\nğŸ ğŸŒŸğŸğŸŒŸğŸ ğŸ§½ğŸ\nğŸŒŸğŸ ğŸŒğŸğŸŒğŸŒğŸŒŸ\nğŸ ğŸ ğŸğŸ§½ğŸğŸŒğŸŒ\nğŸŒŸğŸŒŸğŸŒğŸŒŸğŸ ğŸŒğŸŒ\nğŸ§½ğŸŒŸğŸ ğŸŒŸğŸ§½ğŸ§½ğŸ \nğŸğŸŒŸğŸ ğŸŒŸğŸ ğŸ ğŸ\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14800, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:59:20 async_llm_engine.py:211] Added request chatcmpl-8be07430b3594f0b9bc0b30cea386518.
INFO 03-02 02:59:21 metrics.py:455] Avg prompt throughput: 40.0 tokens/s, Avg generation throughput: 314.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-02 02:59:22 async_llm_engine.py:179] Finished request chatcmpl-ab63784694b54359b3c366d80e1e86f8.
INFO 03-02 02:59:22 async_llm_engine.py:179] Finished request chatcmpl-a09e1a0a11c140219572a9042d1c4f93.
INFO:     127.0.0.1:60316 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:33324 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:59:22 logger.py:39] Received request chatcmpl-bbb0efac0c6341bb8605cffc638ccacf: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¦©ğŸ¦ğŸ¦©ğŸ¦ğŸ›ğŸ¦©ğŸ¦¢\nğŸğŸ¦¢ğŸ›ğŸğŸğŸ¦ğŸ\nğŸ¦ğŸ¦©ğŸ¦©ğŸ¦¢ğŸ¦¢ğŸ¦©ğŸ¦¢\nğŸ›ğŸ¦ğŸğŸ¦¢ğŸ¦ğŸ›ğŸ\nğŸ¦©ğŸ¦¢ğŸ¦¢ğŸ¦¢ğŸğŸğŸ¦¢\nğŸ¦ğŸ¦ğŸğŸ¦©ğŸ¦¢ğŸ¦©ğŸ¦©\nğŸğŸ›ğŸ¦¢ğŸ¦ğŸ›ğŸğŸ¦¢\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14763, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:59:22 async_llm_engine.py:211] Added request chatcmpl-bbb0efac0c6341bb8605cffc638ccacf.
INFO 03-02 02:59:22 logger.py:39] Received request chatcmpl-fa5771c1c49f476c9b01c4839d128020: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ„ğŸ§ğŸ¦‘ğŸ§ğŸ¦‘ğŸ ğŸ§\nğŸ§ğŸ§ğŸ„ğŸ„ğŸ¦‘ğŸ¦‘ğŸ§\nğŸ¦‘ğŸ„ğŸ¦‘ğŸ ğŸ„ğŸ ğŸ¦‘\nğŸ†ğŸ§ğŸ„ğŸ ğŸ†ğŸ†ğŸ„\nğŸ„ğŸ†ğŸ§ğŸ§ğŸ ğŸ§ğŸ†\nğŸ†ğŸ¦‘ğŸ†ğŸ§ğŸ ğŸ¦‘ğŸ§\nğŸ ğŸ§ğŸ¦‘ğŸ†ğŸ¦‘ğŸ†ğŸ¦‘\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14806, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:59:22 async_llm_engine.py:211] Added request chatcmpl-fa5771c1c49f476c9b01c4839d128020.
INFO 03-02 02:59:26 metrics.py:455] Avg prompt throughput: 86.0 tokens/s, Avg generation throughput: 339.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 03-02 02:59:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 344.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 03-02 02:59:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 338.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 03-02 02:59:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 333.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 03-02 02:59:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 334.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 03-02 02:59:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 326.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 03-02 02:59:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 322.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-02 02:59:57 async_llm_engine.py:179] Finished request chatcmpl-d45c6ad4f62947ab848fc2d39a1e5cab.
INFO:     127.0.0.1:37466 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 02:59:57 logger.py:39] Received request chatcmpl-d873bdca34bd4727aa26dadd3cd4d8f9: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¤šğŸŒ¾ğŸ¦ƒğŸŒ¾ğŸ¦ƒğŸ¤šğŸ¦ƒ\nğŸŒ¾ğŸ¦ƒğŸ¦‡ğŸ¦ƒğŸ¦ƒğŸŒ¾ğŸŒ¾\nğŸ˜ğŸ¤šğŸ¤šğŸ¤šğŸŒ¾ğŸ˜ğŸ¤š\nğŸ˜ğŸ¦ƒğŸŒ¾ğŸ˜ğŸŒ¾ğŸ¦ƒğŸ¦ƒ\nğŸ¦‡ğŸ¦ƒğŸ˜ğŸŒ¾ğŸ¤šğŸ¤šğŸ¦ƒ\nğŸ¤šğŸ˜ğŸ¤šğŸ˜ğŸ¦‡ğŸ¦‡ğŸ¤š\nğŸ¦ƒğŸ¦ƒğŸ¤šğŸ¦ƒğŸ¤šğŸ¤šğŸ¦ƒ\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14806, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 02:59:57 async_llm_engine.py:211] Added request chatcmpl-d873bdca34bd4727aa26dadd3cd4d8f9.
INFO 03-02 03:00:00 async_llm_engine.py:179] Finished request chatcmpl-86cbbf7306134bebbd7054d84970f523.
INFO:     127.0.0.1:37936 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 03:00:00 logger.py:39] Received request chatcmpl-c0703b754db742b3b16d9d72f430df50: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¦ğŸŒğŸŒ˜ğŸŒµğŸŒğŸŒµğŸ\nğŸŒµğŸŒµğŸŒğŸğŸŒ˜ğŸ¦ğŸ\nğŸ¦ğŸŒ˜ğŸŒ˜ğŸŒµğŸŒ˜ğŸŒ˜ğŸ\nğŸŒğŸŒµğŸğŸŒµğŸğŸŒµğŸŒ˜\nğŸŒ˜ğŸŒµğŸŒ˜ğŸŒµğŸŒ˜ğŸ¦ğŸŒµ\nğŸ¦ğŸ¦ğŸŒğŸŒğŸŒğŸŒ˜ğŸŒ\nğŸŒµğŸŒµğŸ¦ğŸŒµğŸŒ˜ğŸŒ˜ğŸ¦\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14806, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 03:00:00 async_llm_engine.py:211] Added request chatcmpl-c0703b754db742b3b16d9d72f430df50.
INFO 03-02 03:00:01 metrics.py:455] Avg prompt throughput: 77.4 tokens/s, Avg generation throughput: 329.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 03-02 03:00:03 async_llm_engine.py:179] Finished request chatcmpl-7bc710c751f043fe87f5d8f4fda6296e.
INFO:     127.0.0.1:42512 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 03:00:03 logger.py:39] Received request chatcmpl-bb37553beb9e4d6fa1c05ef6157d64b6: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸŒ¼ğŸŒ¼ğŸ…ğŸ…ğŸ³ğŸ…ğŸ…\nğŸŒ¼ğŸ…ğŸŒ¼ğŸ…ğŸ¨ğŸ…ğŸŒ²\nğŸŒ¼ğŸŒ²ğŸŒ²ğŸ³ğŸ…ğŸ¨ğŸ³\nğŸŒ²ğŸ…ğŸ…ğŸŒ²ğŸŒ¼ğŸŒ¼ğŸŒ¼\nğŸŒ¼ğŸ…ğŸ³ğŸ…ğŸ…ğŸŒ¼ğŸŒ¼\nğŸ³ğŸ¨ğŸ¨ğŸ¨ğŸŒ¼ğŸ¨ğŸ…\nğŸ³ğŸŒ¼ğŸŒ²ğŸŒ¼ğŸŒ²ğŸ…ğŸŒ²\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14806, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 03:00:03 async_llm_engine.py:211] Added request chatcmpl-bb37553beb9e4d6fa1c05ef6157d64b6.
INFO 03-02 03:00:06 metrics.py:455] Avg prompt throughput: 38.8 tokens/s, Avg generation throughput: 347.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 03-02 03:00:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 346.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 03-02 03:00:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 345.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 03-02 03:00:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 343.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 03-02 03:00:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 338.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:00:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 334.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:00:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 328.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:00:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 326.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:00:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 324.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-02 03:00:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 316.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:00:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 319.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-02 03:01:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 314.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-02 03:01:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 312.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-02 03:01:07 async_llm_engine.py:179] Finished request chatcmpl-735be6724f9c47cca206e2ee2846e96a.
INFO:     127.0.0.1:45280 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 03:01:07 logger.py:39] Received request chatcmpl-67b068ffca774da38a62701c88ea199a: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¦ğŸ¿ğŸ¦ğŸ¦ğŸ¼ğŸ¼ğŸ¼\nğŸš€ğŸ¼ğŸ¼ğŸ¿ğŸ¦ğŸš€ğŸ¦\nğŸš€ğŸ¼âœ¨âœ¨ğŸš€ğŸ¿âœ¨\nâœ¨ğŸ¼âœ¨ğŸ¦ğŸ¦ğŸ¼âœ¨\nğŸ¿ğŸš€ğŸ¦âœ¨ğŸš€ğŸ¿ğŸ¼\nğŸ¦ğŸš€ğŸ¼ğŸ¦ğŸš€âœ¨âœ¨\nğŸ¿âœ¨ğŸš€ğŸš€ğŸ¿ğŸ¼ğŸ¦\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14806, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 03:01:07 async_llm_engine.py:211] Added request chatcmpl-67b068ffca774da38a62701c88ea199a.
INFO 03-02 03:01:11 metrics.py:455] Avg prompt throughput: 38.7 tokens/s, Avg generation throughput: 313.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:01:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 320.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-02 03:01:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 315.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-02 03:01:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 313.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-02 03:01:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 308.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-02 03:01:31 async_llm_engine.py:179] Finished request chatcmpl-fa5771c1c49f476c9b01c4839d128020.
INFO:     127.0.0.1:53398 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 03:01:31 logger.py:39] Received request chatcmpl-f3e4c85c1ef24dc6b78bd6ca1d84fac8: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ‚ğŸŒ¿ğŸŒ¿ğŸ‚ğŸŒ“ğŸŒ“ğŸ‚\nğŸ©ğŸŒ“ğŸ©ğŸ©ğŸ©ğŸŒ¿ğŸŒ“\nğŸ‚ğŸ©ğŸŒ¿ğŸ‚ğŸŒ“ğŸ°ğŸ°\nğŸ‚ğŸ‚ğŸŒ¿ğŸŒ“ğŸ°ğŸ©ğŸ‚\nğŸŒ¿ğŸŒ¿ğŸ‚ğŸŒ¿ğŸ‚ğŸ‚ğŸ°\nğŸ°ğŸ©ğŸ°ğŸŒ¿ğŸ©ğŸŒ¿ğŸ‚\nğŸŒ“ğŸ‚ğŸ‚ğŸŒ¿ğŸ©ğŸ°ğŸŒ¿\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14806, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 03:01:31 async_llm_engine.py:211] Added request chatcmpl-f3e4c85c1ef24dc6b78bd6ca1d84fac8.
INFO 03-02 03:01:34 async_llm_engine.py:179] Finished request chatcmpl-d873bdca34bd4727aa26dadd3cd4d8f9.
INFO:     127.0.0.1:48370 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 03:01:34 logger.py:39] Received request chatcmpl-2e9e6b88a7634f91a7c6d3e9e52998ff: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¦†ğŸŒ–ğŸƒğŸŒ”ğŸƒğŸƒğŸŒ’\nğŸŒ”ğŸƒğŸƒğŸŒ–ğŸŒ–ğŸŒ–ğŸƒ\nğŸŒ”ğŸƒğŸŒ’ğŸŒ’ğŸŒ”ğŸŒ’ğŸƒ\nğŸŒ”ğŸƒğŸƒğŸŒ’ğŸŒ–ğŸ¦†ğŸ¦†\nğŸ¦†ğŸŒ’ğŸƒğŸ¦†ğŸ¦†ğŸŒ’ğŸŒ–\nğŸ¦†ğŸƒğŸ¦†ğŸŒ”ğŸƒğŸƒğŸŒ”\nğŸŒ–ğŸ¦†ğŸŒ’ğŸ¦†ğŸŒ’ğŸ¦†ğŸŒ–\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14789, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 03:01:34 async_llm_engine.py:211] Added request chatcmpl-2e9e6b88a7634f91a7c6d3e9e52998ff.
INFO 03-02 03:01:36 metrics.py:455] Avg prompt throughput: 81.0 tokens/s, Avg generation throughput: 306.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-02 03:01:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 322.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:01:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 319.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-02 03:01:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 317.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-02 03:01:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 311.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-02 03:01:57 async_llm_engine.py:179] Finished request chatcmpl-8be07430b3594f0b9bc0b30cea386518.
INFO:     127.0.0.1:53390 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 03:01:57 logger.py:39] Received request chatcmpl-9170a5118d3448c6ba389328d32d2a56: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ„ğŸ„ğŸ¦ŠğŸ¦ŠğŸ„ğŸğŸ§\nğŸ¦ŠğŸ„ğŸ„ğŸğŸ„ğŸ„ğŸ„\nğŸ„ğŸ§ğŸ¦ŠğŸğŸğŸ¦ŠğŸ§\nğŸ§ğŸ„ğŸ„ğŸ„ğŸ§ğŸ„ğŸ„\nğŸ¦ŠğŸ§ğŸ„ğŸ„ğŸğŸ¦ŠğŸ„\nğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸğŸ§\nğŸğŸ„ğŸ„ğŸ„ğŸ§ğŸ„ğŸ§\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14806, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 03:01:57 async_llm_engine.py:211] Added request chatcmpl-9170a5118d3448c6ba389328d32d2a56.
INFO 03-02 03:02:01 metrics.py:455] Avg prompt throughput: 38.8 tokens/s, Avg generation throughput: 314.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:02:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 318.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-02 03:02:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 312.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-02 03:02:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 315.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-02 03:02:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 311.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-02 03:02:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 308.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-02 03:02:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 288.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-02 03:02:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-02 03:02:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-02 03:02:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:02:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:02:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:03:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:03:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 198.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:03:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:03:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 199.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-02 03:03:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 196.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-02 03:03:23 async_llm_engine.py:179] Finished request chatcmpl-bb37553beb9e4d6fa1c05ef6157d64b6.
INFO:     127.0.0.1:48388 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 03:03:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 177.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:03:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 174.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:03:35 async_llm_engine.py:179] Finished request chatcmpl-c0703b754db742b3b16d9d72f430df50.
INFO:     127.0.0.1:48386 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 03:03:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 171.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-02 03:03:40 async_llm_engine.py:179] Finished request chatcmpl-67b068ffca774da38a62701c88ea199a.
INFO:     127.0.0.1:57598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 03:03:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 151.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-02 03:03:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 132.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-02 03:03:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 132.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-02 03:03:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 132.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-02 03:04:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 130.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-02 03:04:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 130.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-02 03:04:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 129.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-02 03:04:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 130.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-02 03:04:19 async_llm_engine.py:179] Finished request chatcmpl-bbb0efac0c6341bb8605cffc638ccacf.
INFO:     127.0.0.1:53396 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 03:04:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 122.4 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:04:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.6 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-02 03:04:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.1 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-02 03:04:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.6 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-02 03:04:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.4 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-02 03:04:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.4 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:04:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.9 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:04:52 async_llm_engine.py:179] Finished request chatcmpl-cffee3cd772c4be48f903c8c75095320.
INFO:     127.0.0.1:57788 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 03:04:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 129.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 03-02 03:05:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 131.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 03-02 03:05:04 async_llm_engine.py:179] Finished request chatcmpl-9170a5118d3448c6ba389328d32d2a56.
INFO:     127.0.0.1:34200 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 03:05:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 03-02 03:05:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 03-02 03:05:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 03-02 03:05:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 03-02 03:05:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 03-02 03:05:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 03-02 03:05:32 async_llm_engine.py:179] Finished request chatcmpl-f3e4c85c1ef24dc6b78bd6ca1d84fac8.
INFO:     127.0.0.1:53266 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-02 03:05:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:05:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:05:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:05:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:05:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:06:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:06:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:06:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:06:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:06:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 3.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:06:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:06:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:06:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:06:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:06:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:07:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:07:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:07:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:07:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:07:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 3.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:07:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:07:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:07:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:07:48 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:07:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:08:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:08:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:08:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:08:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:08:23 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:08:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:08:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:08:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:08:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:08:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:08:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:09:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:09:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:09:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:09:23 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:09:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:09:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:09:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:09:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:09:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:10:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:10:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:10:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:10:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:10:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:10:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:10:33 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:10:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:10:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:10:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:10:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:11:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:11:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:11:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:11:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:11:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:11:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-02 03:11:34 async_llm_engine.py:223] Aborted request chatcmpl-2e9e6b88a7634f91a7c6d3e9e52998ff.
INFO 03-02 03:11:35 logger.py:39] Received request chatcmpl-0f339681c4724d6dae30ed4eb26bb8e6: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¦†ğŸŒ–ğŸƒğŸŒ”ğŸƒğŸƒğŸŒ’\nğŸŒ”ğŸƒğŸƒğŸŒ–ğŸŒ–ğŸŒ–ğŸƒ\nğŸŒ”ğŸƒğŸŒ’ğŸŒ’ğŸŒ”ğŸŒ’ğŸƒ\nğŸŒ”ğŸƒğŸƒğŸŒ’ğŸŒ–ğŸ¦†ğŸ¦†\nğŸ¦†ğŸŒ’ğŸƒğŸ¦†ğŸ¦†ğŸŒ’ğŸŒ–\nğŸ¦†ğŸƒğŸ¦†ğŸŒ”ğŸƒğŸƒğŸŒ”\nğŸŒ–ğŸ¦†ğŸŒ’ğŸ¦†ğŸŒ’ğŸ¦†ğŸŒ–\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14789, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 03:11:35 async_llm_engine.py:211] Added request chatcmpl-0f339681c4724d6dae30ed4eb26bb8e6.
INFO 03-02 03:11:38 metrics.py:455] Avg prompt throughput: 31.8 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:11:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:11:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:11:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:11:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:12:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:12:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:12:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:12:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:12:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:12:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:12:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:12:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:12:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:12:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:12:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:13:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:13:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:13:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:13:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:13:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:13:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:13:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:13:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:13:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:13:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:13:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:13:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:14:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:14:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:14:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:14:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:14:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:14:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:14:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:14:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:14:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:14:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:14:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:14:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:15:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:15:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:15:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:15:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:15:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:15:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:15:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:15:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:15:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:15:48 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:15:53 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:15:58 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:16:03 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:16:08 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:16:13 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:16:18 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:16:23 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:16:28 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:16:33 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:16:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:16:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:16:48 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:16:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:16:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:17:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:17:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:17:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:17:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:17:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:17:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:17:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:17:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:17:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:17:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:17:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:17:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:18:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:18:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:18:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 11.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:18:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:18:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:18:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:18:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:18:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:18:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:18:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:18:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:19:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:19:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:19:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:19:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:19:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 4.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:19:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:19:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:19:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:19:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:19:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:19:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:19:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:20:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:20:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:20:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:20:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:20:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:20:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:20:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:20:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:20:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:20:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:20:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:20:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:21:03 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:21:08 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:21:13 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:21:18 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:21:23 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:21:28 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:21:33 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:21:35 async_llm_engine.py:223] Aborted request chatcmpl-0f339681c4724d6dae30ed4eb26bb8e6.
INFO 03-02 03:21:36 logger.py:39] Received request chatcmpl-d80763917cae4ea7a856ab590a0c4446: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¦†ğŸŒ–ğŸƒğŸŒ”ğŸƒğŸƒğŸŒ’\nğŸŒ”ğŸƒğŸƒğŸŒ–ğŸŒ–ğŸŒ–ğŸƒ\nğŸŒ”ğŸƒğŸŒ’ğŸŒ’ğŸŒ”ğŸŒ’ğŸƒ\nğŸŒ”ğŸƒğŸƒğŸŒ’ğŸŒ–ğŸ¦†ğŸ¦†\nğŸ¦†ğŸŒ’ğŸƒğŸ¦†ğŸ¦†ğŸŒ’ğŸŒ–\nğŸ¦†ğŸƒğŸ¦†ğŸŒ”ğŸƒğŸƒğŸŒ”\nğŸŒ–ğŸ¦†ğŸŒ’ğŸ¦†ğŸŒ’ğŸ¦†ğŸŒ–\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14789, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-02 03:21:36 async_llm_engine.py:211] Added request chatcmpl-d80763917cae4ea7a856ab590a0c4446.
INFO 03-02 03:21:38 metrics.py:455] Avg prompt throughput: 41.3 tokens/s, Avg generation throughput: 2.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:21:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:21:48 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:21:53 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:21:58 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:22:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 11.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:22:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:22:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:22:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:22:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:22:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:22:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:22:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:22:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:22:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:22:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:22:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:23:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:23:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:23:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:23:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:23:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 4.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:23:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-02 03:23:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:23:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:23:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 11.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:23:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 11.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:23:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:24:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:24:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:24:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:24:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:24:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:24:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:24:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:24:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:24:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:24:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:24:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:24:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:25:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:25:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:25:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:25:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:25:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:25:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:25:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:25:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:25:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:25:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:25:53 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:25:58 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:26:03 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:26:08 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:26:13 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:26:18 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 23.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:26:23 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:26:28 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:26:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:26:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:26:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:26:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:26:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:26:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:27:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:27:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:27:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:27:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:27:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:27:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:27:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 16.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:27:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-02 03:27:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 35.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:27:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 13.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:27:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:27:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:28:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:28:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:28:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:28:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:28:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:28:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:28:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:28:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:28:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:28:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:28:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:29:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:29:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:29:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:29:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:29:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:29:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:29:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:29:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:29:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:29:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:29:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:29:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:30:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 23.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:30:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:30:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:30:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:30:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:30:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:30:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:30:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:30:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:30:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:30:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:30:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:31:03 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:31:08 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:31:13 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:31:18 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:31:23 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:31:28 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:31:33 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-02 03:31:36 async_llm_engine.py:223] Aborted request chatcmpl-d80763917cae4ea7a856ab590a0c4446.
INFO 03-02 03:31:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
