INFO 03-01 14:33:29 __init__.py:207] Automatically detected platform cuda.
INFO 03-01 14:33:29 api_server.py:912] vLLM API server version 0.7.3
INFO 03-01 14:33:29 api_server.py:913] args: Namespace(host=None, port=9003, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='/map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Llama-70B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=15000, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=2, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.95, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['DeepSeek-R1-Distill-Llama-70B'], qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False)
INFO 03-01 14:33:36 config.py:549] This model supports multiple tasks: {'score', 'generate', 'reward', 'embed', 'classify'}. Defaulting to 'generate'.
INFO 03-01 14:33:36 config.py:1382] Defaulting to use mp for distributed inference
WARNING 03-01 14:33:36 config.py:676] Async output processing can not be enabled with pipeline parallel
INFO 03-01 14:33:36 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Llama-70B', speculative_config=None, tokenizer='/map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Llama-70B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=15000, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=DeepSeek-R1-Distill-Llama-70B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
WARNING 03-01 14:33:36 multiproc_worker_utils.py:300] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 03-01 14:33:36 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-01 14:33:36 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-01 14:33:36 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-01 14:33:36 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-01 14:33:36 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-01 14:33:36 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-01 14:33:36 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-01 14:33:36 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-01 14:33:37 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-01 14:33:37 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-01 14:33:37 cuda.py:229] Using Flash Attention backend.
INFO 03-01 14:33:37 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-01 14:33:37 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-01 14:33:37 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-01 14:33:38 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-01 14:33:38 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-01 14:33:40 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-01 14:33:40 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-01 14:33:40 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-01 14:33:40 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-01 14:33:40 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-01 14:33:40 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-01 14:33:40 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-01 14:33:40 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-01 14:33:40 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-01 14:33:40 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-01 14:33:40 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-01 14:33:40 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-01 14:33:40 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 03-01 14:33:40 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-01 14:33:40 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 03-01 14:33:40 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 03-01 14:33:41 custom_all_reduce_utils.py:206] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
INFO 03-01 14:34:04 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-01 14:34:04 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-01 14:34:04 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-01 14:34:04 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-01 14:34:04 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-01 14:34:04 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-01 14:34:04 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-01 14:34:04 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-01 14:34:04 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_53354d39'), local_subscribe_port=53797, remote_subscribe_port=None)
INFO 03-01 14:34:04 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_ba0f745a'), local_subscribe_port=38195, remote_subscribe_port=None)
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-01 14:34:04 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-01 14:34:04 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-01 14:34:04 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-01 14:34:04 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-01 14:34:04 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-01 14:34:04 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-01 14:34:04 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-01 14:34:04 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-01 14:34:04 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-01 14:34:04 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-01 14:34:04 utils.py:916] Found nccl from library libnccl.so.2
INFO 03-01 14:34:04 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-01 14:34:04 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-01 14:34:04 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-01 14:34:04 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 03-01 14:34:04 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 03-01 14:34:04 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Llama-70B...
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-01 14:34:04 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Llama-70B...
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-01 14:34:04 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Llama-70B...
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-01 14:34:04 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Llama-70B...
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-01 14:34:04 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Llama-70B...
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-01 14:34:04 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Llama-70B...
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-01 14:34:04 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Llama-70B...
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-01 14:34:04 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Llama-70B...
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-01 14:35:34 model_runner.py:1115] Loading model weights took 16.4603 GB
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-01 14:35:34 model_runner.py:1115] Loading model weights took 16.4603 GB
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-01 14:35:34 model_runner.py:1115] Loading model weights took 16.4603 GB
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-01 14:35:34 model_runner.py:1115] Loading model weights took 16.4603 GB
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-01 14:35:36 model_runner.py:1115] Loading model weights took 16.4603 GB
INFO 03-01 14:35:36 model_runner.py:1115] Loading model weights took 16.4603 GB
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-01 14:35:36 model_runner.py:1115] Loading model weights took 16.4603 GB
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-01 14:35:36 model_runner.py:1115] Loading model weights took 16.4603 GB
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-01 14:35:39 worker.py:267] Memory profiling takes 2.59 seconds
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-01 14:35:39 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-01 14:35:39 worker.py:267] model weights take 16.46GiB; non_torch_memory takes 1.70GiB; PyTorch activation peak memory takes 1.58GiB; the rest of the memory reserved for KV Cache is 55.64GiB.
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-01 14:35:39 worker.py:267] Memory profiling takes 2.60 seconds
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-01 14:35:39 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-01 14:35:39 worker.py:267] model weights take 16.46GiB; non_torch_memory takes 1.51GiB; PyTorch activation peak memory takes 1.58GiB; the rest of the memory reserved for KV Cache is 55.83GiB.
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-01 14:35:39 worker.py:267] Memory profiling takes 2.64 seconds
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-01 14:35:39 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-01 14:35:39 worker.py:267] model weights take 16.46GiB; non_torch_memory takes 1.70GiB; PyTorch activation peak memory takes 1.58GiB; the rest of the memory reserved for KV Cache is 55.64GiB.
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-01 14:35:39 worker.py:267] Memory profiling takes 2.70 seconds
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-01 14:35:39 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-01 14:35:39 worker.py:267] model weights take 16.46GiB; non_torch_memory takes 1.70GiB; PyTorch activation peak memory takes 1.86GiB; the rest of the memory reserved for KV Cache is 55.36GiB.
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-01 14:35:42 worker.py:267] Memory profiling takes 5.54 seconds
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-01 14:35:42 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-01 14:35:42 worker.py:267] model weights take 16.46GiB; non_torch_memory takes 1.03GiB; PyTorch activation peak memory takes 1.35GiB; the rest of the memory reserved for KV Cache is 56.54GiB.
INFO 03-01 14:35:42 worker.py:267] Memory profiling takes 5.59 seconds
INFO 03-01 14:35:42 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
INFO 03-01 14:35:42 worker.py:267] model weights take 16.46GiB; non_torch_memory takes 0.94GiB; PyTorch activation peak memory takes 1.35GiB; the rest of the memory reserved for KV Cache is 56.63GiB.
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-01 14:35:42 worker.py:267] Memory profiling takes 5.59 seconds
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-01 14:35:42 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-01 14:35:42 worker.py:267] model weights take 16.46GiB; non_torch_memory takes 1.03GiB; PyTorch activation peak memory takes 1.35GiB; the rest of the memory reserved for KV Cache is 56.54GiB.
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-01 14:35:42 worker.py:267] Memory profiling takes 5.59 seconds
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-01 14:35:42 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-01 14:35:42 worker.py:267] model weights take 16.46GiB; non_torch_memory takes 0.94GiB; PyTorch activation peak memory takes 1.35GiB; the rest of the memory reserved for KV Cache is 56.63GiB.
INFO 03-01 14:35:42 executor_base.py:111] # cuda blocks: 90709, # CPU blocks: 6553
INFO 03-01 14:35:42 executor_base.py:116] Maximum concurrency for 15000 tokens per request: 96.76x
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-01 14:35:45 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 03-01 14:35:45 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-01 14:35:45 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-01 14:35:45 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-01 14:35:45 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-01 14:35:45 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-01 14:35:45 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-01 14:35:45 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-01 14:36:18 custom_all_reduce.py:226] Registering 5600 cuda graph addresses
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-01 14:36:18 custom_all_reduce.py:226] Registering 5600 cuda graph addresses
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-01 14:36:18 custom_all_reduce.py:226] Registering 5600 cuda graph addresses
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-01 14:36:20 custom_all_reduce.py:226] Registering 5600 cuda graph addresses
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-01 14:36:20 model_runner.py:1562] Graph capturing finished in 34 secs, took 2.46 GiB
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-01 14:36:20 model_runner.py:1562] Graph capturing finished in 34 secs, took 2.46 GiB
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-01 14:36:20 model_runner.py:1562] Graph capturing finished in 34 secs, took 2.46 GiB
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-01 14:36:20 model_runner.py:1562] Graph capturing finished in 34 secs, took 2.46 GiB
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-01 14:36:21 custom_all_reduce.py:226] Registering 5670 cuda graph addresses
INFO 03-01 14:36:21 custom_all_reduce.py:226] Registering 5670 cuda graph addresses
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-01 14:36:21 custom_all_reduce.py:226] Registering 5670 cuda graph addresses
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-01 14:36:21 custom_all_reduce.py:226] Registering 5670 cuda graph addresses
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-01 14:36:21 model_runner.py:1562] Graph capturing finished in 36 secs, took 2.49 GiB
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-01 14:36:21 model_runner.py:1562] Graph capturing finished in 36 secs, took 2.49 GiB
INFO 03-01 14:36:21 model_runner.py:1562] Graph capturing finished in 36 secs, took 2.49 GiB
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-01 14:36:21 model_runner.py:1562] Graph capturing finished in 36 secs, took 2.49 GiB
INFO 03-01 14:36:21 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 45.14 seconds
INFO 03-01 14:36:23 api_server.py:958] Starting vLLM API server on http://0.0.0.0:9003
INFO 03-01 14:36:23 launcher.py:23] Available routes are:
INFO 03-01 14:36:23 launcher.py:31] Route: /openapi.json, Methods: GET, HEAD
INFO 03-01 14:36:23 launcher.py:31] Route: /docs, Methods: GET, HEAD
INFO 03-01 14:36:23 launcher.py:31] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 03-01 14:36:23 launcher.py:31] Route: /redoc, Methods: GET, HEAD
INFO 03-01 14:36:23 launcher.py:31] Route: /health, Methods: GET
INFO 03-01 14:36:23 launcher.py:31] Route: /ping, Methods: GET, POST
INFO 03-01 14:36:23 launcher.py:31] Route: /tokenize, Methods: POST
INFO 03-01 14:36:23 launcher.py:31] Route: /detokenize, Methods: POST
INFO 03-01 14:36:23 launcher.py:31] Route: /v1/models, Methods: GET
INFO 03-01 14:36:23 launcher.py:31] Route: /version, Methods: GET
INFO 03-01 14:36:23 launcher.py:31] Route: /v1/chat/completions, Methods: POST
INFO 03-01 14:36:23 launcher.py:31] Route: /v1/completions, Methods: POST
INFO 03-01 14:36:23 launcher.py:31] Route: /v1/embeddings, Methods: POST
INFO 03-01 14:36:23 launcher.py:31] Route: /pooling, Methods: POST
INFO 03-01 14:36:23 launcher.py:31] Route: /score, Methods: POST
INFO 03-01 14:36:23 launcher.py:31] Route: /v1/score, Methods: POST
INFO 03-01 14:36:23 launcher.py:31] Route: /v1/audio/transcriptions, Methods: POST
INFO 03-01 14:36:23 launcher.py:31] Route: /rerank, Methods: POST
INFO 03-01 14:36:23 launcher.py:31] Route: /v1/rerank, Methods: POST
INFO 03-01 14:36:23 launcher.py:31] Route: /v2/rerank, Methods: POST
INFO 03-01 14:36:23 launcher.py:31] Route: /invocations, Methods: POST
INFO 03-01 14:38:18 chat_utils.py:332] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
INFO 03-01 14:38:18 logger.py:39] Received request chatcmpl-fa5eade89b8f4267a9a228bc988fd1d6: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ‹ğŸ‹ğŸš€ğŸ‹ğŸš€ğŸšğŸŒ³\nğŸšğŸŒ³ğŸš€ğŸŒ³ğŸ¦©ğŸšğŸš€\nğŸšğŸšğŸŒ³ğŸš€ğŸ¦©ğŸ¦©ğŸš€\nğŸ‹ğŸšğŸ¦©ğŸš€ğŸ‹ğŸš€ğŸš\nğŸŒ³ğŸšğŸ‹ğŸ‹ğŸšğŸš€ğŸ¦©\nğŸšğŸ¦©ğŸ¦©ğŸ‹ğŸ¦©ğŸšğŸ‹\nğŸš€ğŸŒ³ğŸš€ğŸ¦©ğŸŒ³ğŸšğŸŒ³\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:38:18 async_llm_engine.py:211] Added request chatcmpl-fa5eade89b8f4267a9a228bc988fd1d6.
INFO 03-01 14:38:18 logger.py:39] Received request chatcmpl-aa1ed52d3ca140d3818d907b5c6b7237: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ³ğŸ³ğŸ³ğŸ³ğŸŒğŸ¦…ğŸ³\nğŸ¦…ğŸ³ğŸ³ğŸ¤ğŸ¦…ğŸ³ğŸ°\nğŸŒğŸ¤ğŸ¦…ğŸ°ğŸ¦…ğŸ¦…ğŸ¦…\nğŸ¤ğŸ¦…ğŸ³ğŸŒğŸ³ğŸ¦…ğŸ¤\nğŸŒğŸ³ğŸ³ğŸ¤ğŸŒğŸ°ğŸŒ\nğŸŒğŸ³ğŸ°ğŸ¦…ğŸ³ğŸ¤ğŸ¦…\nğŸŒğŸ°ğŸ¦…ğŸ°ğŸ¤ğŸ³ğŸ¤\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:38:18 async_llm_engine.py:211] Added request chatcmpl-aa1ed52d3ca140d3818d907b5c6b7237.
INFO 03-01 14:38:18 logger.py:39] Received request chatcmpl-47f5ec01c1d34bac9a7e10c3616e1754: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¯ğŸ¯ğŸ¦‹ğŸ®ğŸ¦‹ğŸ­ğŸ¦‹\nğŸ®ğŸˆğŸˆğŸ¦‹ğŸ¯ğŸ¦‹ğŸˆ\nğŸ¦‹ğŸ¯ğŸ­ğŸ­ğŸ¯ğŸˆğŸ¯\nğŸˆğŸˆğŸ¦‹ğŸ®ğŸ¦‹ğŸ®ğŸ®\nğŸ®ğŸ­ğŸ®ğŸ¯ğŸ®ğŸ®ğŸ¦‹\nğŸ¦‹ğŸ¯ğŸ¦‹ğŸ¦‹ğŸ®ğŸˆğŸˆ\nğŸ¦‹ğŸ¯ğŸ¦‹ğŸ¯ğŸ¯ğŸˆğŸ®\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:38:18 async_llm_engine.py:211] Added request chatcmpl-47f5ec01c1d34bac9a7e10c3616e1754.
INFO 03-01 14:38:18 logger.py:39] Received request chatcmpl-8cc0106533e54795b297738da3c0c26d: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¦ƒğŸ¦†ğŸ¦ƒğŸ¦€ğŸ¦ƒğŸ¦€ğŸ¦†\nğŸŒ‘ğŸ¦ƒâ˜„ï¸â˜„ï¸ğŸ¦†ğŸ¦ƒğŸ¦ƒ\nğŸ¦†ğŸ¦†â˜„ï¸â˜„ï¸â˜„ï¸ğŸ¦ƒğŸ¦†\nğŸ¦€ğŸ¦€â˜„ï¸ğŸ¦ƒğŸ¦€ğŸŒ‘ğŸ¦€\nğŸŒ‘ğŸ¦†ğŸ¦ƒğŸ¦†ğŸ¦†ğŸ¦†ğŸ¦ƒ\nğŸ¦†â˜„ï¸ğŸŒ‘ğŸ¦€ğŸ¦€â˜„ï¸ğŸ¦†\nâ˜„ï¸ğŸŒ‘ğŸ¦†ğŸŒ‘ğŸ¦†ğŸ¦ƒğŸ¦†\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:38:18 async_llm_engine.py:211] Added request chatcmpl-8cc0106533e54795b297738da3c0c26d.
INFO 03-01 14:38:18 logger.py:39] Received request chatcmpl-d3dc88bc0dc84ee0bd3e11d556fb3db6: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ«ğŸ¦€ğŸ¦€ğŸ¦€ğŸ¦¥ğŸ¥€ğŸµ\nğŸ¦€ğŸ«ğŸ¥€ğŸ¥€ğŸµğŸµğŸ«\nğŸ¦€ğŸµğŸ«ğŸ¦€ğŸµğŸµğŸ«\nğŸ«ğŸµğŸµğŸµğŸ¦€ğŸ¥€ğŸµ\nğŸ¦¥ğŸ¥€ğŸ«ğŸ«ğŸ«ğŸ¦¥ğŸµ\nğŸ¦€ğŸ¥€ğŸµğŸ¦€ğŸµğŸ¥€ğŸµ\nğŸ¥€ğŸ«ğŸ¦¥ğŸ¦¥ğŸ¥€ğŸµğŸ¦¥\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:38:18 async_llm_engine.py:211] Added request chatcmpl-d3dc88bc0dc84ee0bd3e11d556fb3db6.
INFO 03-01 14:38:18 logger.py:39] Received request chatcmpl-a0097d0740ff4d46ab6f2e9f93649e08: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸŒ•ğŸ³ğŸ’§ğŸ³ğŸ’§ğŸ¦®ğŸŒ•\nğŸ³ğŸŒ•ğŸ¦­ğŸ’§ğŸ¦­ğŸ³ğŸ’§\nğŸ³ğŸŒ•ğŸ¦®ğŸ³ğŸŒ•ğŸ³ğŸ’§\nğŸ³ğŸ¦­ğŸ¦­ğŸŒ•ğŸ³ğŸ³ğŸ’§\nğŸ’§ğŸ³ğŸ³ğŸ³ğŸ³ğŸ¦®ğŸ¦®\nğŸ³ğŸ¦­ğŸ³ğŸ³ğŸ³ğŸŒ•ğŸ¦®\nğŸ’§ğŸ¦®ğŸŒ•ğŸ³ğŸ³ğŸ¦®ğŸ’§\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14719, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:38:18 async_llm_engine.py:211] Added request chatcmpl-a0097d0740ff4d46ab6f2e9f93649e08.
INFO 03-01 14:38:18 logger.py:39] Received request chatcmpl-3d67730d46a643b092eb7e426e784316: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸŒ™ğŸ¨ğŸ¤šğŸ‹ğŸŒ¼ğŸŒ¼ğŸŒ™\nğŸŒ¼ğŸ‹ğŸ¨ğŸ‹ğŸ¤šğŸ¤šğŸŒ™\nğŸ¨ğŸ¤šğŸ¤šğŸŒ¼ğŸ¤šğŸŒ™ğŸ‹\nğŸŒ¼ğŸŒ™ğŸŒ¼ğŸ‹ğŸŒ¼ğŸ‹ğŸŒ™\nğŸ¨ğŸŒ¼ğŸŒ™ğŸŒ™ğŸ¤šğŸ¨ğŸŒ¼\nğŸ‹ğŸ¨ğŸ¤šğŸ¤šğŸ¤šğŸŒ™ğŸŒ¼\nğŸ¨ğŸ¤šğŸ‹ğŸŒ¼ğŸŒ¼ğŸ¤šğŸŒ™\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:38:18 async_llm_engine.py:211] Added request chatcmpl-3d67730d46a643b092eb7e426e784316.
INFO 03-01 14:38:18 logger.py:39] Received request chatcmpl-43b15bf1869d4f35a492a844fcc76dc3: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ»ğŸ„ğŸ¦¥ğŸ»ğŸ„ğŸ»ğŸ—\nğŸ„ğŸ„ğŸ¶ğŸ¶ğŸ„ğŸ—ğŸ„\nğŸ»ğŸ¶ğŸ„ğŸ»ğŸ„ğŸ—ğŸ»\nğŸ„ğŸ»ğŸ»ğŸ¶ğŸ„ğŸ—ğŸ„\nğŸ»ğŸ—ğŸ¦¥ğŸ¶ğŸ—ğŸ»ğŸ„\nğŸ»ğŸ¦¥ğŸ»ğŸ—ğŸ„ğŸ»ğŸ»\nğŸ—ğŸ¦¥ğŸ„ğŸ»ğŸ„ğŸ»ğŸ„\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:38:18 async_llm_engine.py:211] Added request chatcmpl-43b15bf1869d4f35a492a844fcc76dc3.
INFO 03-01 14:38:19 metrics.py:455] Avg prompt throughput: 50.2 tokens/s, Avg generation throughput: 0.2 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:38:24 metrics.py:455] Avg prompt throughput: 403.1 tokens/s, Avg generation throughput: 220.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:38:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 230.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:38:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 228.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:38:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 225.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:38:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 226.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:38:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 225.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:38:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 224.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:38:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 223.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:39:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 222.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:39:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 219.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:39:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 219.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:39:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 218.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:39:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 216.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:39:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 216.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:39:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 215.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:39:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 214.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:39:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 213.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:39:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:39:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 210.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:39:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 210.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:40:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 210.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:40:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 209.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:40:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:40:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:40:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:40:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:40:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:40:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:40:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 200.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:40:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 200.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:40:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 198.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:40:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 198.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:41:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 196.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:41:06 async_llm_engine.py:179] Finished request chatcmpl-3d67730d46a643b092eb7e426e784316.
INFO:     127.0.0.1:37594 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:41:06 logger.py:39] Received INFO 03-01 14:41:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 198.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:41:12 async_llm_engine.py:179] Finished request chatcmpl-22932c40a22c4e63a2028ce5141feffa.
INFO:     127.0.0.1:44376 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:41:12 logger.py:39] Received request chatcmpl-2533a55541b34c0cac94cb7da0950e01: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¦ğŸ¦ğŸ¦ƒğŸ¦ƒğŸ•·ğŸ¦ƒğŸ¥\nğŸ¦ğŸ¥ğŸ¥ğŸ¥ğŸ¥ğŸ¥ğŸ•·\nğŸ¦ƒğŸ¥ğŸ¦ğŸ¥ğŸ¦ƒğŸ¦ğŸ¦œ\nğŸ•·ğŸ¥ğŸ¥ğŸ¥ğŸ¦ğŸ¦œğŸ¦\nğŸ¦ğŸ¥ğŸ•·ğŸ¥ğŸ¦ğŸ¦ğŸ¦œ\nğŸ¦ƒğŸ¦ğŸ¥ğŸ•·ğŸ¦ƒğŸ•·ğŸ¦ƒ\nğŸ•·ğŸ¦ğŸ•·ğŸ¦ğŸ¥ğŸ¦ƒğŸ¦ƒ\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_INFO 03-01 14:41:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:41:17 async_llm_engine.py:179] Finished request chatcmpl-43b15bf1869d4f35a492a844fcc76dc3.
INFO:     127.0.0.1:37604 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:41:17 logger.py:39] Received INFO 03-01 14:41:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 200.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:41:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 199.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:41:26 async_llm_engine.py:179] Finished request chatcmpl-3988a72be57f40bf87f2e0e2e8216955.
INFO:     127.0.0.1:44352 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:41:26 logger.py:39] Received request chatcmpl-b5dc06bfc3d340b5ad1d1773407f8ee9: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nâ›…ï¸ğŸŒ¾ğŸ¦šğŸ¦‰ğŸŠğŸ¦‰ğŸŒ¾\nğŸ¦šğŸ¦‰ğŸ¦‰ğŸŒ¾ğŸŠâ›…ï¸ğŸŠ\nğŸŠâ›…ï¸â›…ï¸â›…ï¸ğŸŒ¾ğŸŒ¾â›…ï¸\nğŸ¦‰ğŸ¦‰ğŸŒ¾ğŸŠğŸŒ¾ğŸ¦šâ›…ï¸\nâ›…ï¸ğŸŠğŸŒ¾ğŸ¦šâ›…ï¸ğŸŠğŸ¦š\nğŸ¦šğŸŒ¾â›…ï¸ğŸŒ¾ğŸŠğŸŒ¾ğŸŠ\nğŸŠâ›…ï¸ğŸŒ¾â›…ï¸ğŸŠğŸŒ¾ğŸ¦‰\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14698, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_INFO 03-01 14:41:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:41:31 async_llm_engine.py:179] Finished request chatcmpl-a0097d0740ff4d46ab6f2e9f93649e08.
INFO:     127.0.0.1:37592 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:41:31 logger.py:39] Received INFO 03-01 14:41:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:41:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:41:41 async_llm_engine.py:179] Finished request chatcmpl-fc301420f354418ab9780e71b5aac6cc.
INFO:     127.0.0.1:44342 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:41:41 logger.py:39] Received request chatcmpl-c73f62342e9a4209ab05ccf03fca1461: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸŒ™ğŸ§½ğŸŒ·ğŸŒ·ğŸŒ±ğŸ§½ğŸŒ™\nâ­ï¸ğŸŒ·ğŸŒ±ğŸŒ™ğŸ§½ğŸŒ±ğŸŒ™\nğŸŒ·ğŸ§½â­ï¸ğŸŒ±ğŸŒ·ğŸŒ·ğŸŒ±\nğŸŒ±ğŸŒ·ğŸ§½ğŸŒ±ğŸŒ±ğŸ§½ğŸŒ·\nğŸ§½ğŸŒ·â­ï¸ğŸŒ·ğŸŒ™â­ï¸ğŸŒ™\nğŸ§½ğŸ§½ğŸ§½ğŸŒ±â­ï¸ğŸŒ±ğŸ§½\nğŸŒ±â­ï¸ğŸŒ·ğŸŒ·ğŸŒ·ğŸŒ™â­ï¸\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14717, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:41:41 async_llm_engine.py:211] Added request chatcmpl-c73f62342e9a4209ab05ccf03fca1461.
INFO 03-01 14:41:45 metrics.py:455] Avg prompt throughput: 56.5 tokens/s, Avg generation throughput: 199.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:41:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:41:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:42:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:42:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:42:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 199.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:42:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 198.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:42:15 async_llm_engine.py:179] Finished request chatcmpl-8d2f0304c0914e80be1f564ff3903253.
INFO:     127.0.0.1:44314 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:42:15 logger.py:39] Received request chatcmpl-d1df2f7ab6f54c2f93b9bf3783a2f4d6: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸŒšğŸŒšâœ¨ğŸšâœ¨ğŸŒšğŸƒ\nğŸšğŸŒºğŸŒšğŸƒâœ¨ğŸƒâœ¨\nğŸšğŸšâœ¨ğŸŒšâœ¨ğŸƒâœ¨\nğŸƒğŸƒğŸƒğŸŒšğŸŒšâœ¨ğŸƒ\nğŸšğŸŒºğŸšâœ¨ğŸŒšâœ¨ğŸŒš\nğŸŒºğŸšğŸƒğŸƒğŸšğŸŒºğŸš\nâœ¨ğŸƒğŸŒºğŸŒºğŸŒšâœ¨ğŸŒº\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14722, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_INFO 03-01 14:42:19 async_llm_engine.py:179] Finished request chatcmpl-aa1ed52d3ca140d3818d907b5c6b7237.
INFO:     127.0.0.1:37548 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:42:19 logger.py:39] Received request chatcmpl-ceb590d6209244f89fbec87960c76522: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ•ğŸŒ¹ğŸŒ•ğŸ•ğŸŒ•ğŸ•ğŸ¦‡\nğŸŠğŸ¦‡ğŸŒ¹ğŸ¦‡ğŸŠğŸ•ğŸŒ•\nğŸŠğŸŒ•ğŸŠğŸŒ¹ğŸ¦‡ğŸŠğŸŒ¹\nğŸ¦‡ğŸŒ•ğŸ•ğŸŒ¹ğŸŒ¹ğŸŒ¹ğŸ¦‡\nğŸŠğŸŒ¹ğŸ•ğŸ•ğŸŒ¹ğŸ•ğŸŠ\nğŸ¦‡ğŸŒ•ğŸ¦‡ğŸ¦‡ğŸŠğŸŒ¹ğŸ¦‡\nğŸŠğŸ¦‡ğŸŒ¹ğŸ•ğŸ¦‡ğŸ•ğŸ¦‡\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:42:19 async_llm_engine.py:211] Added request chatcmpl-ceb590d6209244f89fbec87960c76522.
INFO 03-01 14:42:19 metrics.py:455] Avg prompt throughput: 57.9 tokens/s, Avg generation throughput: 200.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.IINFO 03-01 14:42:22 async_llm_engine.py:179] Finished request chatcmpl-8cc0106533e54795b297738da3c0c26d.
INFO:     127.0.0.1:37576 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:42:22 logger.py:39] Received INFO 03-01 14:42:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:42:27 async_llm_engine.py:179] Finished request chatcmpl-53e7c894819544f0be4fdde8cbdf80e8.
INFO:     127.0.0.1:44336 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:42:27 logger.py:39] Received request chatcmpl-9a74adb3e1e146fd9c97025b704ff58a: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¦ğŸ¦ğŸ¦ğŸ‹ğŸºğŸ¦ğŸ¦Ÿ\nğŸ‹ğŸ¦ğŸ¦ğŸºğŸ‹ğŸ¦ŸğŸ¦\nğŸ‹ğŸ¦ŸğŸ¦ğŸ¦ğŸ¦ğŸ¦ŸğŸ¦Ÿ\nğŸ¦ğŸ¦ğŸºğŸºğŸ¦ğŸ¦ŸğŸ¦\nğŸ¦ŸğŸ‹ğŸ¦ŸğŸºğŸ¦ŸğŸ¦ğŸº\nğŸºğŸ¦ğŸ¦ğŸ¦ğŸºğŸ¦ğŸ¦Ÿ\nğŸ¦ğŸ¦ğŸºğŸ¦ŸğŸ¦ŸğŸ¦ŸğŸ¦\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_INFO 03-01 14:42:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:42:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 210.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:42:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 209.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:42:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:42:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:42:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:42:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:43:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:43:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:43:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:43:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 200.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:43:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 176.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:43:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 164.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:43:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 169.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:43:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 168.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:43:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 169.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:43:47 async_llm_engine.py:179] Finished request chatcmpl-47f5ec01c1d34bac9a7e10c3616e1754.
INFO:     127.0.0.1:37560 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:43:47 logger.py:39] Received INFO 03-01 14:43:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 168.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:43:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 167.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:44:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 167.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:44:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 168.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:44:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 168.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:44:10 async_llm_engine.py:179] Finished request chatcmpl-2788169910f6415f83b391a9b81fae33.
INFO:     127.0.0.1:44362 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:44:10 logger.py:39] Received request chatcmpl-d9a37ea8ba684e3fa34cb0063d0d1620: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸŒğŸŒğŸ”ğŸŸğŸŒğŸŸğŸŸ\nğŸŸğŸ”ğŸŒâ˜ï¸ğŸŒğŸŒğŸŒ\nğŸŒğŸ…â˜ï¸â˜ï¸ğŸ…ğŸŒâ˜ï¸\nğŸ”ğŸ”ğŸ”ğŸ…ğŸŒğŸ”ğŸ”\nğŸŒâ˜ï¸ğŸŸğŸŸğŸ…ğŸŸğŸ…\nğŸ”ğŸ”ğŸ…ğŸ…ğŸŒâ˜ï¸ğŸ”\nğŸŸğŸ…â˜ï¸ğŸŒğŸŸğŸ…â˜ï¸\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:44:10 async_llm_engine.py:211] Added request chatcmpl-d9a37ea8ba684e3fa34cb0063d0d1620.
INFO 03-01 14:44:15 metrics.py:455] Avg prompt throughput: 57.9 tokens/s, Avg generation throughput: 166.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:44:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 166.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:44:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 169.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:44:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 169.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:44:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 169.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:44:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 168.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:44:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 167.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:44:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 168.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:44:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 167.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:44:57 async_llm_engine.py:179] Finished request chatcmpl-bb444482c3a74e5096e14e9c13aa5558.
INFO:     127.0.0.1:44306 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:44:57 logger.py:39] Received request chatcmpl-14eb42f409044a0ebfaec97c15b321f7: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ˜„ğŸ¦ğŸ¦ğŸ˜„ğŸŒğŸ˜„ğŸ¦\nğŸŒğŸ¤šğŸŒğŸŒğŸ¤šğŸŒğŸŒ\nğŸ˜„ğŸ˜„ğŸŒğŸŒğŸ˜„ğŸŒğŸ¦„\nğŸ¤šğŸ˜„ğŸ¦„ğŸ¤šğŸ¤šğŸ¤šğŸ¦\nğŸ¤šğŸ¦„ğŸŒğŸ¤šğŸ¦„ğŸŒğŸ¤š\nğŸ¤šğŸ¦ğŸ¤šğŸ¦ğŸ˜„ğŸ˜„ğŸ¦„\nğŸ˜„ğŸ¦„ğŸ¦ğŸŒğŸ¦„ğŸ¤šğŸ˜„\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14721, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:44:57 async_llm_engine.py:211] Added request chatcmpl-14eb42f409044a0ebfaec97c15b321f7.
INFO 03-01 14:45:00 metrics.py:455] Avg prompt throughput: 55.6 tokens/s, Avg generation throughput: 183.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:45:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:45:08 async_llm_engine.py:179] Finished request chatcmpl-c73f62342e9a4209ab05ccf03fca1461.
INFO:     127.0.0.1:33970 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:45:09 logger.py:39] Received request chatcmpl-0fe62044502c4f28b3baa3de36aa4697: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸŒ»ğŸ¦ğŸˆğŸ¦ğŸŒ·ğŸŒ»ğŸŒ»\nğŸˆğŸŒ·ğŸŒ·ğŸŒ·ğŸŒ»ğŸŒ²ğŸŒ·\nğŸˆğŸŒ»ğŸŒ²ğŸŒ·ğŸˆğŸŒ»ğŸŒ²\nğŸˆğŸ¦ğŸ¦ğŸŒ²ğŸŒ·ğŸŒ»ğŸŒ·\nğŸŒ·ğŸŒ·ğŸ¦ğŸˆğŸŒ·ğŸ¦ğŸ¦\nğŸŒ²ğŸŒ»ğŸŒ²ğŸŒ·ğŸŒ²ğŸˆğŸŒ·\nğŸˆğŸˆğŸŒ²ğŸŒ·ğŸŒ»ğŸ¦ğŸŒ»\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:45:09 async_llm_engine.py:211] Added request chatcmpl-0fe62044502c4f28b3baa3de36aa4697.
INFO 03-01 14:45:10 metrics.py:455] Avg prompt throughput: 57.9 tokens/s, Avg generation throughput: 202.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:45:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:45:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:45:24 async_llm_engine.py:179] Finished request chatcmpl-2533a55541b34c0cac94cb7da0950e01.
INFO:     127.0.0.1:59054 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:45:24 logger.py:39] Received request chatcmpl-4624f8192a9b4ceaa5427b89018e3c4a: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸŒ²ğŸ¦ğŸ«ğŸŒğŸŒğŸŒ²ğŸ“\nğŸ“ğŸ«ğŸ¦ğŸ«ğŸ“ğŸŒ²ğŸ¦\nğŸŒğŸŒ²ğŸŒ²ğŸŒ²ğŸ¦ğŸ¦ğŸŒ²\nğŸ«ğŸ¦ğŸŒğŸŒğŸ«ğŸ“ğŸŒ\nğŸ¦ğŸ“ğŸ¦ğŸŒ²ğŸ“ğŸ¦ğŸ«\nğŸŒğŸ“ğŸ«ğŸ¦ğŸŒğŸ«ğŸŒ\nğŸ“ğŸ«ğŸ¦ğŸŒğŸ“ğŸŒ²ğŸŒ\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_INFO 03-01 14:45:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:45:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:45:33 async_llm_engine.py:179] Finished request chatcmpl-1a1b08fb98fd40c290dfce6dddc696cc.
INFO:     127.0.0.1:58290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:45:33 logger.py:39] Received INFO 03-01 14:45:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:45:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:45:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:45:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:45:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:46:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:46:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 200.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:46:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:46:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 200.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:46:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 199.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:46:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 199.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:46:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 198.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:46:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 198.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:46:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 197.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:46:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 195.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:46:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 195.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:46:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 188.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:47:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 164.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:47:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 164.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:47:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 164.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:47:13 async_llm_engine.py:179] Finished request chatcmpl-d9a37ea8ba684e3fa34cb0063d0d1620.
INFO:     127.0.0.1:49628 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:47:13 logger.py:39] Received request chatcmpl-3766441b0603450b9400cc78d4236b81: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ£ğŸğŸğŸ£ğŸğŸ‹ğŸ\nğŸ‹ğŸ£ğŸ‹ğŸœğŸ‹ğŸ¡ğŸ£\nğŸ£ğŸ£ğŸ£ğŸ£ğŸ‹ğŸ¡ğŸ¡\nğŸ£ğŸğŸ£ğŸ‹ğŸ‹ğŸœğŸ‹\nğŸğŸ£ğŸğŸ¡ğŸ‹ğŸ¡ğŸ¡\nğŸ¡ğŸ‹ğŸ‹ğŸ¡ğŸ‹ğŸ¡ğŸœ\nğŸ‹ğŸğŸ¡ğŸ¡ğŸ£ğŸğŸ£\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14713, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:47:13 async_llm_engine.py:211] Added request chatcmpl-3766441b0603450b9400cc78d4236b81.
INFO 03-01 14:47:15 metrics.py:455] Avg prompt throughput: 57.3 tokens/s, Avg generation throughput: 160.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:47:16 async_llm_engine.py:179] Finished request chatcmpl-36da4eabba1541e6ad78c20aaa3f212f.
INFO:     127.0.0.1:48100 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:47:16 logger.py:39] Received request chatcmpl-fd85c45cd4664aab9ac2e110efeef5b1: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸšğŸ¤ğŸ¢ğŸ¤ğŸ¢ğŸŒğŸ¢\nğŸ¤ğŸğŸ¤ğŸšğŸ¤ğŸ¤ğŸ\nğŸ¤ğŸ¤ğŸğŸ¢ğŸ¢ğŸŒğŸ\nğŸŒğŸğŸ¢ğŸšğŸšğŸğŸ\nğŸšğŸğŸğŸ¢ğŸšğŸšğŸš\nğŸ¤ğŸ¢ğŸ¢ğŸŒğŸšğŸŒğŸ¢\nğŸ¢ğŸğŸŒğŸšğŸŒğŸŒğŸŒ\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:47:16 async_llm_engine.py:211] Added request chatcmpl-fd85c45cd4664aab9ac2e110efeef5b1.
INFO 03-01 14:47:20 metrics.py:455] Avg prompt throughput: 57.8 tokens/s, Avg generation throughput: 164.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:47:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 165.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:47:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 167.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:47:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 167.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:47:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 167.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:47:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 168.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:47:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 165.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:47:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 167.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:47:56 async_llm_engine.py:179] Finished request chatcmpl-b5dc06bfc3d340b5ad1d1773407f8ee9.
INFO:     127.0.0.1:57336 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:47:56 logger.py:39] Received request chatcmpl-80400bca6b454580b84164bd640efd32: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ‹ğŸ—ğŸğŸğŸ—ğŸ—ğŸ‹\nğŸ¸ğŸğŸŒ™ğŸğŸğŸ‹ğŸ‹\nğŸ¸ğŸ¸ğŸ¸ğŸğŸ—ğŸŒ™ğŸ\nğŸğŸ—ğŸ‹ğŸŒ™ğŸ‹ğŸ—ğŸ‹\nğŸ—ğŸ—ğŸ‹ğŸ¸ğŸ¸ğŸ¸ğŸ¸\nğŸ‹ğŸŒ™ğŸ—ğŸ—ğŸ¸ğŸ‹ğŸ‹\nğŸŒ™ğŸ‹ğŸ—ğŸ—ğŸŒ™ğŸ¸ğŸ¸\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:47:56 async_llm_engine.py:211] Added request chatcmpl-80400bca6b454580b84164bd640efd32.
INFO 03-01 14:48:00 metrics.py:455] Avg prompt throughput: 57.9 tokens/s, Avg generation throughput: 167.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:48:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 168.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:48:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 167.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:48:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 166.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:48:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 165.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:48:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 167.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:48:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 168.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:48:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 168.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:48:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 166.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:48:42 async_llm_engine.py:179] Finished request chatcmpl-d1df2f7ab6f54c2f93b9bf3783a2f4d6.
INFO:     127.0.0.1:48092 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:48:42 logger.py:39] Received request chatcmpl-75eb24a77caf41d0abcaf2395f62ccea: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¥€ğŸ¦•ğŸŒ³ğŸ¦•ğŸ¦•ğŸŒ³ğŸ›\nğŸ›ğŸ¦ğŸ¦•ğŸ¥€ğŸ›ğŸŒ³ğŸŒ³\nğŸŒ³ğŸ›ğŸ¥€ğŸ¥€ğŸ¦•ğŸ¥€ğŸ›\nğŸ¦ğŸ¦•ğŸ¥€ğŸ›ğŸ¦ğŸ›ğŸŒ³\nğŸ›ğŸ›ğŸ›ğŸ¥€ğŸŒ³ğŸ¦•ğŸ¦\nğŸ¥€ğŸ¥€ğŸŒ³ğŸŒ³ğŸ›ğŸ¥€ğŸŒ³\nğŸ›ğŸŒ³ğŸŒ³ğŸ¦•ğŸ›ğŸ›ğŸ¦•\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:48:42 async_llm_engine.py:211] Added request chatcmpl-75eb24a77caf41d0abcaf2395f62ccea.
INFO 03-01 14:48:45 metrics.py:455] Avg prompt throughput: 57.9 tokens/s, Avg generation throughput: 162.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:48:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 169.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:48:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 169.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:49:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 169.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:49:04 async_llm_engine.py:179] Finished request chatcmpl-9a74adb3e1e146fd9c97025b704ff58a.
INFO:     127.0.0.1:57180 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:49:04 logger.py:39] Received request chatcmpl-7923d9303ee643478d30f9c11bbdac48: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ ğŸ¦‰ğŸ„ğŸ¦‰ğŸ ğŸ„ğŸ¦‰\nğŸ„ğŸ‹ğŸ ğŸ‹â›…ï¸ğŸ¦‰ğŸ‹\nğŸ¦‰ğŸ‹ğŸ„ğŸ„â›…ï¸ğŸ ğŸ \nğŸ¦‰ğŸ‹ğŸ‹ğŸ‹ğŸ¦‰ğŸ„ğŸ‹\nğŸ„ğŸ„ğŸ â›…ï¸ğŸ„ğŸ‹ğŸ \nğŸ„ğŸ„ğŸ â›…ï¸ğŸ¦‰ğŸ„ğŸ‹\nğŸ¦‰â›…ï¸ğŸ„ğŸ‹ğŸ„ğŸ‹â›…ï¸\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14704, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:49:04 async_llm_engine.py:211] Added request chatcmpl-7923d9303ee643478d30f9c11bbdac48.
INFO 03-01 14:49:06 metrics.py:455] Avg prompt throughput: 59.2 tokens/s, Avg generation throughput: 177.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:49:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:49:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:49:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:49:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:49:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:49:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:49:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:49:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:49:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:49:53 async_llm_engine.py:179] Finished request chatcmpl-14eb42f409044a0ebfaec97c15b321f7.
INFO:     127.0.0.1:51936 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:49:53 logger.py:39] Received request chatcmpl-5b829dbb123540ee8051b4b03fd9d8f0: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nâ˜„ï¸â˜„ï¸ğŸ¦’ğŸ˜†ğŸ¯ğŸ¯ğŸ˜†\nâ˜„ï¸ğŸ¦ğŸ¯ğŸ¦’â˜„ï¸ğŸ¦ğŸ¦\nğŸ¦â˜„ï¸â˜„ï¸ğŸ¦ğŸ¦’â˜„ï¸ğŸ˜†\nğŸ¦ğŸ¯ğŸ¦ğŸ¦’ğŸ˜†ğŸ¯ğŸ¦’\nğŸ¯ğŸ˜†ğŸ¦’ğŸ˜†ğŸ¦’ğŸ˜†ğŸ˜†\nğŸ¦’ğŸ¦â˜„ï¸ğŸ¯â˜„ï¸ğŸ¦’â˜„ï¸\nğŸ¯ğŸ¯ğŸ¯ğŸ¦’ğŸ˜†ğŸ¦’ğŸ¯\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14719, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:49:53 async_llm_engine.py:211] Added request chatcmpl-5b829dbb123540ee8051b4b03fd9d8f0.
INFO 03-01 14:49:56 metrics.py:455] Avg prompt throughput: 56.2 tokens/s, Avg generation throughput: 200.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:49:57 async_llm_engine.py:179] Finished request chatcmpl-4624f8192a9b4ceaa5427b89018e3c4a.
INFO:     127.0.0.1:52420 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:49:57 logger.py:39] Received request chatcmpl-10007e35e32f423297498a43516cbc52: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ’«ğŸ’«ğŸ¦©ğŸ’«ğŸœğŸ¦©ğŸ¦•\nğŸ¦•ğŸ¦©ğŸ¦©ğŸ›ğŸ’«ğŸ¦•ğŸ¦•\nğŸœğŸ¦•ğŸ¦©ğŸœğŸ›ğŸ’«ğŸœ\nğŸœğŸ¦©ğŸ›ğŸœğŸ¦©ğŸ’«ğŸœ\nğŸœğŸ¦©ğŸœğŸ¦©ğŸ’«ğŸ›ğŸ’«\nğŸ¦•ğŸœğŸœğŸ›ğŸœğŸ’«ğŸ¦©\nğŸ›ğŸ¦•ğŸ’«ğŸ’«ğŸœğŸ¦•ğŸ’«\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14735, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:49:57 async_llm_engine.py:211] Added request chatcmpl-10007e35e32f423297498a43516cbc52.
INFO 03-01 14:50:01 metrics.py:455] Avg prompt throughput: 52.9 tokens/s, Avg generation throughput: 206.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:50:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 209.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:50:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:50:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:50:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:50:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:50:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:50:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:50:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:50:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 184.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:50:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 167.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:50:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 168.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:51:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 169.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:51:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 167.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:51:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 166.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:51:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 163.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:51:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 164.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:51:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 166.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:51:27 async_llm_engine.py:179] Finished request chatcmpl-3766441b0603450b9400cc78d4236b81.
INFO:     127.0.0.1:60860 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:51:27 logger.py:39] Received request chatcmpl-6f7536f1e2424c2aba67c1976b124ae8: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¦œğŸ¦–ğŸ¦œğŸ¦–ğŸŒ¹ğŸ¦’ğŸ¦œ\nğŸŒ¹ğŸ¦œğŸŒ¹ğŸ˜ğŸŒ¹ğŸŒ¹ğŸ¦–\nğŸ˜ğŸŒ¹ğŸ¦–ğŸŒ¹ğŸ¦’ğŸŒ¹ğŸ¦œ\nğŸ¦œğŸŒ¹ğŸŒ¹ğŸ¦–ğŸ¦’ğŸ¦–ğŸ˜\nğŸ˜ğŸŒ¹ğŸ¦’ğŸ¦œğŸ¦œğŸ¦’ğŸŒ¹\nğŸ¦œğŸŒ¹ğŸ˜ğŸ¦œğŸŒ¹ğŸ˜ğŸ¦œ\nğŸŒ¹ğŸ˜ğŸ¦’ğŸŒ¹ğŸ¦’ğŸ¦’ğŸ¦’\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14717, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:51:27 async_llm_engine.py:211] Added request chatcmpl-6f7536f1e2424c2aba67c1976b124ae8.
INFO 03-01 14:51:31 metrics.py:455] Avg prompt throughput: 56.5 tokens/s, Avg generation throughput: 164.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:51:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 166.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:51:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 166.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:51:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 165.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:51:48 async_llm_engine.py:179] Finished request chatcmpl-0fe62044502c4f28b3baa3de36aa4697.
INFO:     127.0.0.1:58960 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:51:48 logger.py:39] Received request chatcmpl-a92ee45db68b4a6196f4201bca200e9d: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¢ğŸğŸ’¥ğŸ¢ğŸŒ‘ğŸŒ‘ğŸ\nğŸ’¥ğŸğŸ’¥ğŸğŸ¢ğŸğŸ\nğŸğŸğŸŒ‘ğŸ’¥ğŸ’¥ğŸŒ¿ğŸ\nğŸŒ‘ğŸ¢ğŸ¢ğŸŒ¿ğŸŒ‘ğŸŒ‘ğŸ¢\nğŸŒ‘ğŸ¢ğŸ¢ğŸ’¥ğŸŒ‘ğŸŒ‘ğŸŒ¿\nğŸŒ¿ğŸ’¥ğŸ’¥ğŸğŸ¢ğŸŒ¿ğŸ\nğŸŒ‘ğŸ’¥ğŸ’¥ğŸŒ‘ğŸŒ‘ğŸğŸ¢\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14720, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:51:48 async_llm_engine.py:211] Added request chatcmpl-a92ee45db68b4a6196f4201bca200e9d.
INFO 03-01 14:51:51 metrics.py:455] Avg prompt throughput: 56.0 tokens/s, Avg generation throughput: 187.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:51:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:52:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:52:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:52:07 async_llm_engine.py:179] Finished request chatcmpl-80400bca6b454580b84164bd640efd32.
INFO:     127.0.0.1:42880 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:52:07 logger.py:39] Received request chatcmpl-b9f541affe48489c91a8f197cd4ebe5a: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¥€ğŸ˜ğŸ»ğŸ˜ğŸ˜ğŸ˜ğŸ¦‚\nğŸ†ğŸ†ğŸ†ğŸ¦‚ğŸ¥€ğŸ»ğŸ¥€\nğŸ»ğŸ¦‚ğŸ¦‚ğŸ¥€ğŸ¦‚ğŸ˜ğŸ¥€\nğŸ¦‚ğŸ˜ğŸ†ğŸ»ğŸ˜ğŸ»ğŸ†\nğŸ»ğŸ†ğŸ¥€ğŸ¦‚ğŸ˜ğŸ¥€ğŸ»\nğŸ¥€ğŸ†ğŸ¦‚ğŸ†ğŸ˜ğŸ»ğŸ¥€\nğŸ†ğŸ¥€ğŸ¦‚ğŸ†ğŸ˜ğŸ»ğŸ»\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14719, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:52:07 async_llm_engine.py:211] Added request chatcmpl-b9f541affe48489c91a8f197cd4ebe5a.
INFO 03-01 14:52:11 metrics.py:455] Avg prompt throughput: 56.1 tokens/s, Avg generation throughput: 202.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:52:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:52:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:52:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:52:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:52:34 async_llm_engine.py:179] Finished request chatcmpl-fd85c45cd4664aab9ac2e110efeef5b1.
INFO:     127.0.0.1:60868 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:52:34 logger.py:39] Received request chatcmpl-026b3ed5ebc34d3193fda9b3ac9a2059: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¦†ğŸ¦†ğŸ”¥ğŸ”¥ğŸ¦†ğŸ¦†ğŸ¦†\nğŸ”¥ğŸ¦†ğŸ¦†ğŸğŸ¦†ğŸ¦†ğŸ§\nğŸ”¥ğŸ¦†ğŸ§ğŸ§ğŸ”¥ğŸŒ‘ğŸ¦†\nğŸ¦†ğŸğŸğŸŒ‘ğŸğŸ¦†ğŸ”¥\nğŸ¦†ğŸ¦†ğŸ§ğŸŒ‘ğŸŒ‘ğŸ§ğŸ\nğŸğŸŒ‘ğŸ§ğŸğŸ”¥ğŸ¦†ğŸ¦†\nğŸŒ‘ğŸ§ğŸ§ğŸ”¥ğŸ”¥ğŸŒ‘ğŸ¦†\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:52:34 async_llm_engine.py:211] Added request chatcmpl-026b3ed5ebc34d3193fda9b3ac9a2059.
INFO 03-01 14:52:36 metrics.py:455] Avg prompt throughput: 58.0 tokens/s, Avg generation throughput: 202.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:52:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:52:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:52:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:52:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:53:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:53:03 async_llm_engine.py:179] Finished request chatcmpl-75eb24a77caf41d0abcaf2395f62ccea.
INFO:     127.0.0.1:43286 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:53:03 logger.py:39] Received request chatcmpl-bd88b8942ce64de0b99a63409aaf9ab9: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸğŸŒ’â˜€ï¸ğŸğŸ’¥ğŸ’¥ğŸŒ’\nğŸŒ’ğŸŒ’ğŸ’¥ğŸ¼ğŸğŸŒ’ğŸ’¥\nğŸâ˜€ï¸â˜€ï¸ğŸ’¥ğŸ¼ğŸ’¥ğŸ¼\nğŸ¼ğŸ’¥ğŸ’¥â˜€ï¸â˜€ï¸ğŸ¼â˜€ï¸\nğŸ’¥ğŸğŸâ˜€ï¸ğŸ¼ğŸ’¥ğŸ’¥\nâ˜€ï¸ğŸ¼ğŸğŸâ˜€ï¸ğŸ’¥ğŸ’¥\nâ˜€ï¸ğŸŒ’ğŸğŸ¼ğŸğŸ¼ğŸ\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14729, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:53:03 async_llm_engine.py:211] Added request chatcmpl-bd88b8942ce64de0b99a63409aaf9ab9.
INFO 03-01 14:53:06 metrics.py:455] Avg prompt throughput: 54.0 tokens/s, Avg generation throughput: 204.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:53:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:53:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:53:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:53:25 async_llm_engine.py:179] Finished request chatcmpl-7923d9303ee643478d30f9c11bbdac48.
INFO:     127.0.0.1:51500 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:53:26 logger.py:39] Received request chatcmpl-d7236f45c15f4c7cbc657a892cbe3812: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¦â›…ï¸ğŸŒ˜ğŸ¦­ğŸ¦­ğŸ¦ğŸŒ˜\nğŸ¦­ğŸŒ˜ğŸ’§ğŸ¦ğŸŒ˜ğŸŒ˜ğŸ¦\nğŸ’§ğŸ’§ğŸ¦­â›…ï¸ğŸ’§ğŸŒ˜ğŸ¦­\nğŸŒ˜ğŸ¦ğŸ’§â›…ï¸ğŸ¦­ğŸ’§ğŸŒ˜\nâ›…ï¸ğŸ’§ğŸŒ˜ğŸ’§ğŸ¦­â›…ï¸ğŸ¦\nâ›…ï¸â›…ï¸â›…ï¸ğŸ’§ğŸŒ˜ğŸ¦ğŸ¦\nâ›…ï¸ğŸŒ˜ğŸŒ˜ğŸ¦­ğŸ’§â›…ï¸ğŸ¦\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:53:26 async_llm_engine.py:211] Added request chatcmpl-d7236f45c15f4c7cbc657a892cbe3812.
INFO 03-01 14:53:26 metrics.py:455] Avg prompt throughput: 58.0 tokens/s, Avg generation throughput: 203.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:53:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 210.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:53:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 209.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:53:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:53:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:53:46 async_llm_engine.py:179] Finished request chatcmpl-5b829dbb123540ee8051b4b03fd9d8f0.
INFO:     127.0.0.1:58952 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:53:46 logger.py:39] Received request chatcmpl-b6e880fe6a9147a181cfdedbbb7e1fc2: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ˜ğŸŒ»ğŸ˜ğŸ¦‡ğŸŒ»ğŸŒ»ğŸŒ»\nğŸ˜ğŸ˜ğŸŒ»ğŸŒ»ğŸ¦‡ğŸ¦¦ğŸ¦‡\nğŸ¦¦ğŸŒ»ğŸ¦‡ğŸ˜ğŸ˜ğŸ˜ğŸ¦¦\nğŸ§ğŸŒ»ğŸŒ»ğŸ§ğŸ¦¦ğŸ§ğŸ¦‡\nğŸŒ»ğŸŒ»ğŸŒ»ğŸ¦¦ğŸ¦‡ğŸ¦‡ğŸ˜\nğŸ˜ğŸŒ»ğŸ§ğŸŒ»ğŸŒ»ğŸŒ»ğŸ¦¦\nğŸ˜ğŸ˜ğŸ¦‡ğŸ¦¦ğŸŒ»ğŸŒ»ğŸ¦¦\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14721, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:53:46 async_llm_engine.py:211] Added request chatcmpl-b6e880fe6a9147a181cfdedbbb7e1fc2.
INFO 03-01 14:53:51 metrics.py:455] Avg prompt throughput: 55.7 tokens/s, Avg generation throughput: 208.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:53:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 210.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:54:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 209.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:54:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 209.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:54:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:54:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:54:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:54:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:54:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:54:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:54:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:54:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:54:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 200.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:54:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:55:00 async_llm_engine.py:179] Finished request chatcmpl-10007e35e32f423297498a43516cbc52.
INFO:     127.0.0.1:56378 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:55:00 logger.py:39] Received request chatcmpl-678e3d0e60164bbd8f118b8a43a5d38b: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ«ğŸ¼ğŸ¼ğŸ¼ğŸºğŸ¦€ğŸº\nğŸ¼ğŸ¦€ğŸ«ğŸºğŸ¦€ğŸ¼ğŸ¦€\nğŸ¦–ğŸ¦€ğŸ¼ğŸ«ğŸ¦–ğŸºğŸ¦–\nğŸ«ğŸ¼ğŸºğŸ¼ğŸ¼ğŸ¦€ğŸ¼\nğŸ¦–ğŸ«ğŸ¼ğŸ«ğŸ¼ğŸ¼ğŸ¦€\nğŸºğŸ¦–ğŸ¦–ğŸ«ğŸºğŸ«ğŸ¦–\nğŸ¦€ğŸºğŸ¦€ğŸ¦€ğŸ¦€ğŸ¼ğŸ¼\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:55:00 async_llm_engine.py:211] Added request chatcmpl-678e3d0e60164bbd8f118b8a43a5d38b.
INFO 03-01 14:55:01 metrics.py:455] Avg prompt throughput: 57.9 tokens/s, Avg generation throughput: 199.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:55:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:55:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:55:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:55:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:55:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:55:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:55:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 200.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:55:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 198.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:55:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 198.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:55:51 async_llm_engine.py:179] Finished request chatcmpl-a92ee45db68b4a6196f4201bca200e9d.
INFO:     127.0.0.1:50906 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:55:51 logger.py:39] Received request chatcmpl-8814d2f1a7eb4c9881eb621d6d54cbf3: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¦ğŸ†ğŸ†ğŸ›ğŸ†ğŸ†ğŸŒŠ\nğŸ†ğŸ¦ğŸ¦ğŸŒŠğŸŒ™ğŸŒŠğŸ¦\nğŸŒŠğŸ†ğŸ¦ğŸŒ™ğŸ†ğŸ¦ğŸŒŠ\nğŸŒŠğŸ†ğŸ›ğŸŒ™ğŸŒ™ğŸŒŠğŸŒŠ\nğŸ›ğŸŒ™ğŸŒŠğŸŒ™ğŸŒŠğŸ›ğŸ†\nğŸŒŠğŸŒŠğŸŒ™ğŸŒŠğŸ†ğŸŒŠğŸ†\nğŸ¦ğŸ¦ğŸ›ğŸŒ™ğŸŒ™ğŸŒŠğŸ†\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:55:51 async_llm_engine.py:211] Added request chatcmpl-8814d2f1a7eb4c9881eb621d6d54cbf3.
INFO 03-01 14:55:51 metrics.py:455] Avg prompt throughput: 57.8 tokens/s, Avg generation throughput: 196.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:55:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:56:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:56:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 200.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:56:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 199.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:56:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 198.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:56:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 197.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:56:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 196.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:56:28 async_llm_engine.py:179] Finished request chatcmpl-b9f541affe48489c91a8f197cd4ebe5a.
INFO:     127.0.0.1:55276 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:56:28 logger.py:39] Received request chatcmpl-28d2b98776974c3c89c97d98579b4ed7: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸŒ•ğŸ¦­ğŸ’«ğŸŒ•ğŸ¦­ğŸ’«ğŸ’«\nğŸŒ•ğŸ¦ğŸ¦ğŸ¦®ğŸŒ•ğŸ’«ğŸ’«\nğŸ¦ğŸŒ•ğŸ¦®ğŸ¦ğŸŒ•ğŸŒ•ğŸ’«\nğŸ’«ğŸ¦ğŸŒ•ğŸ¦­ğŸ¦®ğŸ¦ğŸ’«\nğŸ’«ğŸ¦­ğŸ’«ğŸ¦®ğŸ¦®ğŸ¦­ğŸ¦\nğŸ¦®ğŸ¦ğŸ¦®ğŸ¦ğŸ¦­ğŸ¦ğŸ¦­\nğŸ¦®ğŸ¦®ğŸŒ•ğŸ¦ğŸŒ•ğŸ¦­ğŸ’«\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14721, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:56:28 async_llm_engine.py:211] Added request chatcmpl-28d2b98776974c3c89c97d98579b4ed7.
INFO 03-01 14:56:29 async_llm_engine.py:179] Finished request chatcmpl-026b3ed5ebc34d3193fda9b3ac9a2059.
INFO:     127.0.0.1:38878 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:56:29 logger.py:39] Received request chatcmpl-89479a99958b4c5eab6b1883c28cc0a3: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¦†ğŸ¦™ğŸ¦†ğŸ´ğŸŒ’ğŸ´ğŸ¦†\nğŸ¦†ğŸ´ğŸŒ’ğŸ¦†ğŸŒ’ğŸ´ğŸŒ’\nğŸŒ’ğŸ¦™ğŸ¦†ğŸŒ’ğŸ¦†ğŸ¦™ğŸŒ’\nğŸŒ’ğŸğŸ¦†ğŸŒ’ğŸğŸ¦†ğŸ\nğŸ´ğŸŒ’ğŸ¦™ğŸ¦†ğŸŒ’ğŸ¦†ğŸ\nğŸ´ğŸŒ’ğŸğŸ´ğŸ¦™ğŸ¦™ğŸ¦†\nğŸ¦™ğŸ´ğŸ¦™ğŸŒ’ğŸ¦†ğŸ´ğŸ¦™\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14723, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:56:29 async_llm_engine.py:211] Added request chatcmpl-89479a99958b4c5eab6b1883c28cc0a3.
INFO 03-01 14:56:31 metrics.py:455] Avg prompt throughput: 110.9 tokens/s, Avg generation throughput: 196.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:56:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:56:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:56:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 167.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:56:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 168.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:56:56 async_llm_engine.py:179] Finished request chatcmpl-bd88b8942ce64de0b99a63409aaf9ab9.
INFO:     127.0.0.1:47024 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:56:56 logger.py:39] Received request chatcmpl-53e365e39f3c49b7ba24766c032b9419: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ–ğŸ¨ğŸ­ğŸ­ğŸ¥°ğŸ¦ğŸ¥°\nğŸ¨ğŸ¥°ğŸ¨ğŸ–ğŸ¨ğŸ¦ğŸ¦\nğŸ¦ğŸ¥°ğŸ¦ğŸ­ğŸ¦ğŸ­ğŸ–\nğŸ¨ğŸ¦ğŸ­ğŸ¨ğŸ¥°ğŸ­ğŸ¦\nğŸ¨ğŸ¥°ğŸ¨ğŸ­ğŸ–ğŸ¨ğŸ¦\nğŸ¦ğŸ­ğŸ­ğŸ¦ğŸ¥°ğŸ­ğŸ¦\nğŸ¥°ğŸ¨ğŸ–ğŸ­ğŸ¨ğŸ¦ğŸ¨\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:56:56 async_llm_engine.py:211] Added request chatcmpl-53e365e39f3c49b7ba24766c032b9419.
INFO 03-01 14:56:56 metrics.py:455] Avg prompt throughput: 57.8 tokens/s, Avg generation throughput: 165.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:57:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 169.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:57:04 async_llm_engine.py:179] Finished request chatcmpl-6f7536f1e2424c2aba67c1976b124ae8.
INFO:     127.0.0.1:36842 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:57:04 logger.py:39] Received request chatcmpl-1b4fc188e14442dbbe3cfe651042b043: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nâ„ï¸ğŸªğŸ¤šâ˜€ï¸ğŸªğŸ¤šâ„ï¸\nğŸªğŸªğŸªğŸ®â˜€ï¸â˜€ï¸ğŸª\nâ˜€ï¸ğŸ¤šâ„ï¸â„ï¸ğŸªğŸ¤šğŸ¤š\nâ„ï¸ğŸ¤šâ˜€ï¸ğŸ¤šğŸ¤šğŸ®ğŸ¤š\nğŸªâ˜€ï¸â˜€ï¸â˜€ï¸ğŸ¤šâ„ï¸â˜€ï¸\nğŸ¤šğŸ®â˜€ï¸â„ï¸â„ï¸ğŸ¤šğŸ®\nâ˜€ï¸â˜€ï¸ğŸ®â„ï¸ğŸ®ğŸ¤šâ„ï¸\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:57:04 async_llm_engine.py:211] Added request chatcmpl-1b4fc188e14442dbbe3cfe651042b043.
INFO 03-01 14:57:06 metrics.py:455] Avg prompt throughput: 58.0 tokens/s, Avg generation throughput: 186.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:57:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 209.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:57:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:57:20 async_llm_engine.py:179] Finished request chatcmpl-b6e880fe6a9147a181cfdedbbb7e1fc2.
INFO:     127.0.0.1:54690 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:57:20 logger.py:39] Received request chatcmpl-2e63acdb40f64d139b60698ab9c520d5: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ°ğŸµğŸŒ³ğŸ“ğŸŒ±ğŸ“ğŸµ\nğŸµğŸµğŸµğŸ“ğŸµğŸ°ğŸ“\nğŸ°ğŸŒ±ğŸ“ğŸŒ³ğŸ“ğŸ°ğŸŒ±\nğŸ°ğŸ“ğŸŒ±ğŸŒ³ğŸ°ğŸµğŸ“\nğŸ“ğŸ“ğŸŒ³ğŸŒ±ğŸ°ğŸŒ³ğŸµ\nğŸŒ³ğŸŒ±ğŸ°ğŸŒ³ğŸŒ³ğŸ°ğŸµ\nğŸŒ±ğŸµğŸ°ğŸ°ğŸ°ğŸµğŸµ\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:57:20 async_llm_engine.py:211] Added request chatcmpl-2e63acdb40f64d139b60698ab9c520d5.
INFO 03-01 14:57:22 metrics.py:455] Avg prompt throughput: 57.8 tokens/s, Avg generation throughput: 202.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:57:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 209.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:57:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:57:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 209.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:57:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:57:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:57:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:57:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:58:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:58:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:58:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:58:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:58:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:58:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:58:30 async_llm_engine.py:179] Finished request chatcmpl-d7236f45c15f4c7cbc657a892cbe3812.
INFO:     127.0.0.1:40364 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:58:30 logger.py:39] Received request chatcmpl-d47a1335dffe472f96d7b0502c2e59e6: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ‹ğŸ„ğŸ•·ğŸ¦ğŸ¦©ğŸ¦©ğŸ‹\nğŸ¦ğŸ¦©ğŸ¦ğŸ¦©ğŸ¦ğŸ¦ğŸ„\nğŸ¦©ğŸ„ğŸ¦ğŸ„ğŸ•·ğŸ•·ğŸ¦©\nğŸ¦©ğŸ¦ğŸ„ğŸ„ğŸ¦©ğŸ•·ğŸ¦©\nğŸ•·ğŸ¦ğŸ¦ğŸ‹ğŸ•·ğŸ•·ğŸ„\nğŸ•·ğŸ„ğŸ¦©ğŸ¦©ğŸ„ğŸ¦©ğŸ„\nğŸ‹ğŸ•·ğŸ„ğŸ¦ğŸ‹ğŸ¦©ğŸ¦©\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:58:30 async_llm_engine.py:211] Added request chatcmpl-d47a1335dffe472f96d7b0502c2e59e6.
INFO 03-01 14:58:32 metrics.py:455] Avg prompt throughput: 57.9 tokens/s, Avg generation throughput: 199.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:58:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:58:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:58:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:58:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:58:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:59:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:59:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:59:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:59:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 200.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:59:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 200.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:59:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 200.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:59:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 199.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:59:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 198.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:59:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 197.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:59:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 197.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:59:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 196.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:59:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 195.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%.
INFO 03-01 15:00:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 195.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%.
INFO 03-01 15:00:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 195.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%.
INFO 03-01 15:00:11 async_llm_engine.py:179] Finished request chatcmpl-8814d2f1a7eb4c9881eb621d6d54cbf3.
INFO:     127.0.0.1:42230 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:00:11 logger.py:39] Received request chatcmpl-384876f4b95c4bc9ac8752a888d7ad59: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸŒğŸŒŸğŸŒŸğŸŒğŸŒğŸ§½ğŸ\nğŸ ğŸŒŸğŸğŸŒŸğŸ ğŸ§½ğŸ\nğŸŒŸğŸ ğŸŒğŸğŸŒğŸŒğŸŒŸ\nğŸ ğŸ ğŸğŸ§½ğŸğŸŒğŸŒ\nğŸŒŸğŸŒŸğŸŒğŸŒŸğŸ ğŸŒğŸŒ\nğŸ§½ğŸŒŸğŸ ğŸŒŸğŸ§½ğŸ§½ğŸ \nğŸğŸŒŸğŸ ğŸŒŸğŸ ğŸ ğŸ\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 15:00:11 async_llm_engine.py:211] Added request chatcmpl-384876f4b95c4bc9ac8752a888d7ad59.
INFO 03-01 15:00:12 metrics.py:455] Avg prompt throughput: 58.0 tokens/s, Avg generation throughput: 191.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 15:00:12 async_llm_engine.py:179] Finished request chatcmpl-678e3d0e60164bbd8f118b8a43a5d38b.
INFO:     127.0.0.1:59058 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:00:12 logger.py:39] Received request chatcmpl-b3145dd291d241af8a116256d509b4ed: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¦©ğŸ¦ğŸ¦©ğŸ¦ğŸ›ğŸ¦©ğŸ¦¢\nğŸğŸ¦¢ğŸ›ğŸğŸğŸ¦ğŸ\nğŸ¦ğŸ¦©ğŸ¦©ğŸ¦¢ğŸ¦¢ğŸ¦©ğŸ¦¢\nğŸ›ğŸ¦ğŸğŸ¦¢ğŸ¦ğŸ›ğŸ\nğŸ¦©ğŸ¦¢ğŸ¦¢ğŸ¦¢ğŸğŸğŸ¦¢\nğŸ¦ğŸ¦ğŸğŸ¦©ğŸ¦¢ğŸ¦©ğŸ¦©\nğŸğŸ›ğŸ¦¢ğŸ¦ğŸ›ğŸğŸ¦¢\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 15:00:12 async_llm_engine.py:211] Added request chatcmpl-b3145dd291d241af8a116256d509b4ed.
INFO 03-01 15:00:17 metrics.py:455] Avg prompt throughput: 57.9 tokens/s, Avg generation throughput: 199.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:00:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:00:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:00:29 async_llm_engine.py:179] Finished request chatcmpl-89479a99958b4c5eab6b1883c28cc0a3.
INFO:     127.0.0.1:38476 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:00:29 logger.py:39] Received request chatcmpl-091c294d8700403ca088c0a3f4624eef: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ„ğŸ§ğŸ¦‘ğŸ§ğŸ¦‘ğŸ ğŸ§\nğŸ§ğŸ§ğŸ„ğŸ„ğŸ¦‘ğŸ¦‘ğŸ§\nğŸ¦‘ğŸ„ğŸ¦‘ğŸ ğŸ„ğŸ ğŸ¦‘\nğŸ†ğŸ§ğŸ„ğŸ ğŸ†ğŸ†ğŸ„\nğŸ„ğŸ†ğŸ§ğŸ§ğŸ ğŸ§ğŸ†\nğŸ†ğŸ¦‘ğŸ†ğŸ§ğŸ ğŸ¦‘ğŸ§\nğŸ ğŸ§ğŸ¦‘ğŸ†ğŸ¦‘ğŸ†ğŸ¦‘\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 15:00:29 async_llm_engine.py:211] Added request chatcmpl-091c294d8700403ca088c0a3f4624eef.
INFO 03-01 15:00:32 metrics.py:455] Avg prompt throughput: 57.8 tokens/s, Avg generation throughput: 201.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 15:00:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 15:00:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:00:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:00:48 async_llm_engine.py:179] Finished request chatcmpl-28d2b98776974c3c89c97d98579b4ed7.
INFO:     127.0.0.1:38464 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:00:48 logger.py:39] Received request chatcmpl-039441ffaab249a698f69db2132b2098: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¤šğŸŒ¾ğŸ¦ƒğŸŒ¾ğŸ¦ƒğŸ¤šğŸ¦ƒ\nğŸŒ¾ğŸ¦ƒğŸ¦‡ğŸ¦ƒğŸ¦ƒğŸŒ¾ğŸŒ¾\nğŸ˜ğŸ¤šğŸ¤šğŸ¤šğŸŒ¾ğŸ˜ğŸ¤š\nğŸ˜ğŸ¦ƒğŸŒ¾ğŸ˜ğŸŒ¾ğŸ¦ƒğŸ¦ƒ\nğŸ¦‡ğŸ¦ƒğŸ˜ğŸŒ¾ğŸ¤šğŸ¤šğŸ¦ƒ\nğŸ¤šğŸ˜ğŸ¤šğŸ˜ğŸ¦‡ğŸ¦‡ğŸ¤š\nğŸ¦ƒğŸ¦ƒğŸ¤šğŸ¦ƒğŸ¤šğŸ¤šğŸ¦ƒ\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 15:00:48 async_llm_engine.py:211] Added request chatcmpl-039441ffaab249a698f69db2132b2098.
INFO 03-01 15:00:52 metrics.py:455] Avg prompt throughput: 57.8 tokens/s, Avg generation throughput: 202.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 15:00:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 15:01:02 async_llm_engine.py:179] Finished request chatcmpl-1b4fc188e14442dbbe3cfe651042b043.
INFO:     127.0.0.1:36754 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:01:02 logger.py:39] Received request chatcmpl-0fb081551c8d4312ae2453a5ad5b02e4: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¦ğŸŒğŸŒ˜ğŸŒµğŸŒğŸŒµğŸ\nğŸŒµğŸŒµğŸŒğŸğŸŒ˜ğŸ¦ğŸ\nğŸ¦ğŸŒ˜ğŸŒ˜ğŸŒµğŸŒ˜ğŸŒ˜ğŸ\nğŸŒğŸŒµğŸğŸŒµğŸğŸŒµğŸŒ˜\nğŸŒ˜ğŸŒµğŸŒ˜ğŸŒµğŸŒ˜ğŸ¦ğŸŒµ\nğŸ¦ğŸ¦ğŸŒğŸŒğŸŒğŸŒ˜ğŸŒ\nğŸŒµğŸŒµğŸ¦ğŸŒµğŸŒ˜ğŸŒ˜ğŸ¦\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 15:01:02 async_llm_engine.py:211] Added request chatcmpl-0fb081551c8d4312ae2453a5ad5b02e4.
INFO 03-01 15:01:02 metrics.py:455] Avg prompt throughput: 57.8 tokens/s, Avg generation throughput: 204.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:01:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 210.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 15:01:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 15:01:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 15:01:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 15:01:26 async_llm_engine.py:179] Finished request chatcmpl-2e63acdb40f64d139b60698ab9c520d5.
INFO:     127.0.0.1:55048 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:01:27 logger.py:39] Received request chatcmpl-38192a88133b47a982a6f0a29fbabc64: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸŒ¼ğŸŒ¼ğŸ…ğŸ…ğŸ³ğŸ…ğŸ…\nğŸŒ¼ğŸ…ğŸŒ¼ğŸ…ğŸ¨ğŸ…ğŸŒ²\nğŸŒ¼ğŸŒ²ğŸŒ²ğŸ³ğŸ…ğŸ¨ğŸ³\nğŸŒ²ğŸ…ğŸ…ğŸŒ²ğŸŒ¼ğŸŒ¼ğŸŒ¼\nğŸŒ¼ğŸ…ğŸ³ğŸ…ğŸ…ğŸŒ¼ğŸŒ¼\nğŸ³ğŸ¨ğŸ¨ğŸ¨ğŸŒ¼ğŸ¨ğŸ…\nğŸ³ğŸŒ¼ğŸŒ²ğŸŒ¼ğŸŒ²ğŸ…ğŸŒ²\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 15:01:27 async_llm_engine.py:211] Added request chatcmpl-38192a88133b47a982a6f0a29fbabc64.
INFO 03-01 15:01:27 metrics.py:455] Avg prompt throughput: 58.0 tokens/s, Avg generation throughput: 204.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-01 15:01:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:01:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 15:01:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 210.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 15:01:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 210.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 15:01:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 15:01:54 async_llm_engine.py:179] Finished request chatcmpl-53e365e39f3c49b7ba24766c032b9419.
INFO:     127.0.0.1:36748 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:01:54 logger.py:39] Received request chatcmpl-9a732a117a8842198a81a4accadc5153: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¦ğŸ¿ğŸ¦ğŸ¦ğŸ¼ğŸ¼ğŸ¼\nğŸš€ğŸ¼ğŸ¼ğŸ¿ğŸ¦ğŸš€ğŸ¦\nğŸš€ğŸ¼âœ¨âœ¨ğŸš€ğŸ¿âœ¨\nâœ¨ğŸ¼âœ¨ğŸ¦ğŸ¦ğŸ¼âœ¨\nğŸ¿ğŸš€ğŸ¦âœ¨ğŸš€ğŸ¿ğŸ¼\nğŸ¦ğŸš€ğŸ¼ğŸ¦ğŸš€âœ¨âœ¨\nğŸ¿âœ¨ğŸš€ğŸš€ğŸ¿ğŸ¼ğŸ¦\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14720, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 15:01:54 async_llm_engine.py:211] Added request chatcmpl-9a732a117a8842198a81a4accadc5153.
INFO 03-01 15:01:57 metrics.py:455] Avg prompt throughput: 55.9 tokens/s, Avg generation throughput: 209.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:02:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 214.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-01 15:02:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:02:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 15:02:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 210.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 15:02:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 15:02:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 15:02:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 15:02:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 15:02:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 15:02:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:02:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:02:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:03:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:03:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 15:03:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 15:03:16 async_llm_engine.py:179] Finished request chatcmpl-d47a1335dffe472f96d7b0502c2e59e6.
INFO:     127.0.0.1:44558 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:03:16 logger.py:39] Received request chatcmpl-21849fc12c064b16bca434820b0ad068: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ‚ğŸŒ¿ğŸŒ¿ğŸ‚ğŸŒ“ğŸŒ“ğŸ‚\nğŸ©ğŸŒ“ğŸ©ğŸ©ğŸ©ğŸŒ¿ğŸŒ“\nğŸ‚ğŸ©ğŸŒ¿ğŸ‚ğŸŒ“ğŸ°ğŸ°\nğŸ‚ğŸ‚ğŸŒ¿ğŸŒ“ğŸ°ğŸ©ğŸ‚\nğŸŒ¿ğŸŒ¿ğŸ‚ğŸŒ¿ğŸ‚ğŸ‚ğŸ°\nğŸ°ğŸ©ğŸ°ğŸŒ¿ğŸ©ğŸŒ¿ğŸ‚\nğŸŒ“ğŸ‚ğŸ‚ğŸŒ¿ğŸ©ğŸ°ğŸŒ¿\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14717, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 15:03:16 async_llm_engine.py:211] Added request chatcmpl-21849fc12c064b16bca434820b0ad068.
INFO 03-01 15:03:17 metrics.py:455] Avg prompt throughput: 56.6 tokens/s, Avg generation throughput: 197.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 15:03:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:03:23 async_llm_engine.py:179] Finished request chatcmpl-384876f4b95c4bc9ac8752a888d7ad59.
INFO:     127.0.0.1:40286 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:03:23 logger.py:39] Received request chatcmpl-b6c89cc0a3b845c49ef8156269f81393: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ¦†ğŸŒ–ğŸƒğŸŒ”ğŸƒğŸƒğŸŒ’\nğŸŒ”ğŸƒğŸƒğŸŒ–ğŸŒ–ğŸŒ–ğŸƒ\nğŸŒ”ğŸƒğŸŒ’ğŸŒ’ğŸŒ”ğŸŒ’ğŸƒ\nğŸŒ”ğŸƒğŸƒğŸŒ’ğŸŒ–ğŸ¦†ğŸ¦†\nğŸ¦†ğŸŒ’ğŸƒğŸ¦†ğŸ¦†ğŸŒ’ğŸŒ–\nğŸ¦†ğŸƒğŸ¦†ğŸŒ”ğŸƒğŸƒğŸŒ”\nğŸŒ–ğŸ¦†ğŸŒ’ğŸ¦†ğŸŒ’ğŸ¦†ğŸŒ–\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14719, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 15:03:23 async_llm_engine.py:211] Added request chatcmpl-b6c89cc0a3b845c49ef8156269f81393.
INFO 03-01 15:03:27 metrics.py:455] Avg prompt throughput: 56.0 tokens/s, Avg generation throughput: 204.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 15:03:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 209.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 15:03:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 15:03:41 async_llm_engine.py:179] Finished request chatcmpl-38192a88133b47a982a6f0a29fbabc64.
INFO:     127.0.0.1:60768 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:03:41 logger.py:39] Received request chatcmpl-4a755ef82ebb425ab98c12ed07217d41: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ„ğŸ„ğŸ¦ŠğŸ¦ŠğŸ„ğŸğŸ§\nğŸ¦ŠğŸ„ğŸ„ğŸğŸ„ğŸ„ğŸ„\nğŸ„ğŸ§ğŸ¦ŠğŸğŸğŸ¦ŠğŸ§\nğŸ§ğŸ„ğŸ„ğŸ„ğŸ§ğŸ„ğŸ„\nğŸ¦ŠğŸ§ğŸ„ğŸ„ğŸğŸ¦ŠğŸ„\nğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸğŸ§\nğŸğŸ„ğŸ„ğŸ„ğŸ§ğŸ„ğŸ§\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 15:03:41 async_llm_engine.py:211] Added request chatcmpl-4a755ef82ebb425ab98c12ed07217d41.
INFO 03-01 15:03:42 metrics.py:455] Avg prompt throughput: 57.8 tokens/s, Avg generation throughput: 202.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 15:03:47 async_llm_engine.py:179] Finished request chatcmpl-039441ffaab249a698f69db2132b2098.
INFO:     127.0.0.1:57022 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:03:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV caINFO 03-01 15:03:50 async_llm_engine.py:179INFO 03-01 15:03:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 184.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:03:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 182.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 15:04:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 183.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 15:04:03 async_llm_engine.py:179] Finished request chatcmpl-0fb081551c8d4312ae2453a5ad5b02e4.
INFO:     127.0.0.1:42700 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:04:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 161.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-01 15:04:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 158.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-01 15:04:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:04:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 158.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:04:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 158.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 15:04:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 158.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 15:04:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 15:04:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 155.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 15:04:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 154.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 15:04:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 155.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 15:04:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 155.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 15:05:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 154.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 15:05:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 154.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 15:05:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 153.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:05:17 async_llm_engine.py:179] Finished request chatcmpl-b3145dd291d241af8a116256d509b4ed.
INFO:     127.0.0.1:40298 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:05:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 152.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 15:05:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 133.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 15:05:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 133.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 15:05:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 132.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 15:05:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 111.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 15:05:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 15:05:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 15:05:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 15:05:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 15:06:00 async_llm_engine.py:179] Finished request chatcmpl-091c294d8700403ca088c0a3f4624eef.
INFO:     127.0.0.1:37464 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:06:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.0 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:06:06 async_llm_engine.py:179] Finished request chatcmpl-9a732a117a8842198a81a4accadc5153.
INFO:     127.0.0.1:42646 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:06:08 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 103.7 tokINFO 03-01 15:06:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokensINFO 03-01 15:06:13 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 85.2 toINFO 03-01 15:06:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/INFO 03-01 15:06:18 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 84.5 tINFO 03-01 15:06:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/sINFO 03-01 15:06:23 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 84.2 INFO 03-01 15:06:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s,INFO 03-01 15:06:28 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 84.2INFO 03-01 15:06:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.4 tokens/s, INFO 03-01 15:06:33 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 83.INFO 03-01 15:06:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 4.7 tokens/s, RINFO 03-01 15:06:36 async_llm_engine.py:179] Finished request chatcmpl-21849fc12c064b16bca434820b0ad06INFO 03-01 15:06:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-01 15:06:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-01 15:06:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-01 15:06:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-01 15:07:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:07:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:07:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:07:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:07:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:07:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:07:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:07:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:07:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:07:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:07:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:07:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:08:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:08:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:08:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:08:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:08:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:08:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:08:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:08:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:08:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:08:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:08:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:08:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:09:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:09:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:09:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:09:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:09:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:09:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:09:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:09:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:09:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:09:48 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:09:53 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:09:58 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:10:03 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:10:08 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:10:13 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:10:18 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:10:23 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:10:28 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:10:33 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:10:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:10:43 async_llm_engine.py:223] Aborted request chatcmpl-a0fc1765c1904c3b8e9000feee2df3c9.
INFO 03-01 15:10:43 logger.py:39] Received request chatcmpl-8ec2924d93944fc78e73f30fe80d626e: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answINFO 03-01 15:10:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 03-01 15:10:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 03-01 15:10:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 03-01 15:11:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 03-01 15:11:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 13.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 03-01 15:11:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 13.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 03-01 15:11:12 async_llm_engine.py:179] Finished request chatcmpl-b6c89cc0a3b845c49ef8156269f81393.
INFO:     127.0.0.1:39302 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:11:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:11:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:11:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:11:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:11:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:11:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:11:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:11:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:11:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:12:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:12:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:12:13 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:12:18 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:12:23 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:12:28 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:12:33 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:12:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:12:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:12:48 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:12:53 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:12:58 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:13:03 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:13:08 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:13:13 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:13:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:13:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:13:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:13:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:13:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:13:41 async_llm_engine.py:223] Aborted request chatcmpl-4a755ef82ebb425ab98c12ed07217d41.
INFO 03-01 15:13:41 logger.py:39] Received request chatcmpl-ca4c51b126ea48dd92c0b88600adc8eb: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\nğŸ„ğŸ„ğŸ¦ŠğŸ¦ŠğŸ„ğŸğŸ§\nğŸ¦ŠğŸ„ğŸ„ğŸğŸ„ğŸ„ğŸ„\nğŸ„ğŸ§ğŸ¦ŠğŸğŸğŸ¦ŠğŸ§\nğŸ§ğŸ„ğŸ„ğŸ„ğŸ§ğŸ„ğŸ„\nğŸ¦ŠğŸ§ğŸ„ğŸ„ğŸğŸ¦ŠğŸ„\nğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸğŸ§\nğŸğŸ„ğŸ„ğŸ„ğŸ§ğŸ„ğŸ§\n\n\n<ï½œAssistantï½œ>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_INFO 03-01 15:13:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:13:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:13:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:14:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:14:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:14:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:14:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:14:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:14:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:14:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:14:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:14:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:14:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:14:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:14:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:15:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:15:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:15:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:15:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:15:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:15:28 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:15:33 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:15:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:15:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:15:48 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:15:53 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:15:58 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:16:03 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:16:08 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:16:13 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:16:18 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:16:23 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:16:28 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:16:33 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:16:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:16:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:16:48 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:16:53 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:16:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:17:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:17:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:17:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:17:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:17:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:17:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:17:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:17:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:17:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:17:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:17:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:18:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:18:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:18:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:18:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:18:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:18:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:18:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:18:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:18:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:18:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:18:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:18:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:19:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:19:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:19:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:19:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:19:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:19:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:19:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:19:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:19:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:19:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:19:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:19:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:20:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:20:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:20:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:20:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:20:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:20:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:20:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:20:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:20:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:20:44 async_llm_engine.py:223] Aborted request chatcmpl-8ec2924d93944fc78e73f30fe80d626e.
INFO 03-01 15:20:44 logger.py:39] Received request chatcmpl-ae12a2b0cce94754bf0be6621565ded7: prompt: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>\nYou are a good game problem-solver, I'll give you a question.\nINFO 03-01 15:20:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:20:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:20:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:21:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:21:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:21:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:21:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:21:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:21:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:21:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:21:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:21:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:21:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:21:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:21:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:22:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:22:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:22:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:22:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:22:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:22:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:22:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:22:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:22:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:22:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:22:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:22:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:23:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:23:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:23:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:23:12 async_llm_engine.py:179] Finished request chatcmpl-ca4c51b126ea48dd92c0b88600adc8eb.
INFO:     127.0.0.1:36824 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:23:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.6 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:23:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:23:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:23:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:23:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:23:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:23:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:23:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:23:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:23:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:23:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:23:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:23:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:24:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:24:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:24:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:24:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:24:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:24:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:24:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:24:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:24:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:24:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:24:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:24:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:25:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:25:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:25:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:25:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:25:23 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:25:28 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:25:33 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:25:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:25:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:25:48 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:25:53 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:25:58 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:26:03 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:26:08 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:26:13 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:26:18 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:26:23 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:26:28 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:26:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 4.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:26:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:26:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:26:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:26:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:26:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:27:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:27:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:27:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:27:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:27:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:27:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 16.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:27:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:27:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:27:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:27:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:27:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:28:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:28:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:28:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:28:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:28:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:28:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:28:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:28:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:28:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:28:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:28:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:28:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:29:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:29:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:29:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:29:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:29:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:29:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:29:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:29:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:29:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:29:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:29:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:29:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:30:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 13.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:30:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:30:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:30:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:30:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:30:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:30:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:30:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:30:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:30:45 async_llm_engine.py:223] Aborted request chatcmpl-ae12a2b0cce94754bf0be6621565ded7.
INFO 03-01 15:30:53 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.1 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
