INFO 03-01 14:33:29 __init__.py:207] Automatically detected platform cuda.
INFO 03-01 14:33:29 api_server.py:912] vLLM API server version 0.7.3
INFO 03-01 14:33:29 api_server.py:913] args: Namespace(host=None, port=9003, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='/map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Llama-70B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=15000, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=2, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.95, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['DeepSeek-R1-Distill-Llama-70B'], qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False)
INFO 03-01 14:33:36 config.py:549] This model supports multiple tasks: {'score', 'generate', 'reward', 'embed', 'classify'}. Defaulting to 'generate'.
INFO 03-01 14:33:36 config.py:1382] Defaulting to use mp for distributed inference
WARNING 03-01 14:33:36 config.py:676] Async output processing can not be enabled with pipeline parallel
INFO 03-01 14:33:36 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Llama-70B', speculative_config=None, tokenizer='/map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Llama-70B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=15000, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=DeepSeek-R1-Distill-Llama-70B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
WARNING 03-01 14:33:36 multiproc_worker_utils.py:300] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 03-01 14:33:36 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-01 14:33:36 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-01 14:33:36 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-01 14:33:36 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-01 14:33:36 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-01 14:33:36 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-01 14:33:36 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-01 14:33:36 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-01 14:33:37 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-01 14:33:37 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-01 14:33:37 cuda.py:229] Using Flash Attention backend.
INFO 03-01 14:33:37 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-01 14:33:37 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-01 14:33:37 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-01 14:33:38 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-01 14:33:38 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-01 14:33:40 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-01 14:33:40 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-01 14:33:40 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-01 14:33:40 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-01 14:33:40 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-01 14:33:40 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-01 14:33:40 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-01 14:33:40 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-01 14:33:40 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-01 14:33:40 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-01 14:33:40 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-01 14:33:40 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-01 14:33:40 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 03-01 14:33:40 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-01 14:33:40 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 03-01 14:33:40 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 03-01 14:33:41 custom_all_reduce_utils.py:206] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
INFO 03-01 14:34:04 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-01 14:34:04 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-01 14:34:04 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-01 14:34:04 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-01 14:34:04 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-01 14:34:04 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-01 14:34:04 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-01 14:34:04 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-01 14:34:04 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_53354d39'), local_subscribe_port=53797, remote_subscribe_port=None)
INFO 03-01 14:34:04 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_ba0f745a'), local_subscribe_port=38195, remote_subscribe_port=None)
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-01 14:34:04 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-01 14:34:04 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-01 14:34:04 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-01 14:34:04 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-01 14:34:04 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-01 14:34:04 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-01 14:34:04 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-01 14:34:04 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-01 14:34:04 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-01 14:34:04 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-01 14:34:04 utils.py:916] Found nccl from library libnccl.so.2
INFO 03-01 14:34:04 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-01 14:34:04 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-01 14:34:04 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-01 14:34:04 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 03-01 14:34:04 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 03-01 14:34:04 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Llama-70B...
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-01 14:34:04 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Llama-70B...
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-01 14:34:04 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Llama-70B...
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-01 14:34:04 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Llama-70B...
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-01 14:34:04 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Llama-70B...
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-01 14:34:04 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Llama-70B...
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-01 14:34:04 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Llama-70B...
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-01 14:34:04 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Llama-70B...
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-01 14:35:34 model_runner.py:1115] Loading model weights took 16.4603 GB
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-01 14:35:34 model_runner.py:1115] Loading model weights took 16.4603 GB
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-01 14:35:34 model_runner.py:1115] Loading model weights took 16.4603 GB
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-01 14:35:34 model_runner.py:1115] Loading model weights took 16.4603 GB
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-01 14:35:36 model_runner.py:1115] Loading model weights took 16.4603 GB
INFO 03-01 14:35:36 model_runner.py:1115] Loading model weights took 16.4603 GB
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-01 14:35:36 model_runner.py:1115] Loading model weights took 16.4603 GB
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-01 14:35:36 model_runner.py:1115] Loading model weights took 16.4603 GB
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-01 14:35:39 worker.py:267] Memory profiling takes 2.59 seconds
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-01 14:35:39 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-01 14:35:39 worker.py:267] model weights take 16.46GiB; non_torch_memory takes 1.70GiB; PyTorch activation peak memory takes 1.58GiB; the rest of the memory reserved for KV Cache is 55.64GiB.
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-01 14:35:39 worker.py:267] Memory profiling takes 2.60 seconds
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-01 14:35:39 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-01 14:35:39 worker.py:267] model weights take 16.46GiB; non_torch_memory takes 1.51GiB; PyTorch activation peak memory takes 1.58GiB; the rest of the memory reserved for KV Cache is 55.83GiB.
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-01 14:35:39 worker.py:267] Memory profiling takes 2.64 seconds
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-01 14:35:39 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-01 14:35:39 worker.py:267] model weights take 16.46GiB; non_torch_memory takes 1.70GiB; PyTorch activation peak memory takes 1.58GiB; the rest of the memory reserved for KV Cache is 55.64GiB.
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-01 14:35:39 worker.py:267] Memory profiling takes 2.70 seconds
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-01 14:35:39 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-01 14:35:39 worker.py:267] model weights take 16.46GiB; non_torch_memory takes 1.70GiB; PyTorch activation peak memory takes 1.86GiB; the rest of the memory reserved for KV Cache is 55.36GiB.
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-01 14:35:42 worker.py:267] Memory profiling takes 5.54 seconds
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-01 14:35:42 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-01 14:35:42 worker.py:267] model weights take 16.46GiB; non_torch_memory takes 1.03GiB; PyTorch activation peak memory takes 1.35GiB; the rest of the memory reserved for KV Cache is 56.54GiB.
INFO 03-01 14:35:42 worker.py:267] Memory profiling takes 5.59 seconds
INFO 03-01 14:35:42 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
INFO 03-01 14:35:42 worker.py:267] model weights take 16.46GiB; non_torch_memory takes 0.94GiB; PyTorch activation peak memory takes 1.35GiB; the rest of the memory reserved for KV Cache is 56.63GiB.
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-01 14:35:42 worker.py:267] Memory profiling takes 5.59 seconds
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-01 14:35:42 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-01 14:35:42 worker.py:267] model weights take 16.46GiB; non_torch_memory takes 1.03GiB; PyTorch activation peak memory takes 1.35GiB; the rest of the memory reserved for KV Cache is 56.54GiB.
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-01 14:35:42 worker.py:267] Memory profiling takes 5.59 seconds
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-01 14:35:42 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-01 14:35:42 worker.py:267] model weights take 16.46GiB; non_torch_memory takes 0.94GiB; PyTorch activation peak memory takes 1.35GiB; the rest of the memory reserved for KV Cache is 56.63GiB.
INFO 03-01 14:35:42 executor_base.py:111] # cuda blocks: 90709, # CPU blocks: 6553
INFO 03-01 14:35:42 executor_base.py:116] Maximum concurrency for 15000 tokens per request: 96.76x
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-01 14:35:45 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 03-01 14:35:45 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-01 14:35:45 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-01 14:35:45 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-01 14:35:45 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-01 14:35:45 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-01 14:35:45 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-01 14:35:45 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-01 14:36:18 custom_all_reduce.py:226] Registering 5600 cuda graph addresses
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-01 14:36:18 custom_all_reduce.py:226] Registering 5600 cuda graph addresses
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-01 14:36:18 custom_all_reduce.py:226] Registering 5600 cuda graph addresses
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-01 14:36:20 custom_all_reduce.py:226] Registering 5600 cuda graph addresses
[1;36m(VllmWorkerProcess pid=913)[0;0m INFO 03-01 14:36:20 model_runner.py:1562] Graph capturing finished in 34 secs, took 2.46 GiB
[1;36m(VllmWorkerProcess pid=912)[0;0m INFO 03-01 14:36:20 model_runner.py:1562] Graph capturing finished in 34 secs, took 2.46 GiB
[1;36m(VllmWorkerProcess pid=911)[0;0m INFO 03-01 14:36:20 model_runner.py:1562] Graph capturing finished in 34 secs, took 2.46 GiB
[1;36m(VllmWorkerProcess pid=910)[0;0m INFO 03-01 14:36:20 model_runner.py:1562] Graph capturing finished in 34 secs, took 2.46 GiB
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-01 14:36:21 custom_all_reduce.py:226] Registering 5670 cuda graph addresses
INFO 03-01 14:36:21 custom_all_reduce.py:226] Registering 5670 cuda graph addresses
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-01 14:36:21 custom_all_reduce.py:226] Registering 5670 cuda graph addresses
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-01 14:36:21 custom_all_reduce.py:226] Registering 5670 cuda graph addresses
[1;36m(VllmWorkerProcess pid=909)[0;0m INFO 03-01 14:36:21 model_runner.py:1562] Graph capturing finished in 36 secs, took 2.49 GiB
[1;36m(VllmWorkerProcess pid=908)[0;0m INFO 03-01 14:36:21 model_runner.py:1562] Graph capturing finished in 36 secs, took 2.49 GiB
INFO 03-01 14:36:21 model_runner.py:1562] Graph capturing finished in 36 secs, took 2.49 GiB
[1;36m(VllmWorkerProcess pid=907)[0;0m INFO 03-01 14:36:21 model_runner.py:1562] Graph capturing finished in 36 secs, took 2.49 GiB
INFO 03-01 14:36:21 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 45.14 seconds
INFO 03-01 14:36:23 api_server.py:958] Starting vLLM API server on http://0.0.0.0:9003
INFO 03-01 14:36:23 launcher.py:23] Available routes are:
INFO 03-01 14:36:23 launcher.py:31] Route: /openapi.json, Methods: GET, HEAD
INFO 03-01 14:36:23 launcher.py:31] Route: /docs, Methods: GET, HEAD
INFO 03-01 14:36:23 launcher.py:31] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 03-01 14:36:23 launcher.py:31] Route: /redoc, Methods: GET, HEAD
INFO 03-01 14:36:23 launcher.py:31] Route: /health, Methods: GET
INFO 03-01 14:36:23 launcher.py:31] Route: /ping, Methods: GET, POST
INFO 03-01 14:36:23 launcher.py:31] Route: /tokenize, Methods: POST
INFO 03-01 14:36:23 launcher.py:31] Route: /detokenize, Methods: POST
INFO 03-01 14:36:23 launcher.py:31] Route: /v1/models, Methods: GET
INFO 03-01 14:36:23 launcher.py:31] Route: /version, Methods: GET
INFO 03-01 14:36:23 launcher.py:31] Route: /v1/chat/completions, Methods: POST
INFO 03-01 14:36:23 launcher.py:31] Route: /v1/completions, Methods: POST
INFO 03-01 14:36:23 launcher.py:31] Route: /v1/embeddings, Methods: POST
INFO 03-01 14:36:23 launcher.py:31] Route: /pooling, Methods: POST
INFO 03-01 14:36:23 launcher.py:31] Route: /score, Methods: POST
INFO 03-01 14:36:23 launcher.py:31] Route: /v1/score, Methods: POST
INFO 03-01 14:36:23 launcher.py:31] Route: /v1/audio/transcriptions, Methods: POST
INFO 03-01 14:36:23 launcher.py:31] Route: /rerank, Methods: POST
INFO 03-01 14:36:23 launcher.py:31] Route: /v1/rerank, Methods: POST
INFO 03-01 14:36:23 launcher.py:31] Route: /v2/rerank, Methods: POST
INFO 03-01 14:36:23 launcher.py:31] Route: /invocations, Methods: POST
INFO 03-01 14:38:18 chat_utils.py:332] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
INFO 03-01 14:38:18 logger.py:39] Received request chatcmpl-fa5eade89b8f4267a9a228bc988fd1d6: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐋🐋🚀🐋🚀🐚🌳\n🐚🌳🚀🌳🦩🐚🚀\n🐚🐚🌳🚀🦩🦩🚀\n🐋🐚🦩🚀🐋🚀🐚\n🌳🐚🐋🐋🐚🚀🦩\n🐚🦩🦩🐋🦩🐚🐋\n🚀🌳🚀🦩🌳🐚🌳\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:38:18 async_llm_engine.py:211] Added request chatcmpl-fa5eade89b8f4267a9a228bc988fd1d6.
INFO 03-01 14:38:18 logger.py:39] Received request chatcmpl-aa1ed52d3ca140d3818d907b5c6b7237: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐳🐳🐳🐳🌞🦅🐳\n🦅🐳🐳🐤🦅🐳🐰\n🌞🐤🦅🐰🦅🦅🦅\n🐤🦅🐳🌞🐳🦅🐤\n🌞🐳🐳🐤🌞🐰🌞\n🌞🐳🐰🦅🐳🐤🦅\n🌞🐰🦅🐰🐤🐳🐤\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:38:18 async_llm_engine.py:211] Added request chatcmpl-aa1ed52d3ca140d3818d907b5c6b7237.
INFO 03-01 14:38:18 logger.py:39] Received request chatcmpl-47f5ec01c1d34bac9a7e10c3616e1754: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐯🐯🦋🐮🦋🐭🦋\n🐮🐈🐈🦋🐯🦋🐈\n🦋🐯🐭🐭🐯🐈🐯\n🐈🐈🦋🐮🦋🐮🐮\n🐮🐭🐮🐯🐮🐮🦋\n🦋🐯🦋🦋🐮🐈🐈\n🦋🐯🦋🐯🐯🐈🐮\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:38:18 async_llm_engine.py:211] Added request chatcmpl-47f5ec01c1d34bac9a7e10c3616e1754.
INFO 03-01 14:38:18 logger.py:39] Received request chatcmpl-8cc0106533e54795b297738da3c0c26d: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🦃🦆🦃🦀🦃🦀🦆\n🌑🦃☄️☄️🦆🦃🦃\n🦆🦆☄️☄️☄️🦃🦆\n🦀🦀☄️🦃🦀🌑🦀\n🌑🦆🦃🦆🦆🦆🦃\n🦆☄️🌑🦀🦀☄️🦆\n☄️🌑🦆🌑🦆🦃🦆\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:38:18 async_llm_engine.py:211] Added request chatcmpl-8cc0106533e54795b297738da3c0c26d.
INFO 03-01 14:38:18 logger.py:39] Received request chatcmpl-d3dc88bc0dc84ee0bd3e11d556fb3db6: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐫🦀🦀🦀🦥🥀🐵\n🦀🐫🥀🥀🐵🐵🐫\n🦀🐵🐫🦀🐵🐵🐫\n🐫🐵🐵🐵🦀🥀🐵\n🦥🥀🐫🐫🐫🦥🐵\n🦀🥀🐵🦀🐵🥀🐵\n🥀🐫🦥🦥🥀🐵🦥\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:38:18 async_llm_engine.py:211] Added request chatcmpl-d3dc88bc0dc84ee0bd3e11d556fb3db6.
INFO 03-01 14:38:18 logger.py:39] Received request chatcmpl-a0097d0740ff4d46ab6f2e9f93649e08: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🌕🐳💧🐳💧🦮🌕\n🐳🌕🦭💧🦭🐳💧\n🐳🌕🦮🐳🌕🐳💧\n🐳🦭🦭🌕🐳🐳💧\n💧🐳🐳🐳🐳🦮🦮\n🐳🦭🐳🐳🐳🌕🦮\n💧🦮🌕🐳🐳🦮💧\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14719, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:38:18 async_llm_engine.py:211] Added request chatcmpl-a0097d0740ff4d46ab6f2e9f93649e08.
INFO 03-01 14:38:18 logger.py:39] Received request chatcmpl-3d67730d46a643b092eb7e426e784316: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🌙🐨🤚🐋🌼🌼🌙\n🌼🐋🐨🐋🤚🤚🌙\n🐨🤚🤚🌼🤚🌙🐋\n🌼🌙🌼🐋🌼🐋🌙\n🐨🌼🌙🌙🤚🐨🌼\n🐋🐨🤚🤚🤚🌙🌼\n🐨🤚🐋🌼🌼🤚🌙\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:38:18 async_llm_engine.py:211] Added request chatcmpl-3d67730d46a643b092eb7e426e784316.
INFO 03-01 14:38:18 logger.py:39] Received request chatcmpl-43b15bf1869d4f35a492a844fcc76dc3: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐻🐄🦥🐻🐄🐻🐗\n🐄🐄🐶🐶🐄🐗🐄\n🐻🐶🐄🐻🐄🐗🐻\n🐄🐻🐻🐶🐄🐗🐄\n🐻🐗🦥🐶🐗🐻🐄\n🐻🦥🐻🐗🐄🐻🐻\n🐗🦥🐄🐻🐄🐻🐄\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:38:18 async_llm_engine.py:211] Added request chatcmpl-43b15bf1869d4f35a492a844fcc76dc3.
INFO 03-01 14:38:19 metrics.py:455] Avg prompt throughput: 50.2 tokens/s, Avg generation throughput: 0.2 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:38:24 metrics.py:455] Avg prompt throughput: 403.1 tokens/s, Avg generation throughput: 220.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:38:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 230.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:38:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 228.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:38:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 225.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:38:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 226.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:38:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 225.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:38:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 224.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:38:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 223.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:39:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 222.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:39:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 219.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:39:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 219.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:39:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 218.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:39:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 216.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:39:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 216.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:39:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 215.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:39:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 214.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:39:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 213.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:39:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:39:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 210.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:39:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 210.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:40:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 210.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:40:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 209.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:40:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:40:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:40:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:40:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:40:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:40:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:40:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 200.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:40:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 200.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:40:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 198.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:40:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 198.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:41:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 196.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:41:06 async_llm_engine.py:179] Finished request chatcmpl-3d67730d46a643b092eb7e426e784316.
INFO:     127.0.0.1:37594 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:41:06 logger.py:39] Received INFO 03-01 14:41:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 198.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:41:12 async_llm_engine.py:179] Finished request chatcmpl-22932c40a22c4e63a2028ce5141feffa.
INFO:     127.0.0.1:44376 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:41:12 logger.py:39] Received request chatcmpl-2533a55541b34c0cac94cb7da0950e01: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🦐🦐🦃🦃🕷🦃🐥\n🦐🐥🐥🐥🐥🐥🕷\n🦃🐥🦐🐥🦃🦐🦜\n🕷🐥🐥🐥🦐🦜🦐\n🦐🐥🕷🐥🦐🦐🦜\n🦃🦐🐥🕷🦃🕷🦃\n🕷🦐🕷🦐🐥🦃🦃\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_INFO 03-01 14:41:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:41:17 async_llm_engine.py:179] Finished request chatcmpl-43b15bf1869d4f35a492a844fcc76dc3.
INFO:     127.0.0.1:37604 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:41:17 logger.py:39] Received INFO 03-01 14:41:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 200.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:41:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 199.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:41:26 async_llm_engine.py:179] Finished request chatcmpl-3988a72be57f40bf87f2e0e2e8216955.
INFO:     127.0.0.1:44352 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:41:26 logger.py:39] Received request chatcmpl-b5dc06bfc3d340b5ad1d1773407f8ee9: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n⛅️🌾🦚🦉🐊🦉🌾\n🦚🦉🦉🌾🐊⛅️🐊\n🐊⛅️⛅️⛅️🌾🌾⛅️\n🦉🦉🌾🐊🌾🦚⛅️\n⛅️🐊🌾🦚⛅️🐊🦚\n🦚🌾⛅️🌾🐊🌾🐊\n🐊⛅️🌾⛅️🐊🌾🦉\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14698, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_INFO 03-01 14:41:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:41:31 async_llm_engine.py:179] Finished request chatcmpl-a0097d0740ff4d46ab6f2e9f93649e08.
INFO:     127.0.0.1:37592 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:41:31 logger.py:39] Received INFO 03-01 14:41:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:41:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:41:41 async_llm_engine.py:179] Finished request chatcmpl-fc301420f354418ab9780e71b5aac6cc.
INFO:     127.0.0.1:44342 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:41:41 logger.py:39] Received request chatcmpl-c73f62342e9a4209ab05ccf03fca1461: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🌙🧽🌷🌷🌱🧽🌙\n⭐️🌷🌱🌙🧽🌱🌙\n🌷🧽⭐️🌱🌷🌷🌱\n🌱🌷🧽🌱🌱🧽🌷\n🧽🌷⭐️🌷🌙⭐️🌙\n🧽🧽🧽🌱⭐️🌱🧽\n🌱⭐️🌷🌷🌷🌙⭐️\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14717, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:41:41 async_llm_engine.py:211] Added request chatcmpl-c73f62342e9a4209ab05ccf03fca1461.
INFO 03-01 14:41:45 metrics.py:455] Avg prompt throughput: 56.5 tokens/s, Avg generation throughput: 199.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:41:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:41:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:42:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:42:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:42:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 199.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:42:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 198.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:42:15 async_llm_engine.py:179] Finished request chatcmpl-8d2f0304c0914e80be1f564ff3903253.
INFO:     127.0.0.1:44314 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:42:15 logger.py:39] Received request chatcmpl-d1df2f7ab6f54c2f93b9bf3783a2f4d6: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🌚🌚✨🐚✨🌚🍃\n🐚🌺🌚🍃✨🍃✨\n🐚🐚✨🌚✨🍃✨\n🍃🍃🍃🌚🌚✨🍃\n🐚🌺🐚✨🌚✨🌚\n🌺🐚🍃🍃🐚🌺🐚\n✨🍃🌺🌺🌚✨🌺\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14722, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_INFO 03-01 14:42:19 async_llm_engine.py:179] Finished request chatcmpl-aa1ed52d3ca140d3818d907b5c6b7237.
INFO:     127.0.0.1:37548 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:42:19 logger.py:39] Received request chatcmpl-ceb590d6209244f89fbec87960c76522: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐕🌹🌕🐕🌕🐕🦇\n🐊🦇🌹🦇🐊🐕🌕\n🐊🌕🐊🌹🦇🐊🌹\n🦇🌕🐕🌹🌹🌹🦇\n🐊🌹🐕🐕🌹🐕🐊\n🦇🌕🦇🦇🐊🌹🦇\n🐊🦇🌹🐕🦇🐕🦇\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:42:19 async_llm_engine.py:211] Added request chatcmpl-ceb590d6209244f89fbec87960c76522.
INFO 03-01 14:42:19 metrics.py:455] Avg prompt throughput: 57.9 tokens/s, Avg generation throughput: 200.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.IINFO 03-01 14:42:22 async_llm_engine.py:179] Finished request chatcmpl-8cc0106533e54795b297738da3c0c26d.
INFO:     127.0.0.1:37576 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:42:22 logger.py:39] Received INFO 03-01 14:42:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:42:27 async_llm_engine.py:179] Finished request chatcmpl-53e7c894819544f0be4fdde8cbdf80e8.
INFO:     127.0.0.1:44336 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:42:27 logger.py:39] Received request chatcmpl-9a74adb3e1e146fd9c97025b704ff58a: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🦏🦏🦏🐋🐺🦏🦟\n🐋🦞🦏🐺🐋🦟🦏\n🐋🦟🦞🦞🦏🦟🦟\n🦏🦞🐺🐺🦞🦟🦞\n🦟🐋🦟🐺🦟🦞🐺\n🐺🦏🦞🦞🐺🦏🦟\n🦏🦏🐺🦟🦟🦟🦞\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_INFO 03-01 14:42:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:42:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 210.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:42:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 209.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:42:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:42:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:42:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:42:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:43:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:43:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:43:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:43:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 200.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:43:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 176.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:43:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 164.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:43:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 169.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:43:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 168.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:43:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 169.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:43:47 async_llm_engine.py:179] Finished request chatcmpl-47f5ec01c1d34bac9a7e10c3616e1754.
INFO:     127.0.0.1:37560 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:43:47 logger.py:39] Received INFO 03-01 14:43:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 168.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:43:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 167.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:44:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 167.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:44:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 168.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:44:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 168.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:44:10 async_llm_engine.py:179] Finished request chatcmpl-2788169910f6415f83b391a9b81fae33.
INFO:     127.0.0.1:44362 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:44:10 logger.py:39] Received request chatcmpl-d9a37ea8ba684e3fa34cb0063d0d1620: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🌏🌏🐔🐟🌏🐟🐟\n🐟🐔🌏☁️🌏🌏🌏\n🌏🐅☁️☁️🐅🌏☁️\n🐔🐔🐔🐅🌏🐔🐔\n🌏☁️🐟🐟🐅🐟🐅\n🐔🐔🐅🐅🌏☁️🐔\n🐟🐅☁️🌏🐟🐅☁️\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:44:10 async_llm_engine.py:211] Added request chatcmpl-d9a37ea8ba684e3fa34cb0063d0d1620.
INFO 03-01 14:44:15 metrics.py:455] Avg prompt throughput: 57.9 tokens/s, Avg generation throughput: 166.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:44:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 166.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:44:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 169.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:44:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 169.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:44:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 169.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:44:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 168.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:44:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 167.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:44:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 168.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:44:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 167.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:44:57 async_llm_engine.py:179] Finished request chatcmpl-bb444482c3a74e5096e14e9c13aa5558.
INFO:     127.0.0.1:44306 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:44:57 logger.py:39] Received request chatcmpl-14eb42f409044a0ebfaec97c15b321f7: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n😄🦎🦎😄🌍😄🦎\n🌍🤚🌍🌍🤚🌍🌍\n😄😄🌍🌍😄🌍🦄\n🤚😄🦄🤚🤚🤚🦎\n🤚🦄🌍🤚🦄🌍🤚\n🤚🦎🤚🦎😄😄🦄\n😄🦄🦎🌍🦄🤚😄\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14721, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:44:57 async_llm_engine.py:211] Added request chatcmpl-14eb42f409044a0ebfaec97c15b321f7.
INFO 03-01 14:45:00 metrics.py:455] Avg prompt throughput: 55.6 tokens/s, Avg generation throughput: 183.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:45:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:45:08 async_llm_engine.py:179] Finished request chatcmpl-c73f62342e9a4209ab05ccf03fca1461.
INFO:     127.0.0.1:33970 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:45:09 logger.py:39] Received request chatcmpl-0fe62044502c4f28b3baa3de36aa4697: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🌻🦍🐈🦍🌷🌻🌻\n🐈🌷🌷🌷🌻🌲🌷\n🐈🌻🌲🌷🐈🌻🌲\n🐈🦍🦍🌲🌷🌻🌷\n🌷🌷🦍🐈🌷🦍🦍\n🌲🌻🌲🌷🌲🐈🌷\n🐈🐈🌲🌷🌻🦍🌻\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:45:09 async_llm_engine.py:211] Added request chatcmpl-0fe62044502c4f28b3baa3de36aa4697.
INFO 03-01 14:45:10 metrics.py:455] Avg prompt throughput: 57.9 tokens/s, Avg generation throughput: 202.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:45:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:45:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:45:24 async_llm_engine.py:179] Finished request chatcmpl-2533a55541b34c0cac94cb7da0950e01.
INFO:     127.0.0.1:59054 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:45:24 logger.py:39] Received request chatcmpl-4624f8192a9b4ceaa5427b89018e3c4a: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🌲🦏🐫🌍🌍🌲🐓\n🐓🐫🦏🐫🐓🌲🦏\n🌍🌲🌲🌲🦏🦏🌲\n🐫🦏🌍🌍🐫🐓🌍\n🦏🐓🦏🌲🐓🦏🐫\n🌍🐓🐫🦏🌍🐫🌍\n🐓🐫🦏🌍🐓🌲🌍\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_INFO 03-01 14:45:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:45:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:45:33 async_llm_engine.py:179] Finished request chatcmpl-1a1b08fb98fd40c290dfce6dddc696cc.
INFO:     127.0.0.1:58290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:45:33 logger.py:39] Received INFO 03-01 14:45:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:45:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:45:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:45:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:45:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:46:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:46:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 200.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:46:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:46:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 200.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:46:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 199.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:46:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 199.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:46:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 198.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:46:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 198.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:46:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 197.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:46:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 195.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:46:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 195.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:46:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 188.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:47:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 164.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:47:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 164.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:47:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 164.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:47:13 async_llm_engine.py:179] Finished request chatcmpl-d9a37ea8ba684e3fa34cb0063d0d1620.
INFO:     127.0.0.1:49628 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:47:13 logger.py:39] Received request chatcmpl-3766441b0603450b9400cc78d4236b81: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐣🐏🐏🐣🐏🎋🐏\n🎋🐣🎋🐜🎋🐡🐣\n🐣🐣🐣🐣🎋🐡🐡\n🐣🐏🐣🎋🎋🐜🎋\n🐏🐣🐏🐡🎋🐡🐡\n🐡🎋🎋🐡🎋🐡🐜\n🎋🐏🐡🐡🐣🐏🐣\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14713, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:47:13 async_llm_engine.py:211] Added request chatcmpl-3766441b0603450b9400cc78d4236b81.
INFO 03-01 14:47:15 metrics.py:455] Avg prompt throughput: 57.3 tokens/s, Avg generation throughput: 160.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:47:16 async_llm_engine.py:179] Finished request chatcmpl-36da4eabba1541e6ad78c20aaa3f212f.
INFO:     127.0.0.1:48100 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:47:16 logger.py:39] Received request chatcmpl-fd85c45cd4664aab9ac2e110efeef5b1: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐚🐤🐢🐤🐢🌍🐢\n🐤🎁🐤🐚🐤🐤🎁\n🐤🐤🎁🐢🐢🌍🎁\n🌍🎁🐢🐚🐚🎁🎁\n🐚🎁🎁🐢🐚🐚🐚\n🐤🐢🐢🌍🐚🌍🐢\n🐢🎁🌍🐚🌍🌍🌍\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:47:16 async_llm_engine.py:211] Added request chatcmpl-fd85c45cd4664aab9ac2e110efeef5b1.
INFO 03-01 14:47:20 metrics.py:455] Avg prompt throughput: 57.8 tokens/s, Avg generation throughput: 164.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:47:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 165.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:47:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 167.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:47:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 167.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:47:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 167.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:47:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 168.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:47:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 165.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:47:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 167.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:47:56 async_llm_engine.py:179] Finished request chatcmpl-b5dc06bfc3d340b5ad1d1773407f8ee9.
INFO:     127.0.0.1:57336 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:47:56 logger.py:39] Received request chatcmpl-80400bca6b454580b84164bd640efd32: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐋🐗🐎🐎🐗🐗🐋\n🐸🐎🌙🐎🐎🐋🐋\n🐸🐸🐸🐎🐗🌙🐎\n🐎🐗🐋🌙🐋🐗🐋\n🐗🐗🐋🐸🐸🐸🐸\n🐋🌙🐗🐗🐸🐋🐋\n🌙🐋🐗🐗🌙🐸🐸\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:47:56 async_llm_engine.py:211] Added request chatcmpl-80400bca6b454580b84164bd640efd32.
INFO 03-01 14:48:00 metrics.py:455] Avg prompt throughput: 57.9 tokens/s, Avg generation throughput: 167.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:48:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 168.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:48:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 167.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:48:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 166.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:48:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 165.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:48:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 167.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:48:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 168.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:48:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 168.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:48:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 166.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:48:42 async_llm_engine.py:179] Finished request chatcmpl-d1df2f7ab6f54c2f93b9bf3783a2f4d6.
INFO:     127.0.0.1:48092 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:48:42 logger.py:39] Received request chatcmpl-75eb24a77caf41d0abcaf2395f62ccea: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🥀🦕🌳🦕🦕🌳🐛\n🐛🦍🦕🥀🐛🌳🌳\n🌳🐛🥀🥀🦕🥀🐛\n🦍🦕🥀🐛🦍🐛🌳\n🐛🐛🐛🥀🌳🦕🦍\n🥀🥀🌳🌳🐛🥀🌳\n🐛🌳🌳🦕🐛🐛🦕\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:48:42 async_llm_engine.py:211] Added request chatcmpl-75eb24a77caf41d0abcaf2395f62ccea.
INFO 03-01 14:48:45 metrics.py:455] Avg prompt throughput: 57.9 tokens/s, Avg generation throughput: 162.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:48:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 169.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:48:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 169.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:49:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 169.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:49:04 async_llm_engine.py:179] Finished request chatcmpl-9a74adb3e1e146fd9c97025b704ff58a.
INFO:     127.0.0.1:57180 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:49:04 logger.py:39] Received request chatcmpl-7923d9303ee643478d30f9c11bbdac48: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐠🦉🏄🦉🐠🏄🦉\n🏄🎋🐠🎋⛅️🦉🎋\n🦉🎋🏄🏄⛅️🐠🐠\n🦉🎋🎋🎋🦉🏄🎋\n🏄🏄🐠⛅️🏄🎋🐠\n🏄🏄🐠⛅️🦉🏄🎋\n🦉⛅️🏄🎋🏄🎋⛅️\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14704, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:49:04 async_llm_engine.py:211] Added request chatcmpl-7923d9303ee643478d30f9c11bbdac48.
INFO 03-01 14:49:06 metrics.py:455] Avg prompt throughput: 59.2 tokens/s, Avg generation throughput: 177.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:49:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:49:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:49:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:49:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:49:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:49:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:49:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:49:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:49:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:49:53 async_llm_engine.py:179] Finished request chatcmpl-14eb42f409044a0ebfaec97c15b321f7.
INFO:     127.0.0.1:51936 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:49:53 logger.py:39] Received request chatcmpl-5b829dbb123540ee8051b4b03fd9d8f0: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n☄️☄️🦒😆🐯🐯😆\n☄️🦏🐯🦒☄️🦏🦏\n🦏☄️☄️🦏🦒☄️😆\n🦏🐯🦏🦒😆🐯🦒\n🐯😆🦒😆🦒😆😆\n🦒🦏☄️🐯☄️🦒☄️\n🐯🐯🐯🦒😆🦒🐯\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14719, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:49:53 async_llm_engine.py:211] Added request chatcmpl-5b829dbb123540ee8051b4b03fd9d8f0.
INFO 03-01 14:49:56 metrics.py:455] Avg prompt throughput: 56.2 tokens/s, Avg generation throughput: 200.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:49:57 async_llm_engine.py:179] Finished request chatcmpl-4624f8192a9b4ceaa5427b89018e3c4a.
INFO:     127.0.0.1:52420 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:49:57 logger.py:39] Received request chatcmpl-10007e35e32f423297498a43516cbc52: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n💫💫🦩💫🐜🦩🦕\n🦕🦩🦩🐛💫🦕🦕\n🐜🦕🦩🐜🐛💫🐜\n🐜🦩🐛🐜🦩💫🐜\n🐜🦩🐜🦩💫🐛💫\n🦕🐜🐜🐛🐜💫🦩\n🐛🦕💫💫🐜🦕💫\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14735, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:49:57 async_llm_engine.py:211] Added request chatcmpl-10007e35e32f423297498a43516cbc52.
INFO 03-01 14:50:01 metrics.py:455] Avg prompt throughput: 52.9 tokens/s, Avg generation throughput: 206.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:50:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 209.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:50:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:50:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:50:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:50:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:50:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:50:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:50:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:50:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 184.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:50:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 167.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:50:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 168.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:51:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 169.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:51:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 167.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:51:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 166.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:51:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 163.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:51:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 164.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:51:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 166.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:51:27 async_llm_engine.py:179] Finished request chatcmpl-3766441b0603450b9400cc78d4236b81.
INFO:     127.0.0.1:60860 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:51:27 logger.py:39] Received request chatcmpl-6f7536f1e2424c2aba67c1976b124ae8: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🦜🦖🦜🦖🌹🦒🦜\n🌹🦜🌹😁🌹🌹🦖\n😁🌹🦖🌹🦒🌹🦜\n🦜🌹🌹🦖🦒🦖😁\n😁🌹🦒🦜🦜🦒🌹\n🦜🌹😁🦜🌹😁🦜\n🌹😁🦒🌹🦒🦒🦒\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14717, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:51:27 async_llm_engine.py:211] Added request chatcmpl-6f7536f1e2424c2aba67c1976b124ae8.
INFO 03-01 14:51:31 metrics.py:455] Avg prompt throughput: 56.5 tokens/s, Avg generation throughput: 164.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:51:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 166.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:51:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 166.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:51:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 165.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:51:48 async_llm_engine.py:179] Finished request chatcmpl-0fe62044502c4f28b3baa3de36aa4697.
INFO:     127.0.0.1:58960 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:51:48 logger.py:39] Received request chatcmpl-a92ee45db68b4a6196f4201bca200e9d: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐢🐍💥🐢🌑🌑🐍\n💥🐍💥🐍🐢🐍🐍\n🐍🐍🌑💥💥🌿🐍\n🌑🐢🐢🌿🌑🌑🐢\n🌑🐢🐢💥🌑🌑🌿\n🌿💥💥🐍🐢🌿🐍\n🌑💥💥🌑🌑🐍🐢\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14720, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:51:48 async_llm_engine.py:211] Added request chatcmpl-a92ee45db68b4a6196f4201bca200e9d.
INFO 03-01 14:51:51 metrics.py:455] Avg prompt throughput: 56.0 tokens/s, Avg generation throughput: 187.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:51:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:52:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:52:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:52:07 async_llm_engine.py:179] Finished request chatcmpl-80400bca6b454580b84164bd640efd32.
INFO:     127.0.0.1:42880 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:52:07 logger.py:39] Received request chatcmpl-b9f541affe48489c91a8f197cd4ebe5a: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🥀🐘🐻🐘🐘🐘🦂\n🐆🐆🐆🦂🥀🐻🥀\n🐻🦂🦂🥀🦂🐘🥀\n🦂🐘🐆🐻🐘🐻🐆\n🐻🐆🥀🦂🐘🥀🐻\n🥀🐆🦂🐆🐘🐻🥀\n🐆🥀🦂🐆🐘🐻🐻\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14719, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:52:07 async_llm_engine.py:211] Added request chatcmpl-b9f541affe48489c91a8f197cd4ebe5a.
INFO 03-01 14:52:11 metrics.py:455] Avg prompt throughput: 56.1 tokens/s, Avg generation throughput: 202.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:52:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:52:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:52:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:52:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:52:34 async_llm_engine.py:179] Finished request chatcmpl-fd85c45cd4664aab9ac2e110efeef5b1.
INFO:     127.0.0.1:60868 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:52:34 logger.py:39] Received request chatcmpl-026b3ed5ebc34d3193fda9b3ac9a2059: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🦆🦆🔥🔥🦆🦆🦆\n🔥🦆🦆🐞🦆🦆🐧\n🔥🦆🐧🐧🔥🌑🦆\n🦆🐞🐞🌑🐞🦆🔥\n🦆🦆🐧🌑🌑🐧🐞\n🐞🌑🐧🐞🔥🦆🦆\n🌑🐧🐧🔥🔥🌑🦆\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:52:34 async_llm_engine.py:211] Added request chatcmpl-026b3ed5ebc34d3193fda9b3ac9a2059.
INFO 03-01 14:52:36 metrics.py:455] Avg prompt throughput: 58.0 tokens/s, Avg generation throughput: 202.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:52:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:52:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:52:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:52:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:53:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:53:03 async_llm_engine.py:179] Finished request chatcmpl-75eb24a77caf41d0abcaf2395f62ccea.
INFO:     127.0.0.1:43286 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:53:03 logger.py:39] Received request chatcmpl-bd88b8942ce64de0b99a63409aaf9ab9: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐐🌒☀️🐐💥💥🌒\n🌒🌒💥🐼🐐🌒💥\n🐐☀️☀️💥🐼💥🐼\n🐼💥💥☀️☀️🐼☀️\n💥🐐🐐☀️🐼💥💥\n☀️🐼🐐🐐☀️💥💥\n☀️🌒🐐🐼🐐🐼🐐\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14729, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:53:03 async_llm_engine.py:211] Added request chatcmpl-bd88b8942ce64de0b99a63409aaf9ab9.
INFO 03-01 14:53:06 metrics.py:455] Avg prompt throughput: 54.0 tokens/s, Avg generation throughput: 204.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:53:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:53:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:53:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:53:25 async_llm_engine.py:179] Finished request chatcmpl-7923d9303ee643478d30f9c11bbdac48.
INFO:     127.0.0.1:51500 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:53:26 logger.py:39] Received request chatcmpl-d7236f45c15f4c7cbc657a892cbe3812: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🦏⛅️🌘🦭🦭🦏🌘\n🦭🌘💧🦏🌘🌘🦏\n💧💧🦭⛅️💧🌘🦭\n🌘🦏💧⛅️🦭💧🌘\n⛅️💧🌘💧🦭⛅️🦏\n⛅️⛅️⛅️💧🌘🦏🦏\n⛅️🌘🌘🦭💧⛅️🦏\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:53:26 async_llm_engine.py:211] Added request chatcmpl-d7236f45c15f4c7cbc657a892cbe3812.
INFO 03-01 14:53:26 metrics.py:455] Avg prompt throughput: 58.0 tokens/s, Avg generation throughput: 203.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:53:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 210.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:53:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 209.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:53:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:53:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:53:46 async_llm_engine.py:179] Finished request chatcmpl-5b829dbb123540ee8051b4b03fd9d8f0.
INFO:     127.0.0.1:58952 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:53:46 logger.py:39] Received request chatcmpl-b6e880fe6a9147a181cfdedbbb7e1fc2: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n😁🌻😁🦇🌻🌻🌻\n😁😁🌻🌻🦇🦦🦇\n🦦🌻🦇😁😁😁🦦\n🐧🌻🌻🐧🦦🐧🦇\n🌻🌻🌻🦦🦇🦇😁\n😁🌻🐧🌻🌻🌻🦦\n😁😁🦇🦦🌻🌻🦦\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14721, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:53:46 async_llm_engine.py:211] Added request chatcmpl-b6e880fe6a9147a181cfdedbbb7e1fc2.
INFO 03-01 14:53:51 metrics.py:455] Avg prompt throughput: 55.7 tokens/s, Avg generation throughput: 208.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:53:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 210.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:54:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 209.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:54:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 209.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:54:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:54:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:54:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:54:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:54:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:54:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:54:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:54:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:54:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 200.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:54:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:55:00 async_llm_engine.py:179] Finished request chatcmpl-10007e35e32f423297498a43516cbc52.
INFO:     127.0.0.1:56378 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:55:00 logger.py:39] Received request chatcmpl-678e3d0e60164bbd8f118b8a43a5d38b: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐫🐼🐼🐼🐺🦀🐺\n🐼🦀🐫🐺🦀🐼🦀\n🦖🦀🐼🐫🦖🐺🦖\n🐫🐼🐺🐼🐼🦀🐼\n🦖🐫🐼🐫🐼🐼🦀\n🐺🦖🦖🐫🐺🐫🦖\n🦀🐺🦀🦀🦀🐼🐼\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:55:00 async_llm_engine.py:211] Added request chatcmpl-678e3d0e60164bbd8f118b8a43a5d38b.
INFO 03-01 14:55:01 metrics.py:455] Avg prompt throughput: 57.9 tokens/s, Avg generation throughput: 199.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:55:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:55:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:55:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:55:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:55:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:55:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:55:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 200.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:55:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 198.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:55:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 198.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:55:51 async_llm_engine.py:179] Finished request chatcmpl-a92ee45db68b4a6196f4201bca200e9d.
INFO:     127.0.0.1:50906 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:55:51 logger.py:39] Received request chatcmpl-8814d2f1a7eb4c9881eb621d6d54cbf3: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🦞🐆🐆🐛🐆🐆🌊\n🐆🦞🦞🌊🌙🌊🦞\n🌊🐆🦞🌙🐆🦞🌊\n🌊🐆🐛🌙🌙🌊🌊\n🐛🌙🌊🌙🌊🐛🐆\n🌊🌊🌙🌊🐆🌊🐆\n🦞🦞🐛🌙🌙🌊🐆\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:55:51 async_llm_engine.py:211] Added request chatcmpl-8814d2f1a7eb4c9881eb621d6d54cbf3.
INFO 03-01 14:55:51 metrics.py:455] Avg prompt throughput: 57.8 tokens/s, Avg generation throughput: 196.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:55:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:56:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:56:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 200.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:56:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 199.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:56:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 198.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:56:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 197.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:56:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 196.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:56:28 async_llm_engine.py:179] Finished request chatcmpl-b9f541affe48489c91a8f197cd4ebe5a.
INFO:     127.0.0.1:55276 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:56:28 logger.py:39] Received request chatcmpl-28d2b98776974c3c89c97d98579b4ed7: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🌕🦭💫🌕🦭💫💫\n🌕🦐🦐🦮🌕💫💫\n🦐🌕🦮🦐🌕🌕💫\n💫🦐🌕🦭🦮🦐💫\n💫🦭💫🦮🦮🦭🦐\n🦮🦐🦮🦐🦭🦐🦭\n🦮🦮🌕🦐🌕🦭💫\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14721, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:56:28 async_llm_engine.py:211] Added request chatcmpl-28d2b98776974c3c89c97d98579b4ed7.
INFO 03-01 14:56:29 async_llm_engine.py:179] Finished request chatcmpl-026b3ed5ebc34d3193fda9b3ac9a2059.
INFO:     127.0.0.1:38878 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:56:29 logger.py:39] Received request chatcmpl-89479a99958b4c5eab6b1883c28cc0a3: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🦆🦙🦆🐴🌒🐴🦆\n🦆🐴🌒🦆🌒🐴🌒\n🌒🦙🦆🌒🦆🦙🌒\n🌒🐏🦆🌒🐏🦆🐏\n🐴🌒🦙🦆🌒🦆🐏\n🐴🌒🐏🐴🦙🦙🦆\n🦙🐴🦙🌒🦆🐴🦙\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14723, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:56:29 async_llm_engine.py:211] Added request chatcmpl-89479a99958b4c5eab6b1883c28cc0a3.
INFO 03-01 14:56:31 metrics.py:455] Avg prompt throughput: 110.9 tokens/s, Avg generation throughput: 196.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:56:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:56:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:56:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 167.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:56:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 168.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:56:56 async_llm_engine.py:179] Finished request chatcmpl-bd88b8942ce64de0b99a63409aaf9ab9.
INFO:     127.0.0.1:47024 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:56:56 logger.py:39] Received request chatcmpl-53e365e39f3c49b7ba24766c032b9419: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐖🐨🐭🐭🥰🦍🥰\n🐨🥰🐨🐖🐨🦍🦍\n🦍🥰🦍🐭🦍🐭🐖\n🐨🦍🐭🐨🥰🐭🦍\n🐨🥰🐨🐭🐖🐨🦍\n🦍🐭🐭🦍🥰🐭🦍\n🥰🐨🐖🐭🐨🦍🐨\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:56:56 async_llm_engine.py:211] Added request chatcmpl-53e365e39f3c49b7ba24766c032b9419.
INFO 03-01 14:56:56 metrics.py:455] Avg prompt throughput: 57.8 tokens/s, Avg generation throughput: 165.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:57:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 169.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:57:04 async_llm_engine.py:179] Finished request chatcmpl-6f7536f1e2424c2aba67c1976b124ae8.
INFO:     127.0.0.1:36842 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:57:04 logger.py:39] Received request chatcmpl-1b4fc188e14442dbbe3cfe651042b043: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n❄️🪐🤚☀️🪐🤚❄️\n🪐🪐🪐🐮☀️☀️🪐\n☀️🤚❄️❄️🪐🤚🤚\n❄️🤚☀️🤚🤚🐮🤚\n🪐☀️☀️☀️🤚❄️☀️\n🤚🐮☀️❄️❄️🤚🐮\n☀️☀️🐮❄️🐮🤚❄️\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:57:04 async_llm_engine.py:211] Added request chatcmpl-1b4fc188e14442dbbe3cfe651042b043.
INFO 03-01 14:57:06 metrics.py:455] Avg prompt throughput: 58.0 tokens/s, Avg generation throughput: 186.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:57:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 209.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:57:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:57:20 async_llm_engine.py:179] Finished request chatcmpl-b6e880fe6a9147a181cfdedbbb7e1fc2.
INFO:     127.0.0.1:54690 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:57:20 logger.py:39] Received request chatcmpl-2e63acdb40f64d139b60698ab9c520d5: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐰🐵🌳🐓🌱🐓🐵\n🐵🐵🐵🐓🐵🐰🐓\n🐰🌱🐓🌳🐓🐰🌱\n🐰🐓🌱🌳🐰🐵🐓\n🐓🐓🌳🌱🐰🌳🐵\n🌳🌱🐰🌳🌳🐰🐵\n🌱🐵🐰🐰🐰🐵🐵\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:57:20 async_llm_engine.py:211] Added request chatcmpl-2e63acdb40f64d139b60698ab9c520d5.
INFO 03-01 14:57:22 metrics.py:455] Avg prompt throughput: 57.8 tokens/s, Avg generation throughput: 202.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:57:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 209.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:57:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:57:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 209.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:57:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:57:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:57:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:57:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:58:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:58:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:58:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:58:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:58:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:58:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:58:30 async_llm_engine.py:179] Finished request chatcmpl-d7236f45c15f4c7cbc657a892cbe3812.
INFO:     127.0.0.1:40364 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 14:58:30 logger.py:39] Received request chatcmpl-d47a1335dffe472f96d7b0502c2e59e6: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐋🏄🕷🦎🦩🦩🐋\n🦎🦩🦎🦩🦎🦎🏄\n🦩🏄🦎🏄🕷🕷🦩\n🦩🦎🏄🏄🦩🕷🦩\n🕷🦎🦎🐋🕷🕷🏄\n🕷🏄🦩🦩🏄🦩🏄\n🐋🕷🏄🦎🐋🦩🦩\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 14:58:30 async_llm_engine.py:211] Added request chatcmpl-d47a1335dffe472f96d7b0502c2e59e6.
INFO 03-01 14:58:32 metrics.py:455] Avg prompt throughput: 57.9 tokens/s, Avg generation throughput: 199.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:58:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:58:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:58:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 14:58:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:58:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 14:59:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 14:59:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:59:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 14:59:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 200.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 14:59:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 200.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:59:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 200.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 14:59:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 199.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.
INFO 03-01 14:59:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 198.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 14:59:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 197.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:59:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 197.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.
INFO 03-01 14:59:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 196.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%.
INFO 03-01 14:59:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 195.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%.
INFO 03-01 15:00:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 195.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%.
INFO 03-01 15:00:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 195.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%.
INFO 03-01 15:00:11 async_llm_engine.py:179] Finished request chatcmpl-8814d2f1a7eb4c9881eb621d6d54cbf3.
INFO:     127.0.0.1:42230 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:00:11 logger.py:39] Received request chatcmpl-384876f4b95c4bc9ac8752a888d7ad59: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🌏🌟🌟🌏🌏🧽🍁\n🐠🌟🍁🌟🐠🧽🍁\n🌟🐠🌏🍁🌏🌏🌟\n🐠🐠🍁🧽🍁🌏🌏\n🌟🌟🌏🌟🐠🌏🌏\n🧽🌟🐠🌟🧽🧽🐠\n🍁🌟🐠🌟🐠🐠🍁\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 15:00:11 async_llm_engine.py:211] Added request chatcmpl-384876f4b95c4bc9ac8752a888d7ad59.
INFO 03-01 15:00:12 metrics.py:455] Avg prompt throughput: 58.0 tokens/s, Avg generation throughput: 191.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 03-01 15:00:12 async_llm_engine.py:179] Finished request chatcmpl-678e3d0e60164bbd8f118b8a43a5d38b.
INFO:     127.0.0.1:59058 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:00:12 logger.py:39] Received request chatcmpl-b3145dd291d241af8a116256d509b4ed: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🦩🦞🦩🦞🐛🦩🦢\n🐏🦢🐛🐏🐏🦞🐏\n🦞🦩🦩🦢🦢🦩🦢\n🐛🦞🐏🦢🦞🐛🐏\n🦩🦢🦢🦢🐏🐏🦢\n🦞🦞🐏🦩🦢🦩🦩\n🐏🐛🦢🦞🐛🐏🦢\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 15:00:12 async_llm_engine.py:211] Added request chatcmpl-b3145dd291d241af8a116256d509b4ed.
INFO 03-01 15:00:17 metrics.py:455] Avg prompt throughput: 57.9 tokens/s, Avg generation throughput: 199.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:00:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:00:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:00:29 async_llm_engine.py:179] Finished request chatcmpl-89479a99958b4c5eab6b1883c28cc0a3.
INFO:     127.0.0.1:38476 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:00:29 logger.py:39] Received request chatcmpl-091c294d8700403ca088c0a3f4624eef: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🏄🐧🦑🐧🦑🐠🐧\n🐧🐧🏄🏄🦑🦑🐧\n🦑🏄🦑🐠🏄🐠🦑\n🐆🐧🏄🐠🐆🐆🏄\n🏄🐆🐧🐧🐠🐧🐆\n🐆🦑🐆🐧🐠🦑🐧\n🐠🐧🦑🐆🦑🐆🦑\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 15:00:29 async_llm_engine.py:211] Added request chatcmpl-091c294d8700403ca088c0a3f4624eef.
INFO 03-01 15:00:32 metrics.py:455] Avg prompt throughput: 57.8 tokens/s, Avg generation throughput: 201.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 15:00:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 15:00:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:00:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:00:48 async_llm_engine.py:179] Finished request chatcmpl-28d2b98776974c3c89c97d98579b4ed7.
INFO:     127.0.0.1:38464 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:00:48 logger.py:39] Received request chatcmpl-039441ffaab249a698f69db2132b2098: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🤚🌾🦃🌾🦃🤚🦃\n🌾🦃🦇🦃🦃🌾🌾\n🐘🤚🤚🤚🌾🐘🤚\n🐘🦃🌾🐘🌾🦃🦃\n🦇🦃🐘🌾🤚🤚🦃\n🤚🐘🤚🐘🦇🦇🤚\n🦃🦃🤚🦃🤚🤚🦃\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 15:00:48 async_llm_engine.py:211] Added request chatcmpl-039441ffaab249a698f69db2132b2098.
INFO 03-01 15:00:52 metrics.py:455] Avg prompt throughput: 57.8 tokens/s, Avg generation throughput: 202.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 15:00:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 15:01:02 async_llm_engine.py:179] Finished request chatcmpl-1b4fc188e14442dbbe3cfe651042b043.
INFO:     127.0.0.1:36754 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:01:02 logger.py:39] Received request chatcmpl-0fb081551c8d4312ae2453a5ad5b02e4: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐦🌍🌘🌵🌍🌵🐞\n🌵🌵🌍🐞🌘🐦🐞\n🐦🌘🌘🌵🌘🌘🐞\n🌍🌵🐞🌵🐞🌵🌘\n🌘🌵🌘🌵🌘🐦🌵\n🐦🐦🌍🌍🌍🌘🌍\n🌵🌵🐦🌵🌘🌘🐦\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 15:01:02 async_llm_engine.py:211] Added request chatcmpl-0fb081551c8d4312ae2453a5ad5b02e4.
INFO 03-01 15:01:02 metrics.py:455] Avg prompt throughput: 57.8 tokens/s, Avg generation throughput: 204.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:01:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 210.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 15:01:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 15:01:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 15:01:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 15:01:26 async_llm_engine.py:179] Finished request chatcmpl-2e63acdb40f64d139b60698ab9c520d5.
INFO:     127.0.0.1:55048 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:01:27 logger.py:39] Received request chatcmpl-38192a88133b47a982a6f0a29fbabc64: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🌼🌼🐅🐅🐳🐅🐅\n🌼🐅🌼🐅🐨🐅🌲\n🌼🌲🌲🐳🐅🐨🐳\n🌲🐅🐅🌲🌼🌼🌼\n🌼🐅🐳🐅🐅🌼🌼\n🐳🐨🐨🐨🌼🐨🐅\n🐳🌼🌲🌼🌲🐅🌲\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 15:01:27 async_llm_engine.py:211] Added request chatcmpl-38192a88133b47a982a6f0a29fbabc64.
INFO 03-01 15:01:27 metrics.py:455] Avg prompt throughput: 58.0 tokens/s, Avg generation throughput: 204.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-01 15:01:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:01:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 15:01:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 210.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 15:01:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 210.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 15:01:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 15:01:54 async_llm_engine.py:179] Finished request chatcmpl-53e365e39f3c49b7ba24766c032b9419.
INFO:     127.0.0.1:36748 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:01:54 logger.py:39] Received request chatcmpl-9a732a117a8842198a81a4accadc5153: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🦐🐿🦐🦐🐼🐼🐼\n🚀🐼🐼🐿🦐🚀🦐\n🚀🐼✨✨🚀🐿✨\n✨🐼✨🦐🦐🐼✨\n🐿🚀🦐✨🚀🐿🐼\n🦐🚀🐼🦐🚀✨✨\n🐿✨🚀🚀🐿🐼🦐\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14720, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 15:01:54 async_llm_engine.py:211] Added request chatcmpl-9a732a117a8842198a81a4accadc5153.
INFO 03-01 15:01:57 metrics.py:455] Avg prompt throughput: 55.9 tokens/s, Avg generation throughput: 209.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:02:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 214.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-01 15:02:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:02:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 15:02:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 210.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 15:02:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 15:02:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 15:02:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 15:02:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 15:02:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 15:02:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:02:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:02:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:03:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:03:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 15:03:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 03-01 15:03:16 async_llm_engine.py:179] Finished request chatcmpl-d47a1335dffe472f96d7b0502c2e59e6.
INFO:     127.0.0.1:44558 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:03:16 logger.py:39] Received request chatcmpl-21849fc12c064b16bca434820b0ad068: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🍂🌿🌿🍂🌓🌓🍂\n🐩🌓🐩🐩🐩🌿🌓\n🍂🐩🌿🍂🌓🐰🐰\n🍂🍂🌿🌓🐰🐩🍂\n🌿🌿🍂🌿🍂🍂🐰\n🐰🐩🐰🌿🐩🌿🍂\n🌓🍂🍂🌿🐩🐰🌿\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14717, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 15:03:16 async_llm_engine.py:211] Added request chatcmpl-21849fc12c064b16bca434820b0ad068.
INFO 03-01 15:03:17 metrics.py:455] Avg prompt throughput: 56.6 tokens/s, Avg generation throughput: 197.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 15:03:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:03:23 async_llm_engine.py:179] Finished request chatcmpl-384876f4b95c4bc9ac8752a888d7ad59.
INFO:     127.0.0.1:40286 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:03:23 logger.py:39] Received request chatcmpl-b6c89cc0a3b845c49ef8156269f81393: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🦆🌖🐃🌔🐃🐃🌒\n🌔🐃🐃🌖🌖🌖🐃\n🌔🐃🌒🌒🌔🌒🐃\n🌔🐃🐃🌒🌖🦆🦆\n🦆🌒🐃🦆🦆🌒🌖\n🦆🐃🦆🌔🐃🐃🌔\n🌖🦆🌒🦆🌒🦆🌖\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14719, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 15:03:23 async_llm_engine.py:211] Added request chatcmpl-b6c89cc0a3b845c49ef8156269f81393.
INFO 03-01 15:03:27 metrics.py:455] Avg prompt throughput: 56.0 tokens/s, Avg generation throughput: 204.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 15:03:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 209.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 15:03:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 15:03:41 async_llm_engine.py:179] Finished request chatcmpl-38192a88133b47a982a6f0a29fbabc64.
INFO:     127.0.0.1:60768 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:03:41 logger.py:39] Received request chatcmpl-4a755ef82ebb425ab98c12ed07217d41: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐄🐄🦊🦊🎄🐐🐧\n🦊🐄🐄🐐🐄🎄🐄\n🎄🐧🦊🐐🐐🦊🐧\n🐧🎄🎄🎄🐧🐄🐄\n🦊🐧🐄🐄🐐🦊🐄\n🎄🐄🐄🐄🎄🐐🐧\n🐐🎄🎄🐄🐧🐄🐧\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-01 15:03:41 async_llm_engine.py:211] Added request chatcmpl-4a755ef82ebb425ab98c12ed07217d41.
INFO 03-01 15:03:42 metrics.py:455] Avg prompt throughput: 57.8 tokens/s, Avg generation throughput: 202.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 15:03:47 async_llm_engine.py:179] Finished request chatcmpl-039441ffaab249a698f69db2132b2098.
INFO:     127.0.0.1:57022 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:03:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV caINFO 03-01 15:03:50 async_llm_engine.py:179INFO 03-01 15:03:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 184.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:03:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 182.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 15:04:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 183.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 15:04:03 async_llm_engine.py:179] Finished request chatcmpl-0fb081551c8d4312ae2453a5ad5b02e4.
INFO:     127.0.0.1:42700 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:04:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 161.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-01 15:04:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 158.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 03-01 15:04:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:04:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 158.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:04:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 158.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 15:04:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 158.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 15:04:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 15:04:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 155.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 15:04:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 154.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 15:04:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 155.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 15:04:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 155.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 15:05:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 154.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 15:05:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 154.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 03-01 15:05:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 153.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:05:17 async_llm_engine.py:179] Finished request chatcmpl-b3145dd291d241af8a116256d509b4ed.
INFO:     127.0.0.1:40298 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:05:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 152.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 15:05:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 133.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 03-01 15:05:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 133.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 15:05:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 132.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 03-01 15:05:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 111.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 15:05:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 15:05:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 03-01 15:05:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 15:05:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 03-01 15:06:00 async_llm_engine.py:179] Finished request chatcmpl-091c294d8700403ca088c0a3f4624eef.
INFO:     127.0.0.1:37464 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:06:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.0 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:06:06 async_llm_engine.py:179] Finished request chatcmpl-9a732a117a8842198a81a4accadc5153.
INFO:     127.0.0.1:42646 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:06:08 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 103.7 tokINFO 03-01 15:06:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokensINFO 03-01 15:06:13 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 85.2 toINFO 03-01 15:06:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/INFO 03-01 15:06:18 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 84.5 tINFO 03-01 15:06:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/sINFO 03-01 15:06:23 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 84.2 INFO 03-01 15:06:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s,INFO 03-01 15:06:28 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 84.2INFO 03-01 15:06:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.4 tokens/s, INFO 03-01 15:06:33 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 83.INFO 03-01 15:06:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 4.7 tokens/s, RINFO 03-01 15:06:36 async_llm_engine.py:179] Finished request chatcmpl-21849fc12c064b16bca434820b0ad06INFO 03-01 15:06:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-01 15:06:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-01 15:06:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-01 15:06:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 03-01 15:07:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:07:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:07:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:07:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:07:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:07:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:07:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:07:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:07:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:07:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:07:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:07:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:08:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:08:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:08:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:08:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:08:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:08:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:08:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:08:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:08:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:08:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:08:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:08:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:09:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:09:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:09:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:09:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:09:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:09:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:09:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:09:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:09:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:09:48 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:09:53 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:09:58 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:10:03 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:10:08 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:10:13 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:10:18 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:10:23 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:10:28 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:10:33 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:10:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:10:43 async_llm_engine.py:223] Aborted request chatcmpl-a0fc1765c1904c3b8e9000feee2df3c9.
INFO 03-01 15:10:43 logger.py:39] Received request chatcmpl-8ec2924d93944fc78e73f30fe80d626e: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answINFO 03-01 15:10:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 03-01 15:10:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 03-01 15:10:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 03-01 15:11:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 03-01 15:11:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 13.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 03-01 15:11:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 13.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 03-01 15:11:12 async_llm_engine.py:179] Finished request chatcmpl-b6c89cc0a3b845c49ef8156269f81393.
INFO:     127.0.0.1:39302 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:11:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:11:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:11:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:11:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:11:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:11:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:11:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:11:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:11:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:12:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:12:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:12:13 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:12:18 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:12:23 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:12:28 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:12:33 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:12:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:12:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:12:48 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:12:53 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:12:58 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:13:03 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:13:08 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:13:13 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:13:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:13:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:13:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:13:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:13:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 03-01 15:13:41 async_llm_engine.py:223] Aborted request chatcmpl-4a755ef82ebb425ab98c12ed07217d41.
INFO 03-01 15:13:41 logger.py:39] Received request chatcmpl-ca4c51b126ea48dd92c0b88600adc8eb: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: 192'\nThere is a rectangular board made up of emojis. Your task is to count the number of horizontal or vertical lines formed by the same emoji, with a length of 2 or more. Output the total count of such lines as the answer.\nboard:\n🐄🐄🦊🦊🎄🐐🐧\n🦊🐄🐄🐐🐄🎄🐄\n🎄🐧🦊🐐🐐🦊🐧\n🐧🎄🎄🎄🐧🐄🐄\n🦊🐧🐄🐄🐐🦊🐄\n🎄🐄🐄🐄🎄🐐🐧\n🐐🎄🎄🐄🐧🐄🐧\n\n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14710, min_tokens=0, logprobs=None, prompt_INFO 03-01 15:13:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:13:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:13:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:14:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:14:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:14:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:14:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:14:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:14:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:14:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:14:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:14:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:14:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:14:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:14:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:15:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:15:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:15:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:15:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:15:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:15:28 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:15:33 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:15:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:15:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:15:48 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:15:53 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:15:58 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:16:03 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:16:08 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:16:13 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:16:18 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:16:23 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:16:28 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:16:33 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:16:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:16:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:16:48 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:16:53 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:16:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:17:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:17:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:17:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:17:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:17:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:17:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:17:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:17:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:17:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:17:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:17:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:18:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:18:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:18:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:18:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:18:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:18:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:18:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:18:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:18:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:18:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:18:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:18:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:19:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:19:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:19:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:19:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:19:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:19:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:19:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:19:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:19:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:19:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:19:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:19:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:20:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:20:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:20:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:20:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:20:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:20:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:20:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:20:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:20:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:20:44 async_llm_engine.py:223] Aborted request chatcmpl-8ec2924d93944fc78e73f30fe80d626e.
INFO 03-01 15:20:44 logger.py:39] Received request chatcmpl-ae12a2b0cce94754bf0be6621565ded7: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nINFO 03-01 15:20:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:20:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:20:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:21:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:21:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:21:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:21:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:21:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:21:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:21:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:21:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:21:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:21:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:21:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:21:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:22:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:22:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:22:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:22:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:22:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:22:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:22:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:22:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:22:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:22:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:22:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:22:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:23:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:23:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:23:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:23:12 async_llm_engine.py:179] Finished request chatcmpl-ca4c51b126ea48dd92c0b88600adc8eb.
INFO:     127.0.0.1:36824 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-01 15:23:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.6 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:23:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:23:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:23:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:23:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:23:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:23:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:23:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:23:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:23:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:23:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:23:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:23:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:24:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:24:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:24:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:24:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:24:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:24:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:24:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:24:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:24:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:24:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:24:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:24:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:25:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:25:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:25:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:25:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:25:23 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:25:28 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 03-01 15:25:33 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:25:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:25:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:25:48 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:25:53 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:25:58 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:26:03 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:26:08 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:26:13 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:26:18 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:26:23 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:26:28 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:26:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 4.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:26:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:26:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:26:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:26:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:26:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:27:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:27:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:27:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:27:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:27:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:27:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 16.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:27:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:27:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:27:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:27:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:27:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:28:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:28:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:28:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:28:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:28:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:28:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:28:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:28:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:28:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:28:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 03-01 15:28:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:28:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:29:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:29:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:29:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:29:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:29:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:29:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:29:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:29:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:29:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:29:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:29:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:29:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:30:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 13.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:30:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:30:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:30:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:30:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:30:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:30:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:30:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:30:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 03-01 15:30:45 async_llm_engine.py:223] Aborted request chatcmpl-ae12a2b0cce94754bf0be6621565ded7.
INFO 03-01 15:30:53 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.1 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
