INFO 02-27 14:45:41 __init__.py:207] Automatically detected platform cuda.
INFO 02-27 14:45:41 api_server.py:912] vLLM API server version 0.7.3
INFO 02-27 14:45:41 api_server.py:913] args: Namespace(host=None, port=9003, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='/map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Qwen-32B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=15000, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=2, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.95, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['DeepSeek-R1-Distill-Qwen-32B'], qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False)
INFO 02-27 14:45:49 config.py:549] This model supports multiple tasks: {'reward', 'score', 'generate', 'classify', 'embed'}. Defaulting to 'generate'.
INFO 02-27 14:45:49 config.py:1382] Defaulting to use mp for distributed inference
WARNING 02-27 14:45:49 config.py:676] Async output processing can not be enabled with pipeline parallel
INFO 02-27 14:45:49 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Qwen-32B', speculative_config=None, tokenizer='/map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Qwen-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=15000, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=DeepSeek-R1-Distill-Qwen-32B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
WARNING 02-27 14:45:50 multiproc_worker_utils.py:300] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 02-27 14:45:50 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=915)[0;0m INFO 02-27 14:45:50 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=916)[0;0m INFO 02-27 14:45:50 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-27 14:45:50 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=918)[0;0m INFO 02-27 14:45:50 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=919)[0;0m INFO 02-27 14:45:50 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-27 14:45:50 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-27 14:45:50 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
INFO 02-27 14:45:51 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-27 14:45:51 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=919)[0;0m INFO 02-27 14:45:51 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-27 14:45:51 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=916)[0;0m INFO 02-27 14:45:51 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=918)[0;0m INFO 02-27 14:45:51 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=915)[0;0m INFO 02-27 14:45:51 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-27 14:45:51 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=918)[0;0m INFO 02-27 14:45:53 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-27 14:45:53 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-27 14:45:53 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=919)[0;0m INFO 02-27 14:45:53 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-27 14:45:53 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=918)[0;0m INFO 02-27 14:45:53 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-27 14:45:53 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=919)[0;0m INFO 02-27 14:45:53 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=915)[0;0m INFO 02-27 14:45:53 utils.py:916] Found nccl from library libnccl.so.2
INFO 02-27 14:45:53 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-27 14:45:53 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=916)[0;0m INFO 02-27 14:45:53 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=915)[0;0m INFO 02-27 14:45:53 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 02-27 14:45:53 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-27 14:45:53 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=916)[0;0m INFO 02-27 14:45:53 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 02-27 14:45:55 custom_all_reduce_utils.py:206] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
INFO 02-27 14:46:18 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-27 14:46:18 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=918)[0;0m INFO 02-27 14:46:18 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=919)[0;0m INFO 02-27 14:46:18 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=916)[0;0m INFO 02-27 14:46:18 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-27 14:46:18 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=915)[0;0m INFO 02-27 14:46:18 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-27 14:46:18 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=918)[0;0m INFO 02-27 14:46:18 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_9f469c2b'), local_subscribe_port=51151, remote_subscribe_port=None)
INFO 02-27 14:46:18 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_3b228373'), local_subscribe_port=55449, remote_subscribe_port=None)
[1;36m(VllmWorkerProcess pid=918)[0;0m INFO 02-27 14:46:18 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-27 14:46:18 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-27 14:46:18 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=918)[0;0m INFO 02-27 14:46:18 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 02-27 14:46:18 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-27 14:46:18 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-27 14:46:18 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 02-27 14:46:18 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=916)[0;0m INFO 02-27 14:46:18 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-27 14:46:18 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=919)[0;0m INFO 02-27 14:46:18 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=915)[0;0m INFO 02-27 14:46:18 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=916)[0;0m INFO 02-27 14:46:18 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=919)[0;0m INFO 02-27 14:46:18 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=915)[0;0m INFO 02-27 14:46:18 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-27 14:46:18 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=915)[0;0m INFO 02-27 14:46:19 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Qwen-32B...
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-27 14:46:19 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Qwen-32B...
[1;36m(VllmWorkerProcess pid=918)[0;0m INFO 02-27 14:46:19 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Qwen-32B...
INFO 02-27 14:46:19 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Qwen-32B...
[1;36m(VllmWorkerProcess pid=916)[0;0m INFO 02-27 14:46:19 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Qwen-32B...
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-27 14:46:19 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Qwen-32B...
[1;36m(VllmWorkerProcess pid=919)[0;0m INFO 02-27 14:46:19 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Qwen-32B...
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-27 14:46:19 model_runner.py:1110] Starting to load model /map-vepfs/models/jiajun/DeepSeek/DeepSeek-R1-Distill-Qwen-32B...
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-27 14:47:06 model_runner.py:1115] Loading model weights took 7.7215 GB
[1;36m(VllmWorkerProcess pid=918)[0;0m INFO 02-27 14:47:06 model_runner.py:1115] Loading model weights took 7.7215 GB
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-27 14:47:06 model_runner.py:1115] Loading model weights took 7.7215 GB
[1;36m(VllmWorkerProcess pid=919)[0;0m INFO 02-27 14:47:06 model_runner.py:1115] Loading model weights took 7.7215 GB
INFO 02-27 14:47:07 model_runner.py:1115] Loading model weights took 7.7215 GB
[1;36m(VllmWorkerProcess pid=916)[0;0m INFO 02-27 14:47:07 model_runner.py:1115] Loading model weights took 7.7215 GB
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-27 14:47:07 model_runner.py:1115] Loading model weights took 7.7215 GB
[1;36m(VllmWorkerProcess pid=915)[0;0m INFO 02-27 14:47:07 model_runner.py:1115] Loading model weights took 7.7215 GB
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-27 14:47:09 worker.py:267] Memory profiling takes 1.90 seconds
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-27 14:47:09 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-27 14:47:09 worker.py:267] model weights take 7.72GiB; non_torch_memory takes 1.70GiB; PyTorch activation peak memory takes 1.45GiB; the rest of the memory reserved for KV Cache is 64.51GiB.
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-27 14:47:09 worker.py:267] Memory profiling takes 1.90 seconds
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-27 14:47:09 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-27 14:47:09 worker.py:267] model weights take 7.72GiB; non_torch_memory takes 1.51GiB; PyTorch activation peak memory takes 1.45GiB; the rest of the memory reserved for KV Cache is 64.70GiB.
[1;36m(VllmWorkerProcess pid=919)[0;0m INFO 02-27 14:47:09 worker.py:267] Memory profiling takes 1.90 seconds
[1;36m(VllmWorkerProcess pid=919)[0;0m INFO 02-27 14:47:09 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=919)[0;0m INFO 02-27 14:47:09 worker.py:267] model weights take 7.72GiB; non_torch_memory takes 1.70GiB; PyTorch activation peak memory takes 1.45GiB; the rest of the memory reserved for KV Cache is 64.51GiB.
[1;36m(VllmWorkerProcess pid=918)[0;0m INFO 02-27 14:47:09 worker.py:267] Memory profiling takes 2.07 seconds
[1;36m(VllmWorkerProcess pid=918)[0;0m INFO 02-27 14:47:09 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=918)[0;0m INFO 02-27 14:47:09 worker.py:267] model weights take 7.72GiB; non_torch_memory takes 1.70GiB; PyTorch activation peak memory takes 1.82GiB; the rest of the memory reserved for KV Cache is 64.14GiB.
INFO 02-27 14:47:12 worker.py:267] Memory profiling takes 5.28 seconds
INFO 02-27 14:47:12 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
INFO 02-27 14:47:12 worker.py:267] model weights take 7.72GiB; non_torch_memory takes 0.94GiB; PyTorch activation peak memory takes 1.30GiB; the rest of the memory reserved for KV Cache is 65.42GiB.
[1;36m(VllmWorkerProcess pid=915)[0;0m INFO 02-27 14:47:12 worker.py:267] Memory profiling takes 5.28 seconds
[1;36m(VllmWorkerProcess pid=915)[0;0m INFO 02-27 14:47:12 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=915)[0;0m INFO 02-27 14:47:12 worker.py:267] model weights take 7.72GiB; non_torch_memory takes 1.03GiB; PyTorch activation peak memory takes 1.30GiB; the rest of the memory reserved for KV Cache is 65.32GiB.
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-27 14:47:12 worker.py:267] Memory profiling takes 5.29 seconds
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-27 14:47:12 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-27 14:47:12 worker.py:267] model weights take 7.72GiB; non_torch_memory takes 0.94GiB; PyTorch activation peak memory takes 1.30GiB; the rest of the memory reserved for KV Cache is 65.42GiB.
[1;36m(VllmWorkerProcess pid=916)[0;0m INFO 02-27 14:47:12 worker.py:267] Memory profiling takes 5.29 seconds
[1;36m(VllmWorkerProcess pid=916)[0;0m INFO 02-27 14:47:12 worker.py:267] the current vLLM instance can use total_gpu_memory (79.35GiB) x gpu_memory_utilization (0.95) = 75.38GiB
[1;36m(VllmWorkerProcess pid=916)[0;0m INFO 02-27 14:47:12 worker.py:267] model weights take 7.72GiB; non_torch_memory takes 1.03GiB; PyTorch activation peak memory takes 1.30GiB; the rest of the memory reserved for KV Cache is 65.32GiB.
INFO 02-27 14:47:13 executor_base.py:111] # cuda blocks: 131365, # CPU blocks: 8192
INFO 02-27 14:47:13 executor_base.py:116] Maximum concurrency for 15000 tokens per request: 140.12x
[1;36m(VllmWorkerProcess pid=918)[0;0m INFO 02-27 14:47:15 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-27 14:47:15 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-27 14:47:15 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=919)[0;0m INFO 02-27 14:47:15 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-27 14:47:15 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 02-27 14:47:16 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=915)[0;0m INFO 02-27 14:47:16 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=916)[0;0m INFO 02-27 14:47:16 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-27 14:47:46 custom_all_reduce.py:226] Registering 4480 cuda graph addresses
[1;36m(VllmWorkerProcess pid=919)[0;0m INFO 02-27 14:47:46 custom_all_reduce.py:226] Registering 4480 cuda graph addresses
[1;36m(VllmWorkerProcess pid=918)[0;0m INFO 02-27 14:47:47 custom_all_reduce.py:226] Registering 4480 cuda graph addresses
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-27 14:47:47 custom_all_reduce.py:226] Registering 4480 cuda graph addresses
[1;36m(VllmWorkerProcess pid=921)[0;0m INFO 02-27 14:47:47 model_runner.py:1562] Graph capturing finished in 32 secs, took 1.87 GiB
[1;36m(VllmWorkerProcess pid=919)[0;0m INFO 02-27 14:47:47 model_runner.py:1562] Graph capturing finished in 32 secs, took 1.87 GiB
[1;36m(VllmWorkerProcess pid=920)[0;0m INFO 02-27 14:47:47 model_runner.py:1562] Graph capturing finished in 32 secs, took 1.87 GiB
[1;36m(VllmWorkerProcess pid=918)[0;0m INFO 02-27 14:47:47 model_runner.py:1562] Graph capturing finished in 32 secs, took 1.87 GiB
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-27 14:47:50 custom_all_reduce.py:226] Registering 4550 cuda graph addresses
[1;36m(VllmWorkerProcess pid=916)[0;0m INFO 02-27 14:47:50 custom_all_reduce.py:226] Registering 4550 cuda graph addresses
INFO 02-27 14:47:50 custom_all_reduce.py:226] Registering 4550 cuda graph addresses
[1;36m(VllmWorkerProcess pid=915)[0;0m INFO 02-27 14:47:50 custom_all_reduce.py:226] Registering 4550 cuda graph addresses
[1;36m(VllmWorkerProcess pid=917)[0;0m INFO 02-27 14:47:50 model_runner.py:1562] Graph capturing finished in 35 secs, took 1.90 GiB
[1;36m(VllmWorkerProcess pid=915)[0;0m INFO 02-27 14:47:50 model_runner.py:1562] Graph capturing finished in 35 secs, took 1.90 GiB
INFO 02-27 14:47:50 model_runner.py:1562] Graph capturing finished in 35 secs, took 1.90 GiB
[1;36m(VllmWorkerProcess pid=916)[0;0m INFO 02-27 14:47:50 model_runner.py:1562] Graph capturing finished in 35 secs, took 1.90 GiB
INFO 02-27 14:47:50 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 43.18 seconds
INFO 02-27 14:47:52 api_server.py:958] Starting vLLM API server on http://0.0.0.0:9003
INFO 02-27 14:47:52 launcher.py:23] Available routes are:
INFO 02-27 14:47:52 launcher.py:31] Route: /openapi.json, Methods: GET, HEAD
INFO 02-27 14:47:52 launcher.py:31] Route: /docs, Methods: GET, HEAD
INFO 02-27 14:47:52 launcher.py:31] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 02-27 14:47:52 launcher.py:31] Route: /redoc, Methods: GET, HEAD
INFO 02-27 14:47:52 launcher.py:31] Route: /health, Methods: GET
INFO 02-27 14:47:52 launcher.py:31] Route: /ping, Methods: GET, POST
INFO 02-27 14:47:52 launcher.py:31] Route: /tokenize, Methods: POST
INFO 02-27 14:47:52 launcher.py:31] Route: /detokenize, Methods: POST
INFO 02-27 14:47:52 launcher.py:31] Route: /v1/models, Methods: GET
INFO 02-27 14:47:52 launcher.py:31] Route: /version, Methods: GET
INFO 02-27 14:47:52 launcher.py:31] Route: /v1/chat/completions, Methods: POST
INFO 02-27 14:47:52 launcher.py:31] Route: /v1/completions, Methods: POST
INFO 02-27 14:47:52 launcher.py:31] Route: /v1/embeddings, Methods: POST
INFO 02-27 14:47:52 launcher.py:31] Route: /pooling, Methods: POST
INFO 02-27 14:47:52 launcher.py:31] Route: /score, Methods: POST
INFO 02-27 14:47:52 launcher.py:31] Route: /v1/score, Methods: POST
INFO 02-27 14:47:52 launcher.py:31] Route: /v1/audio/transcriptions, Methods: POST
INFO 02-27 14:47:52 launcher.py:31] Route: /rerank, Methods: POST
INFO 02-27 14:47:52 launcher.py:31] Route: /v1/rerank, Methods: POST
INFO 02-27 14:47:52 launcher.py:31] Route: /v2/rerank, Methods: POST
INFO 02-27 14:47:52 launcher.py:31] Route: /invocations, Methods: POST
INFO 02-27 14:50:24 chat_utils.py:332] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
INFO 02-27 14:50:24 logger.py:39] Received request chatcmpl-2a86fb886fca4388b6bf6e0637e063c1: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\na|r|c\nn|i|h\na|s|t\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14840, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 14:50:24 async_llm_engine.py:211] Added request chatcmpl-2a86fb886fca4388b6bf6e0637e063c1.
INFO 02-27 14:50:24 logger.py:39] Received request chatcmpl-2fd97379ce104e28b46f39cca141f420: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\nt|e|h\ni|n|t\nc|y|s\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14838, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 14:50:24 async_llm_engine.py:211] Added request chatcmpl-2fd97379ce104e28b46f39cca141f420.
INFO 02-27 14:50:24 logger.py:39] Received request chatcmpl-0338a30ec02d41e0803bc9afc0258a56: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\nc|l|a\na|c|s\nl|i|s\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14838, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 14:50:24 async_llm_engine.py:211] Added request chatcmpl-0338a30ec02d41e0803bc9afc0258a56.
INFO 02-27 14:50:24 logger.py:39] Received request chatcmpl-f7b807a8ae95466789bf4513db4b32fd: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\np|t|e\ni|u|r\nr|c|s\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14838, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 14:50:24 async_llm_engine.py:211] Added request chatcmpl-f7b807a8ae95466789bf4513db4b32fd.
INFO 02-27 14:50:24 logger.py:39] Received request chatcmpl-ad34252d6ed441b3818b61661bfe3a01: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\ne|r|n\ns|p|o\ne|a|s\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14838, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 14:50:24 async_llm_engine.py:211] Added request chatcmpl-ad34252d6ed441b3818b61661bfe3a01.
INFO 02-27 14:50:24 logger.py:39] Received request chatcmpl-695f86d815d44e178c167439171cbd58: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\nn|o|i\nb|a|t\nd|u|c\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14838, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 14:50:24 async_llm_engine.py:211] Added request chatcmpl-695f86d815d44e178c167439171cbd58.
INFO 02-27 14:50:24 logger.py:39] Received request chatcmpl-fcbb9c84157a40aab7acd6ffee275dd4: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\na|r|y\nt|n|u\nv|o|l\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14838, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 14:50:24 async_llm_engine.py:211] Added request chatcmpl-fcbb9c84157a40aab7acd6ffee275dd4.
INFO 02-27 14:50:24 logger.py:39] Received request chatcmpl-4ffecec3365f452abff22575e7814c97: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\ns|o|a\ns|r|g\ne|r|g\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14839, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 14:50:24 async_llm_engine.py:211] Added request chatcmpl-4ffecec3365f452abff22575e7814c97.
INFO 02-27 14:50:28 metrics.py:455] Avg prompt throughput: 258.1 tokens/s, Avg generation throughput: 144.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 14:50:33 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 368.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 14:50:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 368.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-27 14:50:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 372.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 02-27 14:50:48 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 368.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 02-27 14:50:53 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 359.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 02-27 14:50:58 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 352.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 02-27 14:51:03 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 352.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 02-27 14:51:08 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 02-27 14:51:13 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 02-27 14:51:18 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 346.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 02-27 14:51:23 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 337.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 02-27 14:51:28 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 327.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 02-27 14:51:29 async_llm_engine.py:179] Finished request chatcmpl-0338a30ec02d41e0803bc9afc0258a56.
INFO:     127.0.0.1:54570 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 14:51:29 logger.py:39] Received request chatcmpl-46044dd02cf949a386c216fb919ad8ce: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\nt|h|o\nr|o|d\ny|x|o\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14839, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 14:51:29 async_llm_engine.py:211] Added request chatcmpl-46044dd02cf949a386c216fb919ad8ce.
INFO 02-27 14:51:33 metrics.py:455] Avg prompt throughput: 32.2 tokens/s, Avg generation throughput: 330.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 02-27 14:51:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 325.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 02-27 14:51:39 async_llm_engine.py:179] Finished request chatcmpl-695f86d815d44e178c167439171cbd58.
INFO:     127.0.0.1:54594 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 14:51:39 logger.py:39] Received request chatcmpl-5412bb1dcacc484ba831643c800b8590: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\nr|e|t\nt|w|a\nl|a|s\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14837, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 14:51:39 async_llm_engine.py:211] Added request chatcmpl-5412bb1dcacc484ba831643c800b8590.
INFO 02-27 14:51:43 metrics.py:455] Avg prompt throughput: 32.6 tokens/s, Avg generation throughput: 330.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 02-27 14:51:48 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 332.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 02-27 14:51:49 async_llm_engine.py:179] Finished request chatcmpl-fcbb9c84157a40aab7acd6ffee275dd4.
INFO:     127.0.0.1:54604 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 14:51:49 logger.py:39] Received request chatcmpl-f44b6dd560134087975bb19b966f8b22: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\nl|e|n\ne|r|n\np|s|o\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14838, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 14:51:49 async_llm_engine.py:211] Added request chatcmpl-f44b6dd560134087975bb19b966f8b22.
INFO 02-27 14:51:53 metrics.py:455] Avg prompt throughput: 32.4 tokens/s, Avg generation throughput: 329.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 02-27 14:51:58 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 333.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 02-27 14:52:03 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 330.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 02-27 14:52:08 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 329.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 02-27 14:52:13 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 322.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 02-27 14:52:17 async_llm_engine.py:179] Finished request chatcmpl-4ffecec3365f452abff22575e7814c97.
INFO:     127.0.0.1:54612 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 14:52:17 logger.py:39] Received request chatcmpl-37ba0ead15354474bcd3b2ee56e29e59: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\ni|n|g\nn|a|e\ne|k|w\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14837, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 14:52:17 async_llm_engine.py:211] Added request chatcmpl-37ba0ead15354474bcd3b2ee56e29e59.
INFO 02-27 14:52:18 metrics.py:455] Avg prompt throughput: 32.6 tokens/s, Avg generation throughput: 320.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 02-27 14:52:23 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 329.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 02-27 14:52:28 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 324.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 02-27 14:52:33 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 322.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 02-27 14:52:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 319.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 02-27 14:52:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 316.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 02-27 14:52:48 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 315.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 14:52:53 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 309.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 14:52:58 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 309.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 02-27 14:53:03 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 304.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 02-27 14:53:08 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 301.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 02-27 14:53:13 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 296.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 02-27 14:53:18 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 02-27 14:53:23 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 291.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 02-27 14:53:25 async_llm_engine.py:179] Finished request chatcmpl-2fd97379ce104e28b46f39cca141f420.
INFO:     127.0.0.1:54566 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 14:53:25 logger.py:39] Received request chatcmpl-3613172615264047a2b776fb51af3fab: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\na|p|s\nl|c|e\nl|o|d\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14837, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 14:53:25 async_llm_engine.py:211] Added request chatcmpl-3613172615264047a2b776fb51af3fab.
INFO 02-27 14:53:28 metrics.py:455] Avg prompt throughput: 32.6 tokens/s, Avg generation throughput: 290.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 02-27 14:53:31 async_llm_engine.py:179] Finished request chatcmpl-2a86fb886fca4388b6bf6e0637e063c1.
INFO:     127.0.0.1:54550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 14:53:31 logger.py:39] Received request chatcmpl-063c9fddc6c14205a8f1ca1036656b4e: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\ny|n|i\nl|u|f\nm|r|o\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14839, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 14:53:31 async_llm_engine.py:211] Added request chatcmpl-063c9fddc6c14205a8f1ca1036656b4e.
INFO 02-27 14:53:33 metrics.py:455] Avg prompt throughput: 32.1 tokens/s, Avg generation throughput: 297.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 02-27 14:53:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 309.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 14:53:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 275.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 14:53:48 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 191.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 02-27 14:53:53 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 181.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 02-27 14:53:58 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 186.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 02-27 14:54:03 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 185.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 02-27 14:54:08 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 186.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 02-27 14:54:13 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 187.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 02-27 14:54:18 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 188.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 02-27 14:54:23 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 187.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 02-27 14:54:26 async_llm_engine.py:179] Finished request chatcmpl-ad34252d6ed441b3818b61661bfe3a01.
INFO:     127.0.0.1:54580 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 14:54:26 logger.py:39] Received request chatcmpl-dac1ae1e2b0c4c7b85ff10cf7d87688a: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\ne|c|n\nu|r|a\ns|s|a\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14838, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 14:54:26 async_llm_engine.py:211] Added request chatcmpl-dac1ae1e2b0c4c7b85ff10cf7d87688a.
INFO 02-27 14:54:28 metrics.py:455] Avg prompt throughput: 32.4 tokens/s, Avg generation throughput: 188.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 14:54:33 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 02-27 14:54:34 async_llm_engine.py:179] Finished request chatcmpl-46044dd02cf949a386c216fb919ad8ce.
INFO:     127.0.0.1:35756 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 14:54:34 logger.py:39] Received request chatcmpl-bc06cf816d83470190e0fa7fd5f6bcb6: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\nc|a|k\na|t|e\nr|e|r\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14838, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 14:54:34 async_llm_engine.py:211] Added request chatcmpl-bc06cf816d83470190e0fa7fd5f6bcb6.
INFO 02-27 14:54:37 async_llm_engine.py:179] Finished request chatcmpl-5412bb1dcacc484ba831643c800b8590.
INFO:     127.0.0.1:43592 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 14:54:37 logger.py:39] Received request chatcmpl-5068bdecb12e4439aaae4011b3bdb689: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\na|l|u\nn|t|m\ns|t|i\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14839, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 14:54:37 async_llm_engine.py:211] Added request chatcmpl-5068bdecb12e4439aaae4011b3bdb689.
INFO 02-27 14:54:38 metrics.py:455] Avg prompt throughput: 64.6 tokens/s, Avg generation throughput: 200.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 02-27 14:54:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 02-27 14:54:48 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 02-27 14:54:52 async_llm_engine.py:179] Finished request chatcmpl-f7b807a8ae95466789bf4513db4b32fd.
INFO:     127.0.0.1:54578 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 14:54:52 logger.py:39] Received request chatcmpl-07af9d846daf4a5088823a98095915a4: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\na|t|t\nr|s|h\nl|i|g\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14839, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 14:54:52 async_llm_engine.py:211] Added request chatcmpl-07af9d846daf4a5088823a98095915a4.
INFO 02-27 14:54:53 metrics.py:455] Avg prompt throughput: 32.2 tokens/s, Avg generation throughput: 233.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 02-27 14:54:58 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 339.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 02-27 14:55:03 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 328.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 02-27 14:55:08 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 328.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 02-27 14:55:13 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 328.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 02-27 14:55:18 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 326.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 02-27 14:55:23 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 322.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 02-27 14:55:28 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 319.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 02-27 14:55:32 async_llm_engine.py:179] Finished request chatcmpl-3613172615264047a2b776fb51af3fab.
INFO:     127.0.0.1:53460 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 14:55:32 logger.py:39] Received request chatcmpl-07c7098e51dd4906a98bf2c627e7e8e0: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\nv|e|n\na|e|g\nn|c|e\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14837, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 14:55:32 async_llm_engine.py:211] Added request chatcmpl-07c7098e51dd4906a98bf2c627e7e8e0.
INFO 02-27 14:55:33 metrics.py:455] Avg prompt throughput: 32.6 tokens/s, Avg generation throughput: 316.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 02-27 14:55:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 321.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 02-27 14:55:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 223.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 02-27 14:55:48 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 02-27 14:55:53 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 198.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 02-27 14:55:58 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 199.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 02-27 14:56:03 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 02-27 14:56:07 async_llm_engine.py:179] Finished request chatcmpl-37ba0ead15354474bcd3b2ee56e29e59.
INFO:     127.0.0.1:37084 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 14:56:07 logger.py:39] Received request chatcmpl-c6cf07cc5b164aca8c8fb642cf872866: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\nn|e|f\ns|e|e\ni|v|d\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14837, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 14:56:07 async_llm_engine.py:211] Added request chatcmpl-c6cf07cc5b164aca8c8fb642cf872866.
INFO 02-27 14:56:08 metrics.py:455] Avg prompt throughput: 32.5 tokens/s, Avg generation throughput: 204.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 02-27 14:56:13 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 02-27 14:56:18 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 02-27 14:56:23 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 02-27 14:56:24 async_llm_engine.py:179] Finished request chatcmpl-dac1ae1e2b0c4c7b85ff10cf7d87688a.
INFO:     127.0.0.1:55126 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 14:56:24 logger.py:39] Received request chatcmpl-58e8f356589649caa0a3517b17c3f8d9: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\nr|d|e\na|c|h\nl|a|t\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14838, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 14:56:24 async_llm_engine.py:211] Added request chatcmpl-58e8f356589649caa0a3517b17c3f8d9.
INFO 02-27 14:56:28 metrics.py:455] Avg prompt throughput: 32.4 tokens/s, Avg generation throughput: 206.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 02-27 14:56:33 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 02-27 14:56:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 02-27 14:56:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 02-27 14:56:48 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 02-27 14:56:53 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 02-27 14:56:58 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 02-27 14:57:03 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 14:57:08 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 14:57:13 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 02-27 14:57:18 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 02-27 14:57:23 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 02-27 14:57:28 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 02-27 14:57:33 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 02-27 14:57:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 02-27 14:57:40 async_llm_engine.py:179] Finished request chatcmpl-f44b6dd560134087975bb19b966f8b22.
INFO:     127.0.0.1:45298 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 14:57:40 logger.py:39] Received request chatcmpl-7b1e1035e34d49bfb17abfc3523800cf: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\nd|i|n\nr|n|a\no|c|e\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14838, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 14:57:40 async_llm_engine.py:211] Added request chatcmpl-7b1e1035e34d49bfb17abfc3523800cf.
INFO 02-27 14:57:43 metrics.py:455] Avg prompt throughput: 32.4 tokens/s, Avg generation throughput: 264.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 02-27 14:57:48 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 312.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 02-27 14:57:51 async_llm_engine.py:179] Finished request chatcmpl-bc06cf816d83470190e0fa7fd5f6bcb6.
INFO:     127.0.0.1:43426 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 14:57:51 logger.py:39] Received request chatcmpl-1503e0132e604a9a9b41cea165f0aee3: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\nd|e|d\nu|c|t\nn|o|i\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14838, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 14:57:51 async_llm_engine.py:211] Added request chatcmpl-1503e0132e604a9a9b41cea165f0aee3.
INFO 02-27 14:57:53 metrics.py:455] Avg prompt throughput: 32.3 tokens/s, Avg generation throughput: 308.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 02-27 14:57:58 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 316.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 02-27 14:58:03 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 315.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 02-27 14:58:08 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 209.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 02-27 14:58:13 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 14:58:18 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 14:58:21 async_llm_engine.py:179] Finished request chatcmpl-063c9fddc6c14205a8f1ca1036656b4e.
INFO:     127.0.0.1:53462 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 14:58:21 logger.py:39] Received request chatcmpl-31854230faa44d019191127930e3f9cf: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\nm|a|l\nn|a|i\nt|n|g\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14838, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 14:58:21 async_llm_engine.py:211] Added request chatcmpl-31854230faa44d019191127930e3f9cf.
INFO 02-27 14:58:23 metrics.py:455] Avg prompt throughput: 32.4 tokens/s, Avg generation throughput: 255.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 02-27 14:58:25 async_llm_engine.py:179] Finished request chatcmpl-58e8f356589649caa0a3517b17c3f8d9.
INFO:     127.0.0.1:60062 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 14:58:25 logger.py:39] Received request chatcmpl-f0fdb83eb56b42519d190036adc323ef: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\ni|s|d\nu|h|e\ng|n|a\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14838, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 14:58:25 async_llm_engine.py:211] Added request chatcmpl-f0fdb83eb56b42519d190036adc323ef.
INFO 02-27 14:58:28 metrics.py:455] Avg prompt throughput: 32.4 tokens/s, Avg generation throughput: 322.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 02-27 14:58:33 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 322.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 02-27 14:58:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 320.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 02-27 14:58:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 316.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 02-27 14:58:48 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 314.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 02-27 14:58:53 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 311.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 14:58:56 async_llm_engine.py:179] Finished request chatcmpl-1503e0132e604a9a9b41cea165f0aee3.
INFO:     127.0.0.1:44518 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 14:58:56 logger.py:39] Received request chatcmpl-c7da58bfc1fa4e249efbb7cc3d40d39d: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\ne|n|t\nn|a|e\nc|h|d\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14838, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 14:58:56 async_llm_engine.py:211] Added request chatcmpl-c7da58bfc1fa4e249efbb7cc3d40d39d.
INFO 02-27 14:58:58 metrics.py:455] Avg prompt throughput: 32.2 tokens/s, Avg generation throughput: 207.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 02-27 14:59:03 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 14:59:08 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 14:59:13 async_llm_engine.py:179] Finished request chatcmpl-5068bdecb12e4439aaae4011b3bdb689.
INFO:     127.0.0.1:43438 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 14:59:13 logger.py:39] Received request chatcmpl-b2183617c61a4a6992152a7d0d015dd3: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\nm|u|h\ni|a|t\nl|i|e\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14839, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 14:59:13 async_llm_engine.py:211] Added request chatcmpl-b2183617c61a4a6992152a7d0d015dd3.
INFO 02-27 14:59:13 metrics.py:455] Avg prompt throughput: 32.2 tokens/s, Avg generation throughput: 205.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 02-27 14:59:18 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 210.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 02-27 14:59:23 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 02-27 14:59:28 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 209.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 02-27 14:59:33 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 196.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 02-27 14:59:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 02-27 14:59:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 14:59:48 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 14:59:53 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 02-27 14:59:58 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 02-27 15:00:03 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 02-27 15:00:08 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 02-27 15:00:13 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:00:18 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:00:22 async_llm_engine.py:179] Finished request chatcmpl-07c7098e51dd4906a98bf2c627e7e8e0.
INFO:     127.0.0.1:50584 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 15:00:22 logger.py:39] Received request chatcmpl-981b3bccaec448589eb8cf54fcb0288e: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\nc|l|o\nh|w|t\ns|a|h\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14838, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 15:00:22 async_llm_engine.py:211] Added request chatcmpl-981b3bccaec448589eb8cf54fcb0288e.
INFO 02-27 15:00:23 metrics.py:455] Avg prompt throughput: 32.4 tokens/s, Avg generation throughput: 203.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 15:00:28 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 15:00:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 02-27 15:00:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 02-27 15:00:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 02-27 15:00:48 async_llm_engine.py:179] Finished request chatcmpl-07af9d846daf4a5088823a98095915a4.
INFO:     127.0.0.1:56506 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 15:00:48 logger.py:39] Received request chatcmpl-5f9007da81044a08a27393cfe0978696: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\ni|s|i\nf|n|n\ny|e|t\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14838, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 15:00:48 async_llm_engine.py:211] Added request chatcmpl-5f9007da81044a08a27393cfe0978696.
INFO 02-27 15:00:49 metrics.py:455] Avg prompt throughput: 32.4 tokens/s, Avg generation throughput: 217.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 02-27 15:00:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 229.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 02-27 15:00:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 02-27 15:01:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 02-27 15:01:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:01:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:01:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 15:01:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 15:01:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 02-27 15:01:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 02-27 15:01:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 199.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 02-27 15:01:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 198.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 02-27 15:01:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 200.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:01:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 199.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:01:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 197.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:02:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 193.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:02:08 async_llm_engine.py:179] Finished request chatcmpl-c6cf07cc5b164aca8c8fb642cf872866.
INFO:     127.0.0.1:34654 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 15:02:08 logger.py:39] Received request chatcmpl-c8c67e8fa5fe4c2f92dc3bd0508caeac: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\nn|o|i\ni|m|t\ng|r|a\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14840, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 15:02:08 async_llm_engine.py:211] Added request chatcmpl-c8c67e8fa5fe4c2f92dc3bd0508caeac.
INFO 02-27 15:02:09 metrics.py:455] Avg prompt throughput: 31.9 tokens/s, Avg generation throughput: 203.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 15:02:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 307.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 02-27 15:02:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 306.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 02-27 15:02:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 02-27 15:02:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 02-27 15:02:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:02:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:02:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:02:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:02:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 186.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:02:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 199.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:03:02 async_llm_engine.py:179] Finished request chatcmpl-f0fdb83eb56b42519d190036adc323ef.
INFO:     127.0.0.1:34612 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 15:03:02 logger.py:39] Received request chatcmpl-77274c9a80ba49e2900a86fdafec600a: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\ne|u|q\nt|e|s\no|r|g\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14838, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 15:03:02 async_llm_engine.py:211] Added request chatcmpl-77274c9a80ba49e2900a86fdafec600a.
INFO 02-27 15:03:04 metrics.py:455] Avg prompt throughput: 32.4 tokens/s, Avg generation throughput: 199.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 02-27 15:03:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:03:11 async_llm_engine.py:179] Finished request chatcmpl-c7da58bfc1fa4e249efbb7cc3d40d39d.
INFO:     127.0.0.1:52538 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 15:03:11 logger.py:39] Received request chatcmpl-a7c49b97521d4fdda8e23597aaf60edb: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\ni|t|i\no|n|d\nt|r|a\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14839, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 15:03:11 async_llm_engine.py:211] Added request chatcmpl-a7c49b97521d4fdda8e23597aaf60edb.
INFO 02-27 15:03:14 metrics.py:455] Avg prompt throughput: 32.1 tokens/s, Avg generation throughput: 196.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 15:03:17 async_llm_engine.py:179] Finished request chatcmpl-7b1e1035e34d49bfb17abfc3523800cf.
INFO:     127.0.0.1:36686 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 15:03:18 logger.py:39] Received request chatcmpl-51612678a4c742f58447706b76bc683e: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\no|c|e\ns|t|e\ny|s|m\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14838, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 15:03:18 async_llm_engine.py:211] Added request chatcmpl-51612678a4c742f58447706b76bc683e.
INFO 02-27 15:03:19 metrics.py:455] Avg prompt throughput: 32.3 tokens/s, Avg generation throughput: 203.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:03:21 async_llm_engine.py:179] Finished request chatcmpl-c8c67e8fa5fe4c2f92dc3bd0508caeac.
INFO:     127.0.0.1:46940 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 15:03:21 logger.py:39] Received request chatcmpl-3dbbf4b319ae41e293f077c3ab6ee294: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\nt|t|a\ne|d|e\nm|p|t\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14837, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 15:03:21 async_llm_engine.py:211] Added request chatcmpl-3dbbf4b319ae41e293f077c3ab6ee294.
INFO 02-27 15:03:23 async_llm_engine.py:179] Finished request chatcmpl-31854230faa44d019191127930e3f9cf.
INFO:     127.0.0.1:49722 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 15:03:23 logger.py:39] Received request chatcmpl-ac8d40c3f97c4f2798c1ef6e3e0d43ee: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\nr|a|y\nt|l|l\nn|e|c\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14837, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 15:03:23 async_llm_engine.py:211] Added request chatcmpl-ac8d40c3f97c4f2798c1ef6e3e0d43ee.
INFO 02-27 15:03:24 metrics.py:455] Avg prompt throughput: 65.1 tokens/s, Avg generation throughput: 215.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 02-27 15:03:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 335.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 02-27 15:03:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 333.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:03:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 329.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:03:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 331.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:03:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 328.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:03:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 324.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:03:55 async_llm_engine.py:179] Finished request chatcmpl-981b3bccaec448589eb8cf54fcb0288e.
INFO:     127.0.0.1:60976 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 15:03:55 logger.py:39] Received request chatcmpl-43adb0a59db447f1a084768ef6b9273f: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\ni|v|r\ns|r|e\nt|e|s\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14839, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 15:03:55 async_llm_engine.py:211] Added request chatcmpl-43adb0a59db447f1a084768ef6b9273f.
INFO 02-27 15:03:58 async_llm_engine.py:179] Finished request chatcmpl-b2183617c61a4a6992152a7d0d015dd3.
INFO:     127.0.0.1:35064 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 15:03:58 logger.py:39] Received request chatcmpl-9bd73355be514c8cae96550fd57f4817: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\nh|e|m\ne|g|o\nc|i|n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14839, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 15:03:58 async_llm_engine.py:211] Added request chatcmpl-9bd73355be514c8cae96550fd57f4817.
INFO 02-27 15:03:59 metrics.py:455] Avg prompt throughput: 64.3 tokens/s, Avg generation throughput: 328.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 02-27 15:04:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 343.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 02-27 15:04:04 async_llm_engine.py:179] Finished request chatcmpl-3dbbf4b319ae41e293f077c3ab6ee294.
INFO:     127.0.0.1:43696 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 15:04:04 logger.py:39] Received request chatcmpl-cab18a1a527c4b6bbd9c60c652ae2580: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\ny|t|i\nb|i|l\na|n|i\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14840, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 15:04:04 async_llm_engine.py:211] Added request chatcmpl-cab18a1a527c4b6bbd9c60c652ae2580.
INFO 02-27 15:04:09 metrics.py:455] Avg prompt throughput: 31.9 tokens/s, Avg generation throughput: 345.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 02-27 15:04:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 343.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 02-27 15:04:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 334.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:04:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 330.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:04:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 329.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:04:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 328.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:04:35 async_llm_engine.py:179] Finished request chatcmpl-5f9007da81044a08a27393cfe0978696.
INFO:     127.0.0.1:47392 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 15:04:35 logger.py:39] Received request chatcmpl-32b351215fcf499cacb147506f6b8b4c: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\nr|o|t\ne|f|c\nr|r|a\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14838, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 15:04:35 async_llm_engine.py:211] Added request chatcmpl-32b351215fcf499cacb147506f6b8b4c.
INFO 02-27 15:04:39 metrics.py:455] Avg prompt throughput: 32.4 tokens/s, Avg generation throughput: 336.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:04:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 338.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:04:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 331.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:04:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 330.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:04:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 327.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:05:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 319.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 02-27 15:05:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 323.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 02-27 15:05:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 318.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 02-27 15:05:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 314.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:05:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 310.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 15:05:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 308.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 15:05:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 305.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 02-27 15:05:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 303.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 02-27 15:05:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 300.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 02-27 15:05:45 async_llm_engine.py:179] Finished request chatcmpl-77274c9a80ba49e2900a86fdafec600a.
INFO:     127.0.0.1:36972 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 15:05:45 logger.py:39] Received request chatcmpl-76c66381afbb4937ad00fec45773c611: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\nt|s|i\ni|s|t\nc|t|a\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14838, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 15:05:45 async_llm_engine.py:211] Added request chatcmpl-76c66381afbb4937ad00fec45773c611.
INFO 02-27 15:05:46 async_llm_engine.py:179] Finished request chatcmpl-51612678a4c742f58447706b76bc683e.
INFO:     127.0.0.1:43680 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 15:05:46 logger.py:39] Received request chatcmpl-bd0287a336134e95ad0b10c5bab1d273: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\ni|d|g\nv|i|n\no|r|p\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14839, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 15:05:46 async_llm_engine.py:211] Added request chatcmpl-bd0287a336134e95ad0b10c5bab1d273.
INFO 02-27 15:05:49 metrics.py:455] Avg prompt throughput: 64.6 tokens/s, Avg generation throughput: 307.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 02-27 15:05:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 317.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 02-27 15:05:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 312.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:06:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 311.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 15:06:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 308.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 15:06:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 307.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 02-27 15:06:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 304.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 02-27 15:06:23 async_llm_engine.py:179] Finished request chatcmpl-ac8d40c3f97c4f2798c1ef6e3e0d43ee.
INFO:     127.0.0.1:49008 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 15:06:23 logger.py:39] Received request chatcmpl-c8004a467e964e1cbbf2bd1fd396f848: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\ne|s|e\nn|p|r\nt|e|r\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14839, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 15:06:23 async_llm_engine.py:211] Added request chatcmpl-c8004a467e964e1cbbf2bd1fd396f848.
INFO 02-27 15:06:24 metrics.py:455] Avg prompt throughput: 32.1 tokens/s, Avg generation throughput: 300.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:06:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 305.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 15:06:31 async_llm_engine.py:179] Finished request chatcmpl-cab18a1a527c4b6bbd9c60c652ae2580.
INFO:     127.0.0.1:39114 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 15:06:31 logger.py:39] Received request chatcmpl-211069c73a0740f89e3ef30643eff406: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\ns|l|c\np|e|a\ne|c|t\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14837, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 15:06:31 async_llm_engine.py:211] Added request chatcmpl-211069c73a0740f89e3ef30643eff406.
INFO 02-27 15:06:34 metrics.py:455] Avg prompt throughput: 32.6 tokens/s, Avg generation throughput: 310.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 02-27 15:06:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 235.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 02-27 15:06:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:06:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:06:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 15:06:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 15:07:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 199.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 02-27 15:07:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 02-27 15:07:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 200.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 02-27 15:07:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 02-27 15:07:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:07:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 199.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:07:31 async_llm_engine.py:179] Finished request chatcmpl-9bd73355be514c8cae96550fd57f4817.
INFO:     127.0.0.1:36420 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 15:07:31 logger.py:39] Received request chatcmpl-e8146fa6d77342dfa601460edb4885b0: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\nc|t|r\ni|s|a\ng|e|t\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14838, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 15:07:31 async_llm_engine.py:211] Added request chatcmpl-e8146fa6d77342dfa601460edb4885b0.
INFO 02-27 15:07:34 metrics.py:455] Avg prompt throughput: 32.4 tokens/s, Avg generation throughput: 200.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 15:07:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 193.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 15:07:41 async_llm_engine.py:179] Finished request chatcmpl-a7c49b97521d4fdda8e23597aaf60edb.
INFO:     127.0.0.1:36980 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 15:07:41 logger.py:39] Received request chatcmpl-15169f3856de43fb84b1458fb23b759d: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\ni|s|s\nv|y|a\ni|t|p\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14837, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 15:07:41 async_llm_engine.py:211] Added request chatcmpl-15169f3856de43fb84b1458fb23b759d.
INFO 02-27 15:07:44 metrics.py:455] Avg prompt throughput: 32.6 tokens/s, Avg generation throughput: 204.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:07:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 02-27 15:07:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 209.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 02-27 15:07:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 02-27 15:08:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 02-27 15:08:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:08:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:08:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 15:08:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 15:08:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 02-27 15:08:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 02-27 15:08:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 02-27 15:08:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 02-27 15:08:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 199.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:08:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:08:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 199.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:09:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:09:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:09:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 200.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:09:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 199.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:09:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 188.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:09:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 183.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:09:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 194.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 02-27 15:09:39 async_llm_engine.py:179] Finished request chatcmpl-43adb0a59db447f1a084768ef6b9273f.
INFO:     127.0.0.1:36412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 15:09:39 logger.py:39] Received request chatcmpl-3378035e2f494ee2ae161207bebbf683: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\nl|i|g\nn|t|h\ne|e|n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14839, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 15:09:39 async_llm_engine.py:211] Added request chatcmpl-3378035e2f494ee2ae161207bebbf683.
INFO 02-27 15:09:39 metrics.py:455] Avg prompt throughput: 32.1 tokens/s, Avg generation throughput: 197.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 02-27 15:09:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 200.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 02-27 15:09:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 199.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:09:51 async_llm_engine.py:179] Finished request chatcmpl-bd0287a336134e95ad0b10c5bab1d273.
INFO:     127.0.0.1:38308 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 15:09:51 logger.py:39] Received request chatcmpl-82cb097ae13b443283fddb50a8404bd1: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\ne|a|t\nr|g|h\nb|n|i\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14839, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 15:09:51 async_llm_engine.py:211] Added request chatcmpl-82cb097ae13b443283fddb50a8404bd1.
INFO 02-27 15:09:55 metrics.py:455] Avg prompt throughput: 32.1 tokens/s, Avg generation throughput: 200.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 15:10:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 02-27 15:10:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 02-27 15:10:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 02-27 15:10:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 199.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 02-27 15:10:16 async_llm_engine.py:179] Finished request chatcmpl-32b351215fcf499cacb147506f6b8b4c.
INFO:     127.0.0.1:51274 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 15:10:16 logger.py:39] Received request chatcmpl-23e5b70dfc074f66b208909fc2b33fb1: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\nd|u|s\np|m|g\nl|i|n\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14839, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 15:10:16 async_llm_engine.py:211] Added request chatcmpl-23e5b70dfc074f66b208909fc2b33fb1.
INFO 02-27 15:10:20 metrics.py:455] Avg prompt throughput: 32.2 tokens/s, Avg generation throughput: 279.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 02-27 15:10:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 310.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:10:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 310.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:10:34 async_llm_engine.py:179] Finished request chatcmpl-e8146fa6d77342dfa601460edb4885b0.
INFO:     127.0.0.1:39020 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 15:10:34 logger.py:39] Received request chatcmpl-e573b46f06b444c5b5b420e75ccdadb3: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\nc|i|w\na|s|h\nl|m|i\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14841, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 15:10:34 async_llm_engine.py:211] Added request chatcmpl-e573b46f06b444c5b5b420e75ccdadb3.
INFO 02-27 15:10:35 metrics.py:455] Avg prompt throughput: 31.6 tokens/s, Avg generation throughput: 216.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 02-27 15:10:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 02-27 15:10:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:10:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:10:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 15:11:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 185.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 15:11:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 02-27 15:11:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 02-27 15:11:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 02-27 15:11:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 02-27 15:11:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 02-27 15:11:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 199.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:11:31 async_llm_engine.py:179] Finished request chatcmpl-76c66381afbb4937ad00fec45773c611.
INFO:     127.0.0.1:38296 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 15:11:31 logger.py:39] Received request chatcmpl-ebaa79661e9a4bbf99bc2745ee160597: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\nr|e|m\ne|p|r\nr|f|o\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14839, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 15:11:31 async_llm_engine.py:211] Added request chatcmpl-ebaa79661e9a4bbf99bc2745ee160597.
INFO 02-27 15:11:35 metrics.py:455] Avg prompt throughput: 32.1 tokens/s, Avg generation throughput: 190.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:11:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:11:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 15:11:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 186.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 02-27 15:11:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 188.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 02-27 15:12:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 188.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 02-27 15:12:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 02-27 15:12:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 187.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 02-27 15:12:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 183.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 02-27 15:12:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 187.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:12:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 181.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:12:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 179.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:12:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 186.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:12:36 async_llm_engine.py:179] Finished request chatcmpl-23e5b70dfc074f66b208909fc2b33fb1.
INFO:     127.0.0.1:52398 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 15:12:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 169.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:12:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 162.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:12:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 160.9 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:12:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 161.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:13:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 162.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:13:03 async_llm_engine.py:179] Finished request chatcmpl-82cb097ae13b443283fddb50a8404bd1.
INFO:     127.0.0.1:53158 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 15:13:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 152.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 02-27 15:13:09 async_llm_engine.py:179] Finished request chatcmpl-211069c73a0740f89e3ef30643eff406.
INFO:     127.0.0.1:57600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 15:13:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 138.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 02-27 15:13:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 129.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 02-27 15:13:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 130.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 02-27 15:13:20 async_llm_engine.py:179] Finished request chatcmpl-c8004a467e964e1cbbf2bd1fd396f848.
INFO:     127.0.0.1:57594 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 15:13:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.7 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:13:27 async_llm_engine.py:179] Finished request chatcmpl-ebaa79661e9a4bbf99bc2745ee160597.
INFO:     127.0.0.1:60208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 15:13:29 async_llm_engine.py:179] Finished request chatcmpl-15169f3856de43fb84b1458fb23b759d.
INFO:     127.0.0.1:37476 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 15:13:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 74.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 02-27 15:13:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 02-27 15:13:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.2 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 02-27 15:13:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 13.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 02-27 15:13:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 02-27 15:13:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 02-27 15:14:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 13.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 02-27 15:14:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 02-27 15:14:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 02-27 15:14:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 02-27 15:14:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 02-27 15:14:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 11.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 02-27 15:14:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 13.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 02-27 15:14:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 02-27 15:14:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 02-27 15:14:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.2 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 02-27 15:14:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:14:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 11.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:15:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 13.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:15:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 13.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:15:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:15:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:15:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.2 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:15:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:15:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:15:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 16.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:15:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:15:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 48.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:15:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 52.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:15:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 11.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:16:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:16:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:16:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.2 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:16:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:16:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:16:28 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:16:33 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 16.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:16:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:16:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.2 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 02-27 15:16:48 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 22.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 02-27 15:16:53 async_llm_engine.py:179] Finished request chatcmpl-3378035e2f494ee2ae161207bebbf683.
INFO:     127.0.0.1:54540 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 02-27 15:16:53 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:16:58 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:17:03 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:17:08 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:17:13 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:17:18 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:17:23 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:17:28 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:17:33 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:17:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:17:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:17:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:17:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:17:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:18:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:18:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:18:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:18:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:18:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:18:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:18:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:18:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:18:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:18:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:18:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:18:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 11.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:19:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:19:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:19:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:19:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:19:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:19:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 02-27 15:19:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 02-27 15:19:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 02-27 15:19:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 02-27 15:19:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 02-27 15:19:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 02-27 15:20:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 02-27 15:20:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 02-27 15:20:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 02-27 15:20:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 02-27 15:20:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 02-27 15:20:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 02-27 15:20:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 02-27 15:20:34 async_llm_engine.py:223] Aborted request chatcmpl-e573b46f06b444c5b5b420e75ccdadb3.
INFO 02-27 15:20:35 logger.py:39] Received request chatcmpl-28c7c94e4cfb41f894b3a84fbffa992a: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\nc|i|w\na|s|h\nl|m|i\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14841, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 15:20:35 async_llm_engine.py:211] Added request chatcmpl-28c7c94e4cfb41f894b3a84fbffa992a.
INFO 02-27 15:20:36 metrics.py:455] Avg prompt throughput: 26.3 tokens/s, Avg generation throughput: 4.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:20:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:20:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:20:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:20:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:21:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:21:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:21:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:21:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:21:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:21:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:21:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:21:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:21:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:21:48 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:21:53 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:21:58 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:22:03 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:22:08 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:22:13 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:22:18 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:22:23 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:22:28 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:22:33 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:22:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:22:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:22:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:22:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:22:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:23:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:23:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:23:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:23:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:23:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:23:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:23:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 11.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:23:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:23:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:23:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:23:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:24:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:24:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:24:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 23.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:24:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:24:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:24:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:24:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:24:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 11.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:24:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:24:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:24:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:24:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:25:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:25:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:25:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:25:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:25:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:25:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:25:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:25:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:25:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:25:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:25:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:25:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:26:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:26:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:26:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:26:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:26:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:26:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 10.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:26:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 10.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:26:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:26:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:26:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:26:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:26:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:27:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:27:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:27:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:27:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:27:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:27:28 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:27:33 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:27:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:27:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:27:48 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:27:53 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:27:58 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:28:03 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:28:08 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:28:13 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:28:18 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:28:23 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:28:28 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:28:33 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:28:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:28:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:28:48 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:28:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:28:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:29:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 11.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:29:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:29:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:29:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:29:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:29:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:29:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:29:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:29:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:29:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:29:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:29:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:30:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:30:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:30:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:30:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:30:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:30:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:30:35 async_llm_engine.py:223] Aborted request chatcmpl-28c7c94e4cfb41f894b3a84fbffa992a.
INFO 02-27 15:30:36 logger.py:39] Received request chatcmpl-c8c5ff8ff3de497db90d178571a4bb9b: prompt: "<｜begin▁of▁sentence｜><｜User｜>\nYou are a good game problem-solver, I'll give you a question.\nYour task is:\n- First, answer the question.\n- Second, output the answer in the required format. The last line of your response should be in the following format: 'Answer: $YOUR_ANSWER' (without quotes), where YOUR_ANSWER is your final answer to the question,e.g.'Answer: happy'\nGame rules: A word with a length of 9, randomly select a starting point in a 3x3 square, and fill in the letters in the order they appear in the word, selecting consecutive positions to place them in the grid. Please identify the word in the square.\nThe square:\nc|i|w\na|s|h\nl|m|i\n<｜Assistant｜>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14841, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 02-27 15:30:36 async_llm_engine.py:211] Added request chatcmpl-c8c5ff8ff3de497db90d178571a4bb9b.
INFO 02-27 15:30:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:30:41 metrics.py:455] Avg prompt throughput: 31.7 tokens/s, Avg generation throughput: 2.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:30:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:30:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:30:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:31:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:31:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:31:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:31:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:31:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:31:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:31:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:31:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:31:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:31:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:31:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 11.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:31:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 22.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:32:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:32:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-27 15:32:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:32:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 11.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:32:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:32:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:32:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:32:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:32:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:32:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:32:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:32:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:33:03 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:33:08 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:33:13 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:33:18 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:33:23 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:33:28 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:33:33 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:33:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:33:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:33:48 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:33:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:33:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:34:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:34:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:34:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:34:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:34:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:34:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:34:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:34:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:34:44 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:34:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:34:54 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:34:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:35:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 10.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:35:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:35:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:35:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:35:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:35:30 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:35:35 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:35:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:35:45 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:35:50 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:35:55 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 10.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:36:00 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 11.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:36:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:36:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:36:15 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:36:20 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:36:25 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:36:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-27 15:36:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:36:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:36:46 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:36:51 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:36:56 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:37:01 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:37:06 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:37:11 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:37:16 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:37:21 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:37:26 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:37:31 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:37:36 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:37:41 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:37:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:37:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:37:57 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:38:02 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:38:07 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:38:12 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:38:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:38:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:38:27 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:38:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:38:37 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:38:42 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:38:47 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:38:52 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:38:58 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:39:03 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:39:08 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:39:13 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:39:18 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:39:23 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:39:28 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:39:33 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:39:38 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:39:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:39:48 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:39:53 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 28.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:39:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:40:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:40:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:40:14 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:40:19 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:40:24 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:40:29 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:40:34 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-27 15:40:36 async_llm_engine.py:223] Aborted request chatcmpl-c8c5ff8ff3de497db90d178571a4bb9b.
INFO 02-27 15:40:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.3 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
